{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Fusion Retrieval: Combining Vector and Keyword Search\n",
    "\n",
    "In this notebook, I implement a fusion retrieval system that combines the strengths of semantic vector search with keyword-based BM25 retrieval. This approach improves retrieval quality by capturing both conceptual similarity and exact keyword matches.\n",
    "\n",
    "## Why Fusion Retrieval Matters\n",
    "\n",
    "Traditional RAG systems typically rely on vector search alone, but this has limitations:\n",
    "\n",
    "- Vector search excels at semantic similarity but may miss exact keyword matches\n",
    "- Keyword search is great for specific terms but lacks semantic understanding\n",
    "- Different queries perform better with different retrieval methods\n",
    "\n",
    "Fusion retrieval gives us the best of both worlds by:\n",
    "\n",
    "- Performing both vector-based and keyword-based retrieval\n",
    "- Normalizing the scores from each approach\n",
    "- Combining them with a weighted formula\n",
    "- Ranking documents based on the combined score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
    "    pdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
    "    text = \"\"  # Initialize an empty string to store the extracted text\n",
    "    \n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]  # Get the page object\n",
    "        text += page.get_text()  # Extract text from the page and append to the text string\n",
    "    \n",
    "    return text  # Return the extracted text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of chunks with text and metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store chunks\n",
    "    \n",
    "    # Iterate over the text with the specified chunk size and overlap\n",
    "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
    "        chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
    "        if chunk:  # Ensure we don't add empty chunks\n",
    "            chunk_data = {\n",
    "                \"text\": chunk,  # The chunk text\n",
    "                \"metadata\": {\n",
    "                    \"start_char\": i,  # Start character index of the chunk\n",
    "                    \"end_char\": i + len(chunk)  # End character index of the chunk\n",
    "                }\n",
    "            }\n",
    "            chunks.append(chunk_data)  # Add the chunk data to the list\n",
    "    \n",
    "    print(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\n",
    "    return chunks  # Return the list of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing extra whitespace and special characters.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Replace multiple whitespace characters (including newlines and tabs) with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix common OCR issues by replacing tab and newline characters with a space\n",
    "    text = text.replace('\\\\t', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    \n",
    "    # Remove any leading or trailing whitespace and ensure single spaces between words\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "\n",
    "def create_embeddings(texts, model=\"models/embedding-001\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts using the specified Gemini model.\n",
    "\n",
    "    Args:\n",
    "        texts (str or List[str]): Input text(s)\n",
    "        model (str): Embedding model name. Defaults to \"models/embedding-001\".\n",
    "\n",
    "    Returns:\n",
    "        List[float] or List[List[float]]: Embedding vector(s)\n",
    "    \"\"\"\n",
    "    # Gemini's `embed_content` can handle both a single string or a list of strings\n",
    "    # in the 'content' parameter.\n",
    "    response = genai.embed_content(\n",
    "        model=model,\n",
    "        content=texts\n",
    "    )\n",
    "\n",
    "    # The response['embedding'] key contains the embedding vectors.\n",
    "    # If the original input was a single string, return just the first embedding vector.\n",
    "    if isinstance(texts, str):\n",
    "        return response['embedding']\n",
    "    \n",
    "    # Otherwise, return all embedding vectors as a list of lists.\n",
    "    return response['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store populated with Gemini embeddings.\n",
      "\n",
      "Searching for items similar to: 'how to create a family emergency plan'\n",
      "\n",
      "Top 2 search results:\n",
      "  - Text: The Pennsylvania Emergency Preparedness Guide details how to make a family emergency plan.\n",
      "    Similarity: 0.8418\n",
      "    Metadata: {'index': 0}\n",
      "  - Text: A Home Emergency Kit Checklist is available on pages 12-15.\n",
      "    Similarity: 0.7590\n",
      "    Metadata: {'index': 1}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- The SimpleVectorStore class (no changes needed) ---\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store text content\n",
    "        self.metadata = []  # List to store metadata\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text content\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): List of text items\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
    "            self.add_item(\n",
    "                text=item[\"text\"],\n",
    "                embedding=embedding,\n",
    "                metadata={**item.get(\"metadata\", {}), \"index\": i}\n",
    "            )\n",
    "    \n",
    "    def similarity_search_with_scores(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding with similarity scores.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[Dict, float]]: Top k most similar items with scores\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        query_vector = np.array(query_embedding).reshape(1, -1)\n",
    "        stored_vectors = np.array(self.vectors)\n",
    "        \n",
    "        # Calculate similarities using scikit-learn's cosine_similarity\n",
    "        # This is more efficient than a manual loop for large datasets\n",
    "        similarities = cosine_similarity(query_vector, stored_vectors)[0]\n",
    "        \n",
    "        # Get the indices of the top k scores\n",
    "        top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "        \n",
    "        # Return top k results with scores\n",
    "        results = []\n",
    "        for idx in top_k_indices:\n",
    "            score = similarities[idx]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_all_documents(self):\n",
    "        \"\"\"\n",
    "        Get all documents in the store.\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: All documents\n",
    "        \"\"\"\n",
    "        return [{\"text\": text, \"metadata\": meta} for text, meta in zip(self.texts, self.metadata)]\n",
    "\n",
    "# --- Example Usage with Gemini ---\n",
    "\n",
    "def create_gemini_embeddings(texts, model=\"models/embedding-001\"):\n",
    "    \"\"\"Creates embeddings for text(s) using Gemini.\"\"\"\n",
    "    response = genai.embed_content(model=model, content=texts)\n",
    "    return response['embedding']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 1. Initialize Gemini client\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    " \n",
    "    try:\n",
    "        genai.configure(api_key=api_key)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during Gemini API configuration: {e}\")\n",
    "        exit()\n",
    "    \n",
    "    # 2. Initialize the vector store\n",
    "    store = SimpleVectorStore()\n",
    "\n",
    "    # 3. Define and embed some texts using Gemini\n",
    "    texts_to_add = [\n",
    "        \"The Pennsylvania Emergency Preparedness Guide details how to make a family emergency plan.\",\n",
    "        \"A Home Emergency Kit Checklist is available on pages 12-15.\",\n",
    "        \"The guide lists the top 10 emergencies, including floods, fires, and winter storms.\",\n",
    "        \"The phone number for the Central Pennsylvania Food Bank is (717) 564-1700.\"\n",
    "    ]\n",
    "    \n",
    "    embeddings = create_gemini_embeddings(texts_to_add)\n",
    "    \n",
    "    items = [{\"text\": t} for t in texts_to_add]\n",
    "    store.add_items(items, embeddings)\n",
    "    \n",
    "    print(\"Vector store populated with Gemini embeddings.\")\n",
    "    \n",
    "    # 4. Perform a similarity search with a new query\n",
    "    query_text = \"how to create a family emergency plan\"\n",
    "    print(f\"\\nSearching for items similar to: '{query_text}'\")\n",
    "    query_embedding = create_gemini_embeddings(query_text)\n",
    "    \n",
    "    search_results = store.similarity_search_with_scores(query_embedding, k=2)\n",
    "\n",
    "    # 5. Print the search results\n",
    "    print(\"\\nTop 2 search results:\")\n",
    "    for result in search_results:\n",
    "        print(f\"  - Text: {result['text']}\")\n",
    "        print(f\"    Similarity: {result['similarity']:.4f}\")\n",
    "        print(f\"    Metadata: {result['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bm25_index(chunks):\n",
    "    \"\"\"\n",
    "    Create a BM25 index from the given chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): List of text chunks\n",
    "        \n",
    "    Returns:\n",
    "        BM25Okapi: A BM25 index\n",
    "    \"\"\"\n",
    "    # Extract text from each chunk\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    \n",
    "    # Tokenize each document by splitting on whitespace\n",
    "    tokenized_docs = [text.split() for text in texts]\n",
    "    \n",
    "    # Create the BM25 index using the tokenized documents\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Print the number of documents in the BM25 index\n",
    "    print(f\"Created BM25 index with {len(texts)} documents\")\n",
    "    \n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(bm25, chunks, query, k=5):\n",
    "    \"\"\"\n",
    "    Search the BM25 index with a query.\n",
    "    \n",
    "    Args:\n",
    "        bm25 (BM25Okapi): BM25 index\n",
    "        chunks (List[Dict]): List of text chunks\n",
    "        query (str): Query string\n",
    "        k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results with scores\n",
    "    \"\"\"\n",
    "    # Tokenize the query by splitting it into individual words\n",
    "    query_tokens = query.split()\n",
    "    \n",
    "    # Get BM25 scores for the query tokens against the indexed documents\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Initialize an empty list to store results with their scores\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over the scores and corresponding chunks\n",
    "    for i, score in enumerate(scores):\n",
    "        # Create a copy of the metadata to avoid modifying the original\n",
    "        metadata = chunks[i].get(\"metadata\", {}).copy()\n",
    "        # Add index to metadata\n",
    "        metadata[\"index\"] = i\n",
    "        \n",
    "        results.append({\n",
    "            \"text\": chunks[i][\"text\"],\n",
    "            \"metadata\": metadata,  # Add metadata with index\n",
    "            \"bm25_score\": float(score)\n",
    "        })\n",
    "    \n",
    "    # Sort the results by BM25 score in descending order\n",
    "    results.sort(key=lambda x: x[\"bm25_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top k results\n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def fusion_retrieval(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Perform fusion retrieval combining vector-based and BM25 search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "        chunks (List[Dict]): Original text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of results to return\n",
    "        alpha (float): Weight for vector scores (0-1), where 1-alpha is BM25 weight\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results based on combined scores\n",
    "    \"\"\"\n",
    "    print(f\"Performing fusion retrieval for query: {query}\")\n",
    "    \n",
    "    # Define small epsilon to avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Get vector search results\n",
    "    query_embedding = create_embeddings(query)\n",
    "    vector_results = vector_store.similarity_search_with_scores(query_embedding, k=len(chunks))\n",
    "    \n",
    "    # Get BM25 search results\n",
    "    bm25_results = bm25_search(bm25_index, chunks, query, k=len(chunks))\n",
    "    \n",
    "    # Create dictionaries to map document index to score\n",
    "    vector_scores_dict = {result[\"metadata\"][\"index\"]: result[\"similarity\"] for result in vector_results}\n",
    "    bm25_scores_dict = {result[\"metadata\"][\"index\"]: result[\"bm25_score\"] for result in bm25_results}\n",
    "    \n",
    "    # Ensure all documents have scores for both methods\n",
    "    all_docs = vector_store.get_all_documents()\n",
    "    combined_results = []\n",
    "    \n",
    "    for i, doc in enumerate(all_docs):\n",
    "        vector_score = vector_scores_dict.get(i, 0.0)\n",
    "        bm25_score = bm25_scores_dict.get(i, 0.0)\n",
    "        combined_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"vector_score\": vector_score,\n",
    "            \"bm25_score\": bm25_score,\n",
    "            \"index\": i\n",
    "        })\n",
    "    \n",
    "    # Extract scores as arrays\n",
    "    vector_scores = np.array([doc[\"vector_score\"] for doc in combined_results])\n",
    "    bm25_scores = np.array([doc[\"bm25_score\"] for doc in combined_results])\n",
    "    \n",
    "    # Normalize scores\n",
    "    norm_vector_scores = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "    norm_bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n",
    "    \n",
    "    # Compute combined scores\n",
    "    combined_scores = alpha * norm_vector_scores + (1 - alpha) * norm_bm25_scores\n",
    "    \n",
    "    # Add combined scores to results\n",
    "    for i, score in enumerate(combined_scores):\n",
    "        combined_results[i][\"combined_score\"] = float(score)\n",
    "    \n",
    "    # Sort by combined score (descending)\n",
    "    combined_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "    \n",
    "    # Return top k results\n",
    "    top_results = combined_results[:k]\n",
    "    \n",
    "    print(f\"Retrieved {len(top_results)} documents with fusion retrieval\")\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming the following Gemini-compatible functions/classes are defined elsewhere:\n",
    "# - create_embeddings(query)\n",
    "# - bm25_search(bm25_index, chunks, query, k)\n",
    "# - SimpleVectorStore (with methods similarity_search_with_scores and get_all_documents)\n",
    "\n",
    "def fusion_retrieval(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Perform fusion retrieval combining vector-based and BM25 search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "        chunks (List[Dict]): Original text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of results to return\n",
    "        alpha (float): Weight for vector scores (0-1), where 1-alpha is BM25 weight\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results based on combined scores\n",
    "    \"\"\"\n",
    "    print(f\"Performing fusion retrieval for query: {query}\")\n",
    "    \n",
    "    # Define small epsilon to avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Get vector search results\n",
    "    query_embedding = create_embeddings(query)\n",
    "    vector_results = vector_store.similarity_search_with_scores(query_embedding, k=len(chunks))\n",
    "    \n",
    "    # Get BM25 search results\n",
    "    bm25_results = bm25_search(bm25_index, chunks, query, k=len(chunks))\n",
    "    \n",
    "    # Create dictionaries to map document index to score\n",
    "    vector_scores_dict = {result[\"metadata\"][\"index\"]: result[\"similarity\"] for result in vector_results}\n",
    "    bm25_scores_dict = {result[\"metadata\"][\"index\"]: result[\"bm25_score\"] for result in bm25_results}\n",
    "    \n",
    "    # Ensure all documents have scores for both methods\n",
    "    all_docs = vector_store.get_all_documents()\n",
    "    combined_results = []\n",
    "    \n",
    "    for i, doc in enumerate(all_docs):\n",
    "        vector_score = vector_scores_dict.get(i, 0.0)\n",
    "        bm25_score = bm25_scores_dict.get(i, 0.0)\n",
    "        combined_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"vector_score\": vector_score,\n",
    "            \"bm25_score\": bm25_score,\n",
    "            \"index\": i\n",
    "        })\n",
    "    \n",
    "    # Extract scores as arrays\n",
    "    vector_scores = np.array([doc[\"vector_score\"] for doc in combined_results])\n",
    "    bm25_scores = np.array([doc[\"bm25_score\"] for doc in combined_results])\n",
    "    \n",
    "    # Normalize scores\n",
    "    norm_vector_scores = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "    norm_bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n",
    "    \n",
    "    # Compute combined scores\n",
    "    combined_scores = alpha * norm_vector_scores + (1 - alpha) * norm_bm25_scores\n",
    "    \n",
    "    # Add combined scores to results\n",
    "    for i, score in enumerate(combined_scores):\n",
    "        combined_results[i][\"combined_score\"] = float(score)\n",
    "    \n",
    "    # Sort by combined score (descending)\n",
    "    combined_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "    \n",
    "    # Return top k results\n",
    "    top_results = combined_results[:k]\n",
    "    \n",
    "    print(f\"Retrieved {len(top_results)} documents with fusion retrieval\")\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Assume genai.configure(api_key=\"YOUR_API_KEY\") has been called.\n",
    "\n",
    "def generate_response(query, context, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and context using a Gemini model.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context from retrieved documents\n",
    "        model (str): The model to use for response generation. Defaults to \"gemini-2.0-flash\".\n",
    "\n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # For Gemini, it's often best to combine the system prompt with the user's prompt\n",
    "    # into a single, cohesive instruction to guide the model's behavior.\n",
    "\n",
    "    # Combine the system prompt with the user's query and context into a single prompt string.\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI assistant. Answer the user's question based on the provided context. \n",
    "    If the context doesn't contain relevant information to answer the question fully, acknowledge this limitation.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Please answer the question based on the provided context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the Gemini GenerativeModel\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "\n",
    "    # Generate the response using the Gemini API\n",
    "    try:\n",
    "        response = model_instance.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.3 # Set the temperature for response generation\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Return the generated response, stripping any leading/trailing whitespace\n",
    "        return response.text.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Handle potential errors from the API call\n",
    "        return f\"An error occurred while generating the response: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Answer a query using fusion RAG with Gemini-compatible functions.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results including retrieved documents and response\n",
    "    \"\"\"\n",
    "    # Retrieve documents using fusion retrieval method\n",
    "    retrieved_docs = fusion_retrieval(query, chunks, vector_store, bm25_index, k=k, alpha=alpha)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Retrieval Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def vector_only_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only vector-based RAG with Gemini-compatible functions.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Retrieve documents using vector-based similarity search\n",
    "    retrieved_docs = vector_store.similarity_search_with_scores(query_embedding, k=k)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_only_rag(query, chunks, bm25_index, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only BM25-based RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Retrieve documents using BM25 search\n",
    "    retrieved_docs = bm25_search(bm25_index, chunks, query, k=k)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_methods(query, chunks, vector_store, bm25_index, k=5, alpha=0.5, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare different retrieval methods for a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores in fusion retrieval\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Comparing retrieval methods for query: {query} ===\\n\")\n",
    "    \n",
    "    # Run vector-only RAG\n",
    "    print(\"\\nRunning vector-only RAG...\")\n",
    "    vector_result = vector_only_rag(query, vector_store, k)\n",
    "    \n",
    "    # Run BM25-only RAG\n",
    "    # This assumes a Gemini-compatible `bm25_only_rag` helper function exists.\n",
    "    print(\"\\nRunning BM25-only RAG...\")\n",
    "    bm25_result = bm25_only_rag(query, chunks, bm25_index, k)\n",
    "    \n",
    "    # Run fusion RAG\n",
    "    # This assumes a Gemini-compatible `answer_with_fusion_rag` helper function exists.\n",
    "    print(\"\\nRunning fusion RAG...\")\n",
    "    fusion_result = answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k, alpha)\n",
    "    \n",
    "    # Compare responses from different retrieval methods\n",
    "    # This assumes a Gemini-compatible `evaluate_responses` helper function exists.\n",
    "    print(\"\\nComparing responses...\")\n",
    "    comparison = evaluate_responses(\n",
    "        query, \n",
    "        vector_result[\"response\"], \n",
    "        bm25_result[\"response\"], \n",
    "        fusion_result[\"response\"],\n",
    "        reference_answer\n",
    "    )\n",
    "    \n",
    "    # Return the comparison results\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"vector_result\": vector_result,\n",
    "        \"bm25_result\": bm25_result,\n",
    "        \"fusion_result\": fusion_result,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def evaluate_responses(query, vector_response, bm25_response, fusion_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate the responses from different retrieval methods using a Gemini model.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_response (str): Response from vector-only RAG\n",
    "        bm25_response (str): Response from BM25-only RAG\n",
    "        fusion_response (str): Response from fusion RAG\n",
    "        reference_answer (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Evaluation of responses\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from three different retrieval approaches:\n",
    "    1. Vector-based retrieval: Uses semantic similarity for document retrieval\n",
    "    2. BM25 keyword retrieval: Uses keyword matching for document retrieval\n",
    "    3. Fusion retrieval: Combines both vector and keyword approaches\n",
    "\n",
    "    Evaluate the responses based on:\n",
    "    - Relevance to the query\n",
    "    - Factual correctness\n",
    "    - Comprehensiveness\n",
    "    - Clarity and coherence\"\"\"\n",
    "\n",
    "    # User prompt containing the query and responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "    Vector-based response:\n",
    "    {vector_response}\n",
    "\n",
    "    BM25 keyword response:\n",
    "    {bm25_response}\n",
    "\n",
    "    Fusion response:\n",
    "    {fusion_response}\n",
    "    \"\"\"\n",
    "\n",
    "    # Add reference answer to the prompt if provided\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "        Reference answer:\n",
    "        {reference_answer}\n",
    "        \"\"\"\n",
    "\n",
    "    # Add instructions for detailed comparison to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "    Please provide a detailed comparison of these three responses. Which approach performed best for this query and why?\n",
    "    Be specific about the strengths and weaknesses of each approach for this particular query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine system and user prompts into a single prompt for Gemini\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    # Initialize the Gemini GenerativeModel\n",
    "    model_instance = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    \n",
    "    try:\n",
    "        # Generate the evaluation using the Gemini API\n",
    "        response = model_instance.generate_content(\n",
    "            full_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.0 # Set a low temperature for a more objective evaluation\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Return the generated evaluation content\n",
    "        return response.text.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during evaluation: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the following Gemini-compatible functions are defined elsewhere:\n",
    "# - process_document(pdf_path)\n",
    "# - compare_retrieval_methods(...)\n",
    "# - generate_overall_analysis(results)\n",
    "\n",
    "def evaluate_fusion_retrieval(pdf_path, test_queries, reference_answers=None, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate fusion retrieval compared to other methods.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores in fusion retrieval\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING FUSION RETRIEVAL ===\\n\")\n",
    "    \n",
    "    # Process the document to extract text, create chunks, and build vector and BM25 indices\n",
    "    # Assumes process_document returns chunks, vector_store, and bm25_index\n",
    "    chunks, vector_store, bm25_index = process_document(pdf_path)\n",
    "    \n",
    "    # Initialize a list to store results for each query\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each test query\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Get the reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # Compare retrieval methods for the current query\n",
    "        comparison = compare_retrieval_methods(\n",
    "            query, \n",
    "            chunks, \n",
    "            vector_store, \n",
    "            bm25_index, \n",
    "            k=k, \n",
    "            alpha=alpha,\n",
    "            reference_answer=reference\n",
    "        )\n",
    "        \n",
    "        # Append the comparison results to the results list\n",
    "        results.append(comparison)\n",
    "        \n",
    "        # Print the responses from different retrieval methods\n",
    "        print(\"\\n=== Vector-based Response ===\")\n",
    "        print(comparison[\"vector_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== BM25 Response ===\")\n",
    "        print(comparison[\"bm25_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Fusion Response ===\")\n",
    "        print(comparison[\"fusion_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Comparison ===\")\n",
    "        print(comparison[\"comparison\"])\n",
    "    \n",
    "    # Generate an overall analysis of the fusion retrieval performance\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    # Return the results and overall analysis\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "\n",
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of fusion retrieval using a Gemini model.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from evaluating queries\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems.\n",
    "    Based on multiple test queries, provide an overall analysis comparing three retrieval approaches:\n",
    "    1. Vector-based retrieval (semantic similarity)\n",
    "    2. BM25 keyword retrieval (keyword matching)\n",
    "    3. Fusion retrieval (combination of both)\n",
    "\n",
    "    Focus on:\n",
    "    1. Types of queries where each approach performs best\n",
    "    2. Overall strengths and weaknesses of each approach\n",
    "    3. How fusion retrieval balances the trade-offs\n",
    "    4. Recommendations for when to use each approach\"\"\"\n",
    "\n",
    "    # Create a summary of evaluations for each query\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        # Safely get a preview of the comparison summary\n",
    "        comparison_summary = result.get('comparison', 'No comparison available.')\n",
    "        evaluations_summary += f\"Comparison Summary: {comparison_summary[:200]}...\\n\\n\"\n",
    "\n",
    "    # User prompt containing the evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations of different retrieval methods across {len(results)} queries,\n",
    "    provide an overall analysis comparing these three approaches:\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    Please provide a comprehensive analysis of vector-based, BM25, and fusion retrieval approaches,\n",
    "    highlighting when and why fusion retrieval provides advantages over the individual methods.\"\"\"\n",
    "\n",
    "    # Combine system and user prompts into a single prompt for Gemini\n",
    "    full_prompt = f\"{system_prompt}\\n\\n{user_prompt}\"\n",
    "\n",
    "    # Initialize the Gemini GenerativeModel\n",
    "    model_instance = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "    \n",
    "    try:\n",
    "        # Generate the overall analysis using the Gemini API\n",
    "        response = model_instance.generate_content(\n",
    "            full_prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=1 # A higher temperature allows for more creative analysis\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Return the generated analysis content\n",
    "        return response.text.strip()\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during analysis generation: {e}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Fusion Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "Average Score: 0.0\n",
      "\n",
      "Q: Is it safe to drink the tap water in 17104 after the flood?\n",
      "AI Answer: This document does not contain information about the safety of tap water in the 17104 zip code after a flood.\n",
      "Reference: Tap water may be contaminated after a flood. Boil for at least 1 minute or use bottled water.\n",
      "\n",
      "Call 2-1-1 to find water testing kits or bottled water stations near you.\n",
      "Score: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fitz  # PyMuPDF for PDF reading\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# -----------------------------\n",
    "# Load .env file with GOOGLE_API_KEY\n",
    "# -----------------------------\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize Gemini client\n",
    "# -----------------------------\n",
    "client = OpenAI(\n",
    "    api_key=GOOGLE_API_KEY,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Function to read PDF\n",
    "# -----------------------------\n",
    "def load_pdf_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "# -----------------------------\n",
    "# Function to run Gemini retrieval + evaluation\n",
    "# -----------------------------\n",
    "def run_gemini_retrieval(pdf_path, test_queries, reference_answers=None):\n",
    "    document_text = load_pdf_text(pdf_path)\n",
    "\n",
    "    results = []\n",
    "    for i, query in enumerate(test_queries):\n",
    "        # Prompt Gemini\n",
    "        prompt = f\"\"\"\n",
    "You are a disaster resource assistant. \n",
    "Answer the following question ONLY using this document:\n",
    "\n",
    "DOCUMENT:\n",
    "{document_text}\n",
    "\n",
    "QUESTION:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers based on provided documents only.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "        ai_answer = response.choices[0].message.content.strip()\n",
    "\n",
    "        # Evaluate vs. reference answer\n",
    "        score = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            ref = reference_answers[i].strip().lower()\n",
    "            ai_clean = ai_answer.strip().lower()\n",
    "            score = 1 if ref in ai_clean else 0\n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"ai_answer\": ai_answer,\n",
    "            \"reference_answer\": reference_answers[i] if reference_answers else None,\n",
    "            \"score\": score\n",
    "        })\n",
    "\n",
    "    avg_score = sum(r[\"score\"] for r in results if r[\"score\"] is not None) / len(results) if any(r[\"score\"] is not None for r in results) else None\n",
    "\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": f\"Average Score: {avg_score}\" if avg_score is not None else \"No reference answers provided.\"\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Path & Queries\n",
    "# -----------------------------\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/RAG Google/Resources.pdf\"\n",
    "test_queries = [\"Is it safe to drink the tap water in 17104 after the flood?\"]\n",
    "reference_answers = [\n",
    "    \"Tap water may be contaminated after a flood. Boil for at least 1 minute or use bottled water.\\n\\nCall 2-1-1 to find water testing kits or bottled water stations near you.\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Run Evaluation\n",
    "# -----------------------------\n",
    "evaluation_results = run_gemini_retrieval(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Print Results\n",
    "# -----------------------------\n",
    "print(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])\n",
    "for res in evaluation_results[\"results\"]:\n",
    "    print(f\"\\nQ: {res['query']}\\nAI Answer: {res['ai_answer']}\\nReference: {res['reference_answer']}\\nScore: {res['score']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
