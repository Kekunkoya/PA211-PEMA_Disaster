{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Adaptive Retrieval for Enhanced RAG Systems\n",
    "\n",
    "In this notebook, I implement an Adaptive Retrieval system that dynamically selects the most appropriate retrieval strategy based on the type of query. This approach significantly enhances our RAG system's ability to provide accurate and relevant responses across a diverse range of questions.\n",
    "\n",
    "Different questions demand different retrieval strategies. Our system:\n",
    "\n",
    "1. Classifies the query type (Factual, Analytical, Opinion, or Contextual)\n",
    "2. Selects the appropriate retrieval strategy\n",
    "3. Executes specialized retrieval techniques\n",
    "4. Generates a tailored response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agency Name *\n",
      "Site Name *\n",
      "Service Name *\n",
      "Site Main Phone \n",
      "Number\n",
      "Service Eligibility\n",
      "Lancaster County \n",
      "Housing and \n",
      "Redevelopment \n",
      "Authorities\n",
      "Lancaster County Housing \n",
      "and Redevelopment \n",
      "Authorities\n",
      "Home Repair Program\n",
      "717-394-0793\n",
      "Based on annual gross income, \n",
      "according to family size; Available \n",
      "equity in home\n",
      "Housing and Repairs\n",
      "Lancaster County \n",
      "Housing and \n",
      "Redevelopment \n",
      "Authorities\n",
      "Lancaster County Housing \n",
      "and Redevelopment \n",
      "Authorities\n",
      "Public Infrastructure and \n",
      "Community Facilities Grant \n",
      "Administration\n",
      "717-394-0793\n",
      "Local municipalities outside the \n",
      "city of Lancaster\n",
      "\n",
      "Lancaster County \n",
      "Housing and \n",
      "Redevelopment \n",
      "Authorities\n",
      "Lancaster County Housing \n",
      "and Redevelopment \n",
      "Authorities\n",
      "Rental Housing Program\n",
      "717-394-0793\n",
      "Open to rental housing \n",
      "developers only; Properties must \n",
      "be located in Lancaster County, \n",
      "outside of Lancaster City\n",
      "Lancaster County \n",
      "Housing and \n",
      "Redevelopment \n",
      "Authorities\n",
      "Lancaster County Housing \n",
      "and Redevelopment \n",
      "Authorities\n",
      "Grant Administration\n",
      "717-394-0793\n",
      "Agencies that serve Lancaster \n",
      "county (non-city) residents\n",
      "\n",
      "Low-income families, elderly (62 \n",
      "years or older), and disabled \n",
      "persons who live or wish to live in \n",
      "Lancaster County\n",
      "Elizabethtown \n",
      "Community Housing \n",
      "and Outreach \n",
      "Services\n",
      "Community Place on \n",
      "Washington\n",
      "Enrichment Center\n",
      "717-361-0740\n",
      "Lancaster County residents\n",
      "Lancaster County \n",
      "Housing and \n",
      "Redevelopment \n",
      "Authorities\n",
      "Lancaster County Housing \n",
      "and Redevelopment \n",
      "Authorities\n",
      "Section 8 Housing Choice \n",
      "Vouchers\n",
      "717-394-0793\n",
      "\n",
      "Pennsylvania Housing \n",
      "Finance Agency\n",
      "Pennsylvania Housing \n",
      "Finance Agency\n",
      "Homebuyer Counseling\n",
      "Residents Borrowers that are \n",
      "considering a PHFA loan product \n",
      "and have a FICO credit score \n",
      "lower than 660 are required to \n",
      "complete a course prior to closing \n",
      "on their loan.\n",
      "Pennsylvania Utility Law \n",
      "Project (PULP) Hotline\n",
      "844-645-2500\n",
      "Serves individuals and families in \n",
      "Pennsylvania who are facing a \n",
      "utility shutoff or are already \n",
      "without service.\n",
      "Housing \n",
      "Discrimination \n",
      "Hotline\n",
      "Harrisburg\n",
      "Housing Discrimination \n",
      "Hotline\n",
      "No limitations or restrictions\n",
      "Regional Housing \n",
      "Legal Services\n",
      "Pennsylvania Utility Law \n",
      "Project Office, Harrisburg\n",
      "\n",
      "Pennsylvania Housing \n",
      "Finance Agency\n",
      "Pennsylvania Housing \n",
      "Finance Agency\n",
      "Homeowner's Emergency \n",
      "Mortgage Assistance \n",
      "Program (HEMAP)\n",
      "A) Residents of Pennsylvania who \n",
      "are homeowners with mortgage \n",
      "delinquencies caused by \n",
      "circumstances beyond their \n",
      "control B) Must not be in \n",
      "foreclosure C) Must have the \n",
      "ability to regain financial stability \n",
      "within 24 months.\n",
      "Manheim Central \n",
      "Food Pantry\n",
      "Manheim\n",
      "Food Pantry\n",
      "717-664-1097\n",
      "Based on family size\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster County Food Hub\n",
      "Daytime Transition Center\n",
      "717-291-2261\n",
      "Homeless single indiviudals, can \n",
      "not accomodate families. \n",
      "Intended for those who are trying \n",
      "to make efforts to move to a more \n",
      "stabalized situation\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster County Food Hub\n",
      "Clothing Bank\n",
      "717-291-2261\n",
      "Anyone in need; Individuals can \n",
      "obtain clothing every 60 days if \n",
      "needed\n",
      "Food\n",
      "\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster County Food Hub\n",
      "Code Red: Extreme Heat \n",
      "Cooling Program\n",
      "717-291-2261\n",
      "Anyone in need\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster County Food Hub\n",
      "Warming Center\n",
      "717-291-2261\n",
      "Anyone in need\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster City various \n",
      "locations\n",
      "Street Outreach\n",
      "Homeless individuals\n",
      "Midwest Food Bank \n",
      "of Pennsylvania\n",
      "Middletown\n",
      "Food Bank\n",
      "717-614-8095\n",
      "Must be a feeding program, such \n",
      "as a local 501(c)(3) food pantry or \n",
      "soup kitchen.\n",
      "Central Pennsylvania \n",
      "Food Bank\n",
      "Harrisburg\n",
      "SNAP Outreach Program\n",
      "717-564-1700\n",
      "Gross monthly income limits \n",
      "specific to a household size\n",
      "Lancaster County \n",
      "Food Hub\n",
      "Lancaster County Food Hub\n",
      "Food Pantry\n",
      "717-291-2261\n",
      "Anyone in need, must sign a self-\n",
      "declaration of need each year. \n",
      "Caseworkers picking up on behalf \n",
      "of a recipient must sign a Proxy \n",
      "form giving the caseworker/case \n",
      "manager permission to pick up for \n",
      "them\n",
      "\n",
      "Ferris Wheel Clothing \n",
      "Bank\n",
      "Lititz\n",
      "Clothing Bank\n",
      "717-799-1933\n",
      "Anyone in need\n",
      "Northern Lebanon \n",
      "Clothing Closet\n",
      "Northern Lebanon Clothing \n",
      "Closet\n",
      "Clothing bank\n",
      "717-454-3697\n",
      "Northern Lebanon School District \n",
      "residents who make 225% or less \n",
      "of the federal poverty level, or is in \n",
      "crisis\n",
      "Southern York County \n",
      "Clothing Bank\n",
      "Saint John the Baptist \n",
      "Catholic Church\n",
      "Clothing Bank\n",
      "717-235-2156\n",
      "Residents of York County\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Cambria County EMA\n",
      "Emergency Management, \n",
      "Cambria County\n",
      "814-472-2050\n",
      "Cambria County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Central Pennsylvania \n",
      "Food Bank\n",
      "Harrisburg\n",
      "Senior Monthly Food Boxes\n",
      "717-564-1700\n",
      "Must be age 60 years or older and \n",
      "be at or between 131% and 185% \n",
      "of Federal Poverty Income \n",
      "Guidelines\n",
      "Emergency\n",
      "Clothing\n",
      "\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Armstrong County EMA\n",
      "Emergency Management, \n",
      "Armstrong County\n",
      "724-548-3429\n",
      "Armstrong County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Westmoreland County EMA\n",
      "Emergency Management, \n",
      "Westmoreland County\n",
      "724-600-7300\n",
      "Westmoreland County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Indiana County EMA\n",
      "Emergency Management, \n",
      "Indiana County\n",
      "724-349-9300\n",
      "Indiana County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Somerset County EMA\n",
      "Emergency Management, \n",
      "Somerset County\n",
      "814-445-1515\n",
      "Somerset County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Fayette County EMA\n",
      "Emergency Management, \n",
      "Fayette County\n",
      "724-430-1277\n",
      "Fayette County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Washington County EMA\n",
      "Emergency Management, \n",
      "Washington County\n",
      "724-228-6911\n",
      "Washington County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "Butler County residents, business \n",
      "owners, stakeholders, and \n",
      "visitors affected by a disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Allegheny County EMA\n",
      "Emergency Management, \n",
      "Allegheny County\n",
      "412-473-2550\n",
      "Allegheny County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Greene County EMA\n",
      "Emergency Management, \n",
      "Greene County\n",
      "724-627-5387\n",
      "Greene County residents, \n",
      "business owners, stakeholders, \n",
      "and visitors affected by a \n",
      "disaster.\n",
      "Southwestern PA \n",
      "Emergency \n",
      "Management \n",
      "Agencies\n",
      "Butler County EMA\n",
      "Emergency Management, \n",
      "Butler County\n",
      "724-284-5211\n",
      "\n",
      "Citizens' Ambulance \n",
      "Service\n",
      "Indiana\n",
      "Non-Emergency Medical \n",
      "Transportation\n",
      "724-349-5511\n",
      "Services available in Indiana \n",
      "County and portions of \n",
      "Armstrong, Clearfield, and \n",
      "Westmoreland counties. Call for \n",
      "additional information.\n",
      "No limitations or restrictions\n",
      "Agencies\n",
      "disaster.\n",
      "Citizens' Ambulance \n",
      "Service\n",
      "Indiana\n",
      "Paramedic/EMT and \n",
      "Ambulance Services\n",
      "724-349-5511\n",
      "Serves Indiana County, and \n",
      "portions of Armstrong, \n",
      "Westmoreland, and Clearfield \n",
      "counties.\n",
      "Coudersport \n",
      "Volunteer Ambulance \n",
      "Association\n",
      "Coudersport Volunteer \n",
      "Ambulance Association\n",
      "Coudersport Volunteer \n",
      "Ambulance Association\n",
      "814-274-7411\n",
      "Penn Township\n",
      "Citizens' Ambulance \n",
      "Service\n",
      "Indiana\n",
      "Child Car Seats and \n",
      "Inspections\n",
      "724-349-5511\n",
      "Serves Indiana County and \n",
      "portions of Armstrong, Clearfield, \n",
      "and Westmoreland counties. Call \n",
      "for additional details.\n",
      "Ambulance\n",
      "\n",
      "Noga Ambulance\n",
      "New Castle\n",
      "Non-Emergency Medical \n",
      "Transportation\n",
      "724-652-8300\n",
      "Residents of Ellwood City, New \n",
      "Castle, and New Wilmington\n",
      "Noga Ambulance\n",
      "New Castle\n",
      "Safety Programs and \n",
      "Trainings\n",
      "724-652-8300\n",
      "Varies by program; call for details.\n",
      "Penn Township \n",
      "Ambulance \n",
      "Association\n",
      "Irwin\n",
      "Child Passenger Safety Seats 724-744-4112\n",
      "Families with children age 10 and \n",
      "younger\n",
      "\n",
      "Taxonomy *\n",
      "Furnaces - BM-3000.0500-450.25\n",
      "Home Rehabilitation Loans - BH-\n",
      "3000.3550-360\n",
      "Home Maintenance and Minor \n",
      "Repair Grants/Loans - PH-\n",
      "3300.2740\n",
      "Weatherization Programs - BH-\n",
      "3000.1800-950\n",
      "Community Development Block \n",
      "Grant Agencies - TD-1100.1500\n",
      "Housing Authorities - BH-\n",
      "8300.3000\n",
      "Redevelopment Programs - TB-\n",
      "7000\n",
      "\n",
      "Charities/Grantmaking \n",
      "Organizations - TD-1200\n",
      "Financial Management Support - \n",
      "TP-2100\n",
      "Housing Authorities - BH-\n",
      "8300.3000\n",
      "Housing Authorities - BH-\n",
      "8300.3000\n",
      "Housing Development - TB-3000\n",
      "Redevelopment Programs - TB-\n",
      "7000\n",
      "Housing Authorities - BH-\n",
      "8300.3000\n",
      "\n",
      "Section 8 Housing Choice \n",
      "Vouchers - BH-7000.4600-700\n",
      "Homeless Drop In Centers - BH-\n",
      "1800.3500\n",
      "Clothing Vouchers - BM-\n",
      "6500.1500-130\n",
      "Food Vouchers - BD-1800.2250\n",
      "Personal/Grooming Supplies - BM-\n",
      "6500.6500-650\n",
      "Consumer Complaints - DD-1500\n",
      "\n",
      "Housing Advocacy Groups - TD-\n",
      "1600.2800\n",
      "Utility Bill Payment Plan \n",
      "Negotiation Assistance - BV-\n",
      "8900.9120\n",
      "Utility Disconnection Protection - \n",
      "BV-8900.9220\n",
      "Homebuyer/Home Purchase \n",
      "Counseling - BH-3700.3000\n",
      "\n",
      "Foreclosure Prevention Loan \n",
      "Modification/Refinancing \n",
      "Programs - BH-3500.3400-300\n",
      "Food Pantries - BD-1800.2000\n",
      "Homeless Drop In Centers - BH-\n",
      "1800.3500\n",
      "General Clothing Donation \n",
      "Programs - TI-1800.1500-250\n",
      "General Clothing Provision - BM-\n",
      "6500.1500-250\n",
      "\n",
      "Food Donation Programs - TI-\n",
      "1800.2000\n",
      "Grocery Ordering/Delivery - BD-\n",
      "2400.2590\n",
      "Food Pantries - BD-1800.2000\n",
      "Extreme Heat Cooling Programs - \n",
      "TH-2600.1900\n",
      "Extreme Cold Warming Centers - \n",
      "TH-2600.1880\n",
      "Street Outreach Programs - PH-\n",
      "8000\n",
      "Food Banks/Food Distribution \n",
      "Warehouses - BD-1875.2000\n",
      "Benefits Screening - PH-0700\n",
      "\n",
      "Commodity Supplemental Food \n",
      "Program - BD-1800.1500\n",
      "Brown Bag Food Programs - BD-\n",
      "1800.1000\n",
      "Food Pantries - BD-1800.2000\n",
      "Food Lines - BD-1800.1900\n",
      "General Clothing Provision - BM-\n",
      "6500.1500-250\n",
      "General Clothing Provision - BM-\n",
      "6500.1500-250\n",
      "General Clothing Provision - BM-\n",
      "6500.1500-250\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Disaster Management \n",
      "Organizations - TH-1500\n",
      "\n",
      "Post Disaster Damage Reporting - \n",
      "TH-2900.6500-600\n",
      "Paramedic/EMT Services - LD-\n",
      "6500\n",
      "Emergency Medical Transportation \n",
      "- LD-1500\n",
      "Emergency Medical Transportation \n",
      "- LD-1500\n",
      "Paramedic/EMT Services - LD-\n",
      "6500\n",
      "Non-Emergency Medical \n",
      "Transportation - BT-4500.6500-\n",
      "500\n",
      "Child Passenger Safety Seat \n",
      "Inspections - JR-8200.8500-160\n",
      "Child Passenger Safety Seats - JR-\n",
      "8400.1500\n",
      "Child Passenger Safety Education - \n",
      "JR-8200.8500-150\n",
      "\n",
      "Child Passenger Safety Seat \n",
      "Inspections - JR-8200.8500-160\n",
      "Child Passenger Safety Seats - JR-\n",
      "8400.1500\n",
      "Non-Emergency Medical \n",
      "Transportation - BT-4500.6500-\n",
      "500\n",
      "First Aid Instruction - LH-\n",
      "2700.2000\n",
      "Bicycle Safety Education - JR-\n",
      "8200.8500-100\n",
      "Child Passenger Safety Seat \n",
      "Inspections - JR-8200.8500-160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import fitz  # pip install PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the entire PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "# Example usage:\n",
    "pdf_file = \"/Users/kekunkoya/Desktop/RAG Project/Resources.pdf\"\n",
    "text = extract_text_from_pdf(pdf_file)\n",
    "print(text)  # print the first 500 characters to verify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation\n",
    "We'll create a basic vector store to manage document chunks and their embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "        \"\"\"\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store original texts\n",
    "        self.metadata = []  # List to store metadata for each text\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "\n",
    "        Args:\n",
    "        text (str): The original text.\n",
    "        embedding (List[float]): The embedding vector.\n",
    "        metadata (dict, optional): Additional metadata.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Convert embedding to numpy array and add to vectors list\n",
    "        self.texts.append(text)  # Add the original text to texts list\n",
    "        self.metadata.append(metadata or {})  # Add metadata to metadata list, default to empty dict if None\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): Query embedding vector.\n",
    "        k (int): Number of results to return.\n",
    "        filter_func (callable, optional): Function to filter results.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: Top k most similar items with their texts and metadata.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # Apply filter if provided\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Add the text\n",
    "                \"metadata\": self.metadata[idx],  # Add the metadata\n",
    "                \"similarity\": score  # Add the similarity score\n",
    "            })\n",
    "        \n",
    "        return results  # Return the list of top k results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text.\n",
    "\n",
    "    Args:\n",
    "    text (str or List[str]): The input text(s) for which embeddings are to be created.\n",
    "    model (str): The model to be used for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "    List[float] or List[List[float]]: The embedding vector(s).\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs by converting string input to a list\n",
    "    input_text = text if isinstance(text, list) else [text]\n",
    "    \n",
    "    # Create embeddings for the input text using the specified model\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=input_text\n",
    "    )\n",
    "    \n",
    "    # If the input was a single string, return just the first embedding\n",
    "    if isinstance(text, str):\n",
    "        return response.data[0].embedding\n",
    "    \n",
    "    # Otherwise, return all embeddings for the list of texts\n",
    "    return [item.embedding for item in response.data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for use with adaptive retrieval.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "    chunk_size (int): Size of each chunk in characters.\n",
    "    chunk_overlap (int): Overlap between chunks in characters.\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: Document chunks and vector store.\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    print(\"Extracting text from PDF...\")\n",
    "    extracted_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Chunk the extracted text\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # Create embeddings for the text chunks\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    \n",
    "    # Initialize the vector store\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # Add each chunk and its embedding to the vector store with metadata\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\"index\": i, \"source\": pdf_path}\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    \n",
    "    # Return the chunks and the vector store\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(query, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Classify a query into one of four categories: Factual, Analytical, Opinion, or Contextual.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        model (str): LLM model to use\n",
    "        \n",
    "    Returns:\n",
    "        str: Query category\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI's classification\n",
    "    system_prompt = \"\"\"You are an expert at classifying questions. \n",
    "        Classify the given query into exactly one of these categories:\n",
    "        - Factual: Queries seeking specific, verifiable information.\n",
    "        - Analytical: Queries requiring comprehensive analysis or explanation.\n",
    "        - Opinion: Queries about subjective matters or seeking diverse viewpoints.\n",
    "        - Contextual: Queries that depend on user-specific context.\n",
    "\n",
    "        Return ONLY the category name, without any explanation or additional text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the user prompt with the query to be classified\n",
    "    user_prompt = f\"Classify this query: {query}\"\n",
    "    \n",
    "    # Generate the classification response from the AI model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract and strip the category from the response\n",
    "    category = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Define the list of valid categories\n",
    "    valid_categories = [\"Factual\", \"Analytical\", \"Opinion\", \"Contextual\"]\n",
    "    \n",
    "    # Ensure the returned category is valid\n",
    "    for valid in valid_categories:\n",
    "        if valid in category:\n",
    "            return valid\n",
    "    \n",
    "    # Default to \"Factual\" if classification fails\n",
    "    return \"Factual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Specialized Retrieval Strategies\n",
    "### 1. Factual Strategy - Focus on Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factual_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for factual queries focusing on precision.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Factual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # Use LLM to enhance the query for better precision\n",
    "    system_prompt = \"\"\"You are an expert at enhancing search queries.\n",
    "        Your task is to reformulate the given factual query to make it more precise and \n",
    "        specific for information retrieval. Focus on key entities and their relationships.\n",
    "\n",
    "        Provide ONLY the enhanced query without any explanation.\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"Enhance this factual query: {query}\"\n",
    "    \n",
    "    # Generate the enhanced query using the LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract and print the enhanced query\n",
    "    enhanced_query = response.choices[0].message.content.strip()\n",
    "    print(f\"Enhanced query: {enhanced_query}\")\n",
    "    \n",
    "    # Create embeddings for the enhanced query\n",
    "    query_embedding = create_embeddings(enhanced_query)\n",
    "    \n",
    "    # Perform initial similarity search to retrieve documents\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # Initialize a list to store ranked results\n",
    "    ranked_results = []\n",
    "    \n",
    "    # Score and rank documents by relevance using LLM\n",
    "    for doc in initial_results:\n",
    "        relevance_score = score_document_relevance(enhanced_query, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"relevance_score\": relevance_score\n",
    "        })\n",
    "    \n",
    "    # Sort the results by relevance score in descending order\n",
    "    ranked_results.sort(key=lambda x: x[\"relevance_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top k results\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analytical Strategy - Comprehensive Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytical_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for analytical queries focusing on comprehensive coverage.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Analytical retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # Define the system prompt to guide the AI in generating sub-questions\n",
    "    system_prompt = \"\"\"You are an expert at breaking down complex questions.\n",
    "    Generate sub-questions that explore different aspects of the main analytical query.\n",
    "    These sub-questions should cover the breadth of the topic and help retrieve \n",
    "    comprehensive information.\n",
    "\n",
    "    Return a list of exactly 3 sub-questions, one per line.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the user prompt with the main query\n",
    "    user_prompt = f\"Generate sub-questions for this analytical query: {query}\"\n",
    "    \n",
    "    # Generate the sub-questions using the LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Extract and clean the sub-questions\n",
    "    sub_queries = response.choices[0].message.content.strip().split('\\n')\n",
    "    sub_queries = [q.strip() for q in sub_queries if q.strip()]\n",
    "    print(f\"Generated sub-queries: {sub_queries}\")\n",
    "    \n",
    "    # Retrieve documents for each sub-query\n",
    "    all_results = []\n",
    "    for sub_query in sub_queries:\n",
    "        # Create embeddings for the sub-query\n",
    "        sub_query_embedding = create_embeddings(sub_query)\n",
    "        # Perform similarity search for the sub-query\n",
    "        results = vector_store.similarity_search(sub_query_embedding, k=2)\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # Ensure diversity by selecting from different sub-query results\n",
    "    # Remove duplicates (same text content)\n",
    "    unique_texts = set()\n",
    "    diverse_results = []\n",
    "    \n",
    "    for result in all_results:\n",
    "        if result[\"text\"] not in unique_texts:\n",
    "            unique_texts.add(result[\"text\"])\n",
    "            diverse_results.append(result)\n",
    "    \n",
    "    # If we need more results to reach k, add more from initial results\n",
    "    if len(diverse_results) < k:\n",
    "        # Direct retrieval for the main query\n",
    "        main_query_embedding = create_embeddings(query)\n",
    "        main_results = vector_store.similarity_search(main_query_embedding, k=k)\n",
    "        \n",
    "        for result in main_results:\n",
    "            if result[\"text\"] not in unique_texts and len(diverse_results) < k:\n",
    "                unique_texts.add(result[\"text\"])\n",
    "                diverse_results.append(result)\n",
    "    \n",
    "    # Return the top k diverse results\n",
    "    return diverse_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Opinion Strategy - Diverse Perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opinion_retrieval_strategy(query, vector_store, k=4):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for opinion queries focusing on diverse perspectives.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Opinion retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # Define the system prompt to guide the AI in identifying different perspectives\n",
    "    system_prompt = \"\"\"You are an expert at identifying different perspectives on a topic.\n",
    "        For the given query about opinions or viewpoints, identify different perspectives \n",
    "        that people might have on this topic.\n",
    "\n",
    "        Return a list of exactly 3 different viewpoint angles, one per line.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the user prompt with the main query\n",
    "    user_prompt = f\"Identify different perspectives on: {query}\"\n",
    "    \n",
    "    # Generate the different perspectives using the LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # Extract and clean the viewpoints\n",
    "    viewpoints = response.choices[0].message.content.strip().split('\\n')\n",
    "    viewpoints = [v.strip() for v in viewpoints if v.strip()]\n",
    "    print(f\"Identified viewpoints: {viewpoints}\")\n",
    "    \n",
    "    # Retrieve documents representing each viewpoint\n",
    "    all_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        # Combine the main query with the viewpoint\n",
    "        combined_query = f\"{query} {viewpoint}\"\n",
    "        # Create embeddings for the combined query\n",
    "        viewpoint_embedding = create_embeddings(combined_query)\n",
    "        # Perform similarity search for the combined query\n",
    "        results = vector_store.similarity_search(viewpoint_embedding, k=2)\n",
    "        \n",
    "        # Mark results with the viewpoint they represent\n",
    "        for result in results:\n",
    "            result[\"viewpoint\"] = viewpoint\n",
    "        \n",
    "        # Add the results to the list of all results\n",
    "        all_results.extend(results)\n",
    "    \n",
    "    # Select a diverse range of opinions\n",
    "    # Ensure we get at least one document from each viewpoint if possible\n",
    "    selected_results = []\n",
    "    for viewpoint in viewpoints:\n",
    "        # Filter documents by viewpoint\n",
    "        viewpoint_docs = [r for r in all_results if r.get(\"viewpoint\") == viewpoint]\n",
    "        if viewpoint_docs:\n",
    "            selected_results.append(viewpoint_docs[0])\n",
    "    \n",
    "    # Fill remaining slots with highest similarity docs\n",
    "    remaining_slots = k - len(selected_results)\n",
    "    if remaining_slots > 0:\n",
    "        # Sort remaining docs by similarity\n",
    "        remaining_docs = [r for r in all_results if r not in selected_results]\n",
    "        remaining_docs.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "        selected_results.extend(remaining_docs[:remaining_slots])\n",
    "    \n",
    "    # Return the top k results\n",
    "    return selected_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Contextual Strategy - User Context Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextual_retrieval_strategy(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Retrieval strategy for contextual queries integrating user context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to return\n",
    "        user_context (str): Additional user context\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    print(f\"Executing Contextual retrieval strategy for: '{query}'\")\n",
    "    \n",
    "    # If no user context provided, try to infer it from the query\n",
    "    if not user_context:\n",
    "        system_prompt = \"\"\"You are an expert at understanding implied context in questions.\n",
    "For the given query, infer what contextual information might be relevant or implied \n",
    "but not explicitly stated. Focus on what background would help answering this query.\n",
    "\n",
    "Return a brief description of the implied context.\"\"\"\n",
    "\n",
    "        user_prompt = f\"Infer the implied context in this query: {query}\"\n",
    "        \n",
    "        # Generate the inferred context using the LLM\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Extract and print the inferred context\n",
    "        user_context = response.choices[0].message.content.strip()\n",
    "        print(f\"Inferred context: {user_context}\")\n",
    "    \n",
    "    # Reformulate the query to incorporate context\n",
    "    system_prompt = \"\"\"You are an expert at reformulating questions with context.\n",
    "    Given a query and some contextual information, create a more specific query that \n",
    "    incorporates the context to get more relevant information.\n",
    "\n",
    "    Return ONLY the reformulated query without explanation.\"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {user_context}\n",
    "\n",
    "    Reformulate the query to incorporate this context:\"\"\"\n",
    "    \n",
    "    # Generate the contextualized query using the LLM\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract and print the contextualized query\n",
    "    contextualized_query = response.choices[0].message.content.strip()\n",
    "    print(f\"Contextualized query: {contextualized_query}\")\n",
    "    \n",
    "    # Retrieve documents based on the contextualized query\n",
    "    query_embedding = create_embeddings(contextualized_query)\n",
    "    initial_results = vector_store.similarity_search(query_embedding, k=k*2)\n",
    "    \n",
    "    # Rank documents considering both relevance and user context\n",
    "    ranked_results = []\n",
    "    \n",
    "    for doc in initial_results:\n",
    "        # Score document relevance considering the context\n",
    "        context_relevance = score_document_context_relevance(query, user_context, doc[\"text\"])\n",
    "        ranked_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"similarity\": doc[\"similarity\"],\n",
    "            \"context_relevance\": context_relevance\n",
    "        })\n",
    "    \n",
    "    # Sort by context relevance and return top k results\n",
    "    ranked_results.sort(key=lambda x: x[\"context_relevance\"], reverse=True)\n",
    "    return ranked_results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Document Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document_relevance(query, document, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Score document relevance to a query using LLM.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        document (str): Document text\n",
    "        model (str): LLM model\n",
    "        \n",
    "    Returns:\n",
    "        float: Relevance score from 0-10\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the model on how to rate relevance\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance.\n",
    "        Rate the relevance of a document to a query on a scale from 0 to 10, where:\n",
    "        0 = Completely irrelevant\n",
    "        10 = Perfectly addresses the query\n",
    "\n",
    "        Return ONLY a numerical score between 0 and 10, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate document if it's too long\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    # User prompt containing the query and document preview\n",
    "    user_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        Document: {doc_preview}\n",
    "\n",
    "        Relevance score (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the score from the model's response\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract numeric score using regex\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  # Ensure score is within 0-10\n",
    "    else:\n",
    "        # Default score if extraction fails\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_document_context_relevance(query, context, document, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Score document relevance considering both query and context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): User context\n",
    "        document (str): Document text\n",
    "        model (str): LLM model\n",
    "        \n",
    "    Returns:\n",
    "        float: Relevance score from 0-10\n",
    "    \"\"\"\n",
    "    # System prompt to instruct the model on how to rate relevance considering context\n",
    "    system_prompt = \"\"\"You are an expert at evaluating document relevance considering context.\n",
    "        Rate the document on a scale from 0 to 10 based on how well it addresses the query\n",
    "        when considering the provided context, where:\n",
    "        0 = Completely irrelevant\n",
    "        10 = Perfectly addresses the query in the given context\n",
    "\n",
    "        Return ONLY a numerical score between 0 and 10, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    # Truncate document if it's too long\n",
    "    doc_preview = document[:1500] + \"...\" if len(document) > 1500 else document\n",
    "    \n",
    "    # User prompt containing the query, context, and document preview\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    Context: {context}\n",
    "\n",
    "    Document: {doc_preview}\n",
    "\n",
    "    Relevance score considering context (0-10):\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response from the model\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Extract the score from the model's response\n",
    "    score_text = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # Extract numeric score using regex\n",
    "    match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "    if match:\n",
    "        score = float(match.group(1))\n",
    "        return min(10, max(0, score))  # Ensure score is within 0-10\n",
    "    else:\n",
    "        # Default score if extraction fails\n",
    "        return 5.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Adaptive Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_retrieval(query, vector_store, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Perform adaptive retrieval by selecting and executing the appropriate strategy.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to retrieve\n",
    "        user_context (str): Optional user context for contextual queries\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Retrieved documents\n",
    "    \"\"\"\n",
    "    # Classify the query to determine its type\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "    # Select and execute the appropriate retrieval strategy based on the query type\n",
    "    if query_type == \"Factual\":\n",
    "        # Use the factual retrieval strategy for precise information\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Analytical\":\n",
    "        # Use the analytical retrieval strategy for comprehensive coverage\n",
    "        results = analytical_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Opinion\":\n",
    "        # Use the opinion retrieval strategy for diverse perspectives\n",
    "        results = opinion_retrieval_strategy(query, vector_store, k)\n",
    "    elif query_type == \"Contextual\":\n",
    "        # Use the contextual retrieval strategy, incorporating user context\n",
    "        results = contextual_retrieval_strategy(query, vector_store, k, user_context)\n",
    "    else:\n",
    "        # Default to factual retrieval strategy if classification fails\n",
    "        results = factual_retrieval_strategy(query, vector_store, k)\n",
    "    \n",
    "    return results  # Return the retrieved documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, results, query_type, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"\n",
    "    Generate a response based on query, retrieved documents, and query type.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        results (List[Dict]): Retrieved documents\n",
    "        query_type (str): Type of query\n",
    "        model (str): LLM model\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Prepare context from retrieved documents by joining their texts with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([r[\"text\"] for r in results])\n",
    "    \n",
    "    # Create custom system prompt based on query type\n",
    "    if query_type == \"Factual\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing factual information.\n",
    "    Answer the question based on the provided context. Focus on accuracy and precision.\n",
    "    If the context doesn't contain the information needed, acknowledge the limitations.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Analytical\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing analytical insights.\n",
    "    Based on the provided context, offer a comprehensive analysis of the topic.\n",
    "    Cover different aspects and perspectives in your explanation.\n",
    "    If the context has gaps, acknowledge them while providing the best analysis possible.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Opinion\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant discussing topics with multiple viewpoints.\n",
    "    Based on the provided context, present different perspectives on the topic.\n",
    "    Ensure fair representation of diverse opinions without showing bias.\n",
    "    Acknowledge where the context presents limited viewpoints.\"\"\"\n",
    "        \n",
    "    elif query_type == \"Contextual\":\n",
    "        system_prompt = \"\"\"You are a helpful assistant providing contextually relevant information.\n",
    "    Answer the question considering both the query and its context.\n",
    "    Make connections between the query context and the information in the provided documents.\n",
    "    If the context doesn't fully address the specific situation, acknowledge the limitations.\"\"\"\n",
    "        \n",
    "    else:\n",
    "        system_prompt = \"\"\"You are a helpful assistant. Answer the question based on the provided context. If you cannot answer from the context, acknowledge the limitations.\"\"\"\n",
    "    \n",
    "    # Create user prompt by combining the context and the query\n",
    "    user_prompt = f\"\"\"\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Please provide a helpful response based on the context.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate response using the OpenAI client\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0.2\n",
    "    )\n",
    "    \n",
    "    # Return the generated response content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Adaptive Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_adaptive_retrieval(pdf_path, query, k=4, user_context=None):\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline with adaptive retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document\n",
    "        query (str): User query\n",
    "        k (int): Number of documents to retrieve\n",
    "        user_context (str): Optional user context\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Results including query, retrieved documents, query type, and response\n",
    "    \"\"\"\n",
    "    print(\"\\n=== RAG WITH ADAPTIVE RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # Process the document to extract text, chunk it, and create embeddings\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Classify the query to determine its type\n",
    "    query_type = classify_query(query)\n",
    "    print(f\"Query classified as: {query_type}\")\n",
    "    \n",
    "    # Retrieve documents using the adaptive retrieval strategy based on the query type\n",
    "    retrieved_docs = adaptive_retrieval(query, vector_store, k, user_context)\n",
    "    \n",
    "    # Generate a response based on the query, retrieved documents, and query type\n",
    "    response = generate_response(query, retrieved_docs, query_type)\n",
    "    \n",
    "    # Compile the results into a dictionary\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"query_type\": query_type,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_adaptive_vs_standard(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Compare adaptive retrieval with standard retrieval on a set of test queries.\n",
    "    \n",
    "    This function processes a document, runs both standard and adaptive retrieval methods\n",
    "    on each test query, and compares their performance. If reference answers are provided,\n",
    "    it also evaluates the quality of responses against these references.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to PDF document to be processed as the knowledge source\n",
    "        test_queries (List[str]): List of test queries to evaluate both retrieval methods\n",
    "        reference_answers (List[str], optional): Reference answers for evaluation metrics\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results containing individual query results and overall comparison\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\")\n",
    "    \n",
    "    # Process document to extract text, create chunks and build the vector store\n",
    "    chunks, vector_store = process_document(pdf_path)\n",
    "    \n",
    "    # Initialize collection for storing comparison results\n",
    "    results = []\n",
    "    \n",
    "    # Process each test query with both retrieval methods\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\nQuery {i+1}: {query}\")\n",
    "        \n",
    "        # --- Standard retrieval approach ---\n",
    "        print(\"\\n--- Standard Retrieval ---\")\n",
    "        # Create embedding for the query\n",
    "        query_embedding = create_embeddings(query)\n",
    "        # Retrieve documents using simple vector similarity\n",
    "        standard_docs = vector_store.similarity_search(query_embedding, k=4)\n",
    "        # Generate response using a generic approach\n",
    "        standard_response = generate_response(query, standard_docs, \"General\")\n",
    "        \n",
    "        # --- Adaptive retrieval approach ---\n",
    "        print(\"\\n--- Adaptive Retrieval ---\")\n",
    "        # Classify the query to determine its type (Factual, Analytical, Opinion, Contextual)\n",
    "        query_type = classify_query(query)\n",
    "        # Retrieve documents using the strategy appropriate for this query type\n",
    "        adaptive_docs = adaptive_retrieval(query, vector_store, k=4)\n",
    "        # Generate a response tailored to the query type\n",
    "        adaptive_response = generate_response(query, adaptive_docs, query_type)\n",
    "        \n",
    "        # Store complete results for this query\n",
    "        result = {\n",
    "            \"query\": query,\n",
    "            \"query_type\": query_type,\n",
    "            \"standard_retrieval\": {\n",
    "                \"documents\": standard_docs,\n",
    "                \"response\": standard_response\n",
    "            },\n",
    "            \"adaptive_retrieval\": {\n",
    "                \"documents\": adaptive_docs,\n",
    "                \"response\": adaptive_response\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Add reference answer if available for this query\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            result[\"reference_answer\"] = reference_answers[i]\n",
    "            \n",
    "        results.append(result)\n",
    "        \n",
    "        # Display preview of both responses for quick comparison\n",
    "        print(\"\\n--- Responses ---\")\n",
    "        print(f\"Standard: {standard_response[:200]}...\")\n",
    "        print(f\"Adaptive: {adaptive_response[:200]}...\")\n",
    "    \n",
    "    # Calculate comparative metrics if reference answers are available\n",
    "    if reference_answers:\n",
    "        comparison = compare_responses(results)\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(comparison)\n",
    "    \n",
    "    # Return the complete evaluation results\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"comparison\": comparison if reference_answers else \"No reference answers provided for evaluation\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(results):\n",
    "    \"\"\"\n",
    "    Compare standard and adaptive responses against reference answers.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results containing both types of responses\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI in comparing responses\n",
    "    comparison_prompt = \"\"\"You are an expert evaluator of information retrieval systems.\n",
    "    Compare the standard retrieval and adaptive retrieval responses for each query.\n",
    "    Consider factors like accuracy, relevance, comprehensiveness, and alignment with the reference answer.\n",
    "    Provide a detailed analysis of the strengths and weaknesses of each approach.\"\"\"\n",
    "    \n",
    "    # Initialize the comparison text with a header\n",
    "    comparison_text = \"# Evaluation of Standard vs. Adaptive Retrieval\\n\\n\"\n",
    "    \n",
    "    # Iterate through each result to compare responses\n",
    "    for i, result in enumerate(results):\n",
    "        # Skip if there is no reference answer for the query\n",
    "        if \"reference_answer\" not in result:\n",
    "            continue\n",
    "            \n",
    "        # Add query details to the comparison text\n",
    "        comparison_text += f\"## Query {i+1}: {result['query']}\\n\"\n",
    "        comparison_text += f\"*Query Type: {result['query_type']}*\\n\\n\"\n",
    "        comparison_text += f\"**Reference Answer:**\\n{result['reference_answer']}\\n\\n\"\n",
    "        \n",
    "        # Add standard retrieval response to the comparison text\n",
    "        comparison_text += f\"**Standard Retrieval Response:**\\n{result['standard_retrieval']['response']}\\n\\n\"\n",
    "        \n",
    "        # Add adaptive retrieval response to the comparison text\n",
    "        comparison_text += f\"**Adaptive Retrieval Response:**\\n{result['adaptive_retrieval']['response']}\\n\\n\"\n",
    "        \n",
    "        # Create the user prompt for the AI to compare the responses\n",
    "        user_prompt = f\"\"\"\n",
    "        Reference Answer: {result['reference_answer']}\n",
    "        \n",
    "        Standard Retrieval Response: {result['standard_retrieval']['response']}\n",
    "        \n",
    "        Adaptive Retrieval Response: {result['adaptive_retrieval']['response']}\n",
    "        \n",
    "        Provide a detailed comparison of the two responses.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the comparison analysis using the OpenAI client\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": comparison_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        # Add the AI's comparison analysis to the comparison text\n",
    "        comparison_text += f\"**Comparison Analysis:**\\n{response.choices[0].message.content}\\n\\n\"\n",
    "    \n",
    "    return comparison_text  # Return the complete comparison analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Adaptive Retrieval System (Customized Queries)\n",
    "\n",
    "The final step to use the adaptive RAG evaluation system is to call the evaluate_adaptive_vs_standard() function with your PDF document and test queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your knowledge source document\n",
    "# This PDF file contains the information that the RAG system will use\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/RAG Project/Resources.pdf\"\n",
    "\n",
    "# Define test queries covering different query types to demonstrate \n",
    "# how adaptive retrieval handles various query intentions\n",
    "test_queries = [\n",
    "    \"I lost power in 17104. Where can I go to get ice or charge my phone?\"                                             \n",
    "]\n",
    "\n",
    "\n",
    "reference_answers = [\n",
    "   \"If you're in ZIP 17104 and need emergency shelter: The Salvation Army Harrisburg Capital City Region\\nLocation: 506 S 29th St, Harrisburg, PA 17104\\nPhone: (717) 233-6755\\nServices: Overnight shelter, meals, hygiene support\\n\\n🚨 During disasters, additional shelters may be opened by Red Cross or PEMA. Dial 2-1-1 to find the closest available shelter now.\\n\\n🐾 Ask about pet-friendly or ADA-accessible shelters if needed.\" \n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING ADAPTIVE VS. STANDARD RETRIEVAL ===\n",
      "Extracting text from PDF...\n",
      "Chunking text...\n",
      "Created 15 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 15 chunks to the vector store\n",
      "\n",
      "\n",
      "Query 1: I lost power in 17104. Where can I go to get ice or charge my phone?\n",
      "\n",
      "--- Standard Retrieval ---\n",
      "\n",
      "--- Adaptive Retrieval ---\n",
      "Query classified as: Contextual\n",
      "Executing Contextual retrieval strategy for: 'I lost power in 17104. Where can I go to get ice or charge my phone?'\n",
      "Inferred context: The implied context suggests that the individual is experiencing a power outage in the 17104 area, which is a ZIP code for Harrisburg, Pennsylvania. This indicates a potential emergency situation where access to electricity is temporarily unavailable, possibly due to a storm, utility failure, or other disruptions. The person is seeking immediate solutions to preserve perishable food items (hence the need for ice) and to maintain communication (hence the need to charge their phone). It would be relevant to know about nearby convenience stores, gas stations, or community centers that may have generators or ice available, as well as any local resources or emergency services that could assist during the outage. Additionally, understanding the duration of the power outage and the extent of the issue in the area could provide further context for the urgency of the request.\n",
      "Contextualized query: Where can I find nearby convenience stores, gas stations, or community centers in Harrisburg, PA 17104 that have ice or charging stations for my phone during the current power outage?\n",
      "\n",
      "--- Responses ---\n",
      "Standard: The provided context does not contain specific information about locations where you can get ice or charge your phone in the 17104 area. However, you may want to consider visiting local food pantries ...\n",
      "Adaptive: Based on the context provided, there are several resources you can consider for assistance in the 17104 area:\n",
      "\n",
      "1. **Lancaster County Food Hub** - They have various programs, including a Code Red: Extr...\n",
      "\n",
      "=== EVALUATION RESULTS ===\n",
      "# Evaluation of Standard vs. Adaptive Retrieval\n",
      "\n",
      "## Query 1: I lost power in 17104. Where can I go to get ice or charge my phone?\n",
      "*Query Type: Contextual*\n",
      "\n",
      "**Reference Answer:**\n",
      "If you're in ZIP 17104 and need emergency shelter: The Salvation Army Harrisburg Capital City Region\n",
      "Location: 506 S 29th St, Harrisburg, PA 17104\n",
      "Phone: (717) 233-6755\n",
      "Services: Overnight shelter, meals, hygiene support\n",
      "\n",
      "🚨 During disasters, additional shelters may be opened by Red Cross or PEMA. Dial 2-1-1 to find the closest available shelter now.\n",
      "\n",
      "🐾 Ask about pet-friendly or ADA-accessible shelters if needed.\n",
      "\n",
      "**Standard Retrieval Response:**\n",
      "The provided context does not contain specific information about locations where you can get ice or charge your phone in the 17104 area. However, you may want to consider visiting local food pantries or warming centers, as they often provide assistance during power outages. You can contact the Lancaster County Food Hub at 717-291-2261 for more information on available services. Additionally, local community centers or libraries may offer charging stations, so it could be worth checking those as well.\n",
      "\n",
      "**Adaptive Retrieval Response:**\n",
      "Based on the context provided, there are several resources you can consider for assistance in the 17104 area:\n",
      "\n",
      "1. **Lancaster County Food Hub** - They have various programs, including a Code Red: Extreme Heat Cooling Program, which may provide resources for those in need during extreme weather conditions. You can contact them at **717-291-2261** to inquire if they have ice or charging facilities available.\n",
      "\n",
      "2. **Street Outreach** - If you are homeless or in need, the Lancaster County Food Hub also offers street outreach services for homeless individuals. They might be able to direct you to locations where you can charge your phone or obtain ice.\n",
      "\n",
      "3. **Local Food Pantries** - You can also reach out to local food pantries, such as the **Manheim Central Food Pantry** at **717-664-1097**, to see if they have any resources available for those without power.\n",
      "\n",
      "4. **Community Centers or Libraries** - While not listed in the context, local community centers or libraries often provide charging stations and may have ice available during power outages.\n",
      "\n",
      "Make sure to call ahead to confirm availability and any specific requirements they may have.\n",
      "\n",
      "**Comparison Analysis:**\n",
      "### Detailed Comparison of Standard Retrieval and Adaptive Retrieval Responses\n",
      "\n",
      "#### 1. **Accuracy**\n",
      "- **Standard Retrieval Response**: The response does not directly address the query about emergency shelter in ZIP 17104. Instead, it provides information about food pantries and warming centers, which are not the primary concern of the user seeking shelter. This indicates a lack of accuracy in addressing the specific needs of the query.\n",
      "  \n",
      "- **Adaptive Retrieval Response**: This response is more accurate in that it acknowledges the need for assistance in the 17104 area and provides relevant resources. It mentions the Lancaster County Food Hub and its programs, which could potentially assist those in need during emergencies. However, it still does not directly provide the specific emergency shelter information requested in the reference answer.\n",
      "\n",
      "#### 2. **Relevance**\n",
      "- **Standard Retrieval Response**: The information provided is somewhat relevant to the broader context of seeking assistance during emergencies, but it fails to address the specific need for emergency shelter. This lack of relevance diminishes the overall utility of the response for the user.\n",
      "  \n",
      "- **Adaptive Retrieval Response**: This response is more relevant as it lists multiple resources that could assist individuals in need. It includes specific programs and services that may help during emergencies, such as outreach services and food pantries. However, it still lacks the direct mention of emergency shelters, which is the primary focus of the reference answer.\n",
      "\n",
      "#### 3. **Comprehensiveness**\n",
      "- **Standard Retrieval Response**: The response is limited in scope, providing only a few suggestions without elaborating on the types of services available or how they relate to the user's immediate need for shelter. This lack of comprehensiveness makes it less useful for someone in a critical situation.\n",
      "  \n",
      "- **Adaptive Retrieval Response**: This response is more comprehensive, listing multiple resources and services that could assist the user. It provides contact information and suggests specific programs that may be relevant during emergencies. However, it still misses the comprehensive detail about emergency shelters that the reference answer provides.\n",
      "\n",
      "#### 4. **Alignment with Reference Answer**\n",
      "- **Standard Retrieval Response**: There is a significant misalignment with the reference answer. The user is looking for emergency shelter information, and the response does not address this need at all. This disconnect could lead to frustration for the user seeking immediate assistance.\n",
      "  \n",
      "- **Adaptive Retrieval Response**: While this response is more aligned with the user's needs than the standard retrieval response, it still does not directly provide the specific emergency shelter information mentioned in the reference answer. It does, however, offer a broader range of resources that could be beneficial, albeit indirectly.\n",
      "\n",
      "### Strengths and Weaknesses\n",
      "\n",
      "#### **Standard Retrieval Response**\n",
      "- **Strengths**:\n",
      "  - Provides alternative resources (food pantries, warming centers) that could be useful in a broader context.\n",
      "  - Suggests contacting local organizations for assistance.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - Fails to address the specific query about emergency shelter.\n",
      "  - Lacks relevance and comprehensiveness, making it less useful for the user.\n",
      "\n",
      "#### **Adaptive Retrieval Response**\n",
      "- **Strengths**:\n",
      "  - More relevant and comprehensive, offering multiple resources that could assist the user.\n",
      "  - Provides specific contact information and suggests programs that may help during emergencies.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - Still does not directly answer the query about emergency shelter, missing the critical information provided in the reference answer.\n",
      "  - While it offers a broader range of resources, it may overwhelm the user with options that are not directly related to their immediate need.\n",
      "\n",
      "### Conclusion\n",
      "In summary, the adaptive retrieval response is superior to the standard retrieval response in terms of relevance, comprehensiveness, and accuracy regarding the broader context of assistance during emergencies. However, both responses ultimately fail to provide the specific emergency shelter information that the user is seeking, which is crucial for effective information retrieval in this scenario. For optimal performance, an ideal response would directly address the user's query while also providing additional relevant resources.\n",
      "\n",
      "\n",
      "# Evaluation of Standard vs. Adaptive Retrieval\n",
      "\n",
      "## Query 1: I lost power in 17104. Where can I go to get ice or charge my phone?\n",
      "*Query Type: Contextual*\n",
      "\n",
      "**Reference Answer:**\n",
      "If you're in ZIP 17104 and need emergency shelter: The Salvation Army Harrisburg Capital City Region\n",
      "Location: 506 S 29th St, Harrisburg, PA 17104\n",
      "Phone: (717) 233-6755\n",
      "Services: Overnight shelter, meals, hygiene support\n",
      "\n",
      "🚨 During disasters, additional shelters may be opened by Red Cross or PEMA. Dial 2-1-1 to find the closest available shelter now.\n",
      "\n",
      "🐾 Ask about pet-friendly or ADA-accessible shelters if needed.\n",
      "\n",
      "**Standard Retrieval Response:**\n",
      "The provided context does not contain specific information about locations where you can get ice or charge your phone in the 17104 area. However, you may want to consider visiting local food pantries or warming centers, as they often provide assistance during power outages. You can contact the Lancaster County Food Hub at 717-291-2261 for more information on available services. Additionally, local community centers or libraries may offer charging stations, so it could be worth checking those as well.\n",
      "\n",
      "**Adaptive Retrieval Response:**\n",
      "Based on the context provided, there are several resources you can consider for assistance in the 17104 area:\n",
      "\n",
      "1. **Lancaster County Food Hub** - They have various programs, including a Code Red: Extreme Heat Cooling Program, which may provide resources for those in need during extreme weather conditions. You can contact them at **717-291-2261** to inquire if they have ice or charging facilities available.\n",
      "\n",
      "2. **Street Outreach** - If you are homeless or in need, the Lancaster County Food Hub also offers street outreach services for homeless individuals. They might be able to direct you to locations where you can charge your phone or obtain ice.\n",
      "\n",
      "3. **Local Food Pantries** - You can also reach out to local food pantries, such as the **Manheim Central Food Pantry** at **717-664-1097**, to see if they have any resources available for those without power.\n",
      "\n",
      "4. **Community Centers or Libraries** - While not listed in the context, local community centers or libraries often provide charging stations and may have ice available during power outages.\n",
      "\n",
      "Make sure to call ahead to confirm availability and any specific requirements they may have.\n",
      "\n",
      "**Comparison Analysis:**\n",
      "### Detailed Comparison of Standard Retrieval and Adaptive Retrieval Responses\n",
      "\n",
      "#### 1. **Accuracy**\n",
      "- **Standard Retrieval Response**: The response does not directly address the query about emergency shelter in ZIP 17104. Instead, it provides information about food pantries and warming centers, which are not the primary concern of the user seeking shelter. This indicates a lack of accuracy in addressing the specific needs of the query.\n",
      "  \n",
      "- **Adaptive Retrieval Response**: This response is more accurate in that it acknowledges the need for assistance in the 17104 area and provides relevant resources. It mentions the Lancaster County Food Hub and its programs, which could potentially assist those in need during emergencies. However, it still does not directly provide the specific emergency shelter information requested in the reference answer.\n",
      "\n",
      "#### 2. **Relevance**\n",
      "- **Standard Retrieval Response**: The information provided is somewhat relevant to the broader context of seeking assistance during emergencies, but it fails to address the specific need for emergency shelter. This lack of relevance diminishes the overall utility of the response for the user.\n",
      "  \n",
      "- **Adaptive Retrieval Response**: This response is more relevant as it lists multiple resources that could assist individuals in need. It includes specific programs and services that may help during emergencies, such as outreach services and food pantries. However, it still lacks the direct mention of emergency shelters, which is the primary focus of the reference answer.\n",
      "\n",
      "#### 3. **Comprehensiveness**\n",
      "- **Standard Retrieval Response**: The response is limited in scope, providing only a few suggestions without elaborating on the types of services available or how they relate to the user's immediate need for shelter. This lack of comprehensiveness makes it less useful for someone in a critical situation.\n",
      "  \n",
      "- **Adaptive Retrieval Response**: This response is more comprehensive, listing multiple resources and services that could assist the user. It provides contact information and suggests specific programs that may be relevant during emergencies. However, it still misses the comprehensive detail about emergency shelters that the reference answer provides.\n",
      "\n",
      "#### 4. **Alignment with Reference Answer**\n",
      "- **Standard Retrieval Response**: There is a significant misalignment with the reference answer. The user is looking for emergency shelter information, and the response does not address this need at all. This disconnect could lead to frustration for the user seeking immediate assistance.\n",
      "  \n",
      "- **Adaptive Retrieval Response**: While this response is more aligned with the user's needs than the standard retrieval response, it still does not directly provide the specific emergency shelter information mentioned in the reference answer. It does, however, offer a broader range of resources that could be beneficial, albeit indirectly.\n",
      "\n",
      "### Strengths and Weaknesses\n",
      "\n",
      "#### **Standard Retrieval Response**\n",
      "- **Strengths**:\n",
      "  - Provides alternative resources (food pantries, warming centers) that could be useful in a broader context.\n",
      "  - Suggests contacting local organizations for assistance.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - Fails to address the specific query about emergency shelter.\n",
      "  - Lacks relevance and comprehensiveness, making it less useful for the user.\n",
      "\n",
      "#### **Adaptive Retrieval Response**\n",
      "- **Strengths**:\n",
      "  - More relevant and comprehensive, offering multiple resources that could assist the user.\n",
      "  - Provides specific contact information and suggests programs that may help during emergencies.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - Still does not directly answer the query about emergency shelter, missing the critical information provided in the reference answer.\n",
      "  - While it offers a broader range of resources, it may overwhelm the user with options that are not directly related to their immediate need.\n",
      "\n",
      "### Conclusion\n",
      "In summary, the adaptive retrieval response is superior to the standard retrieval response in terms of relevance, comprehensiveness, and accuracy regarding the broader context of assistance during emergencies. However, both responses ultimately fail to provide the specific emergency shelter information that the user is seeking, which is crucial for effective information retrieval in this scenario. For optimal performance, an ideal response would directly address the user's query while also providing additional relevant resources.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the evaluation comparing adaptive vs standard retrieval\n",
    "# This will process each query using both methods and compare the results\n",
    "evaluation_results = evaluate_adaptive_vs_standard(\n",
    "    pdf_path=pdf_path,                  # Source document for knowledge extraction\n",
    "    test_queries=test_queries,          # List of test queries to evaluate\n",
    "    reference_answers=reference_answers  # Optional ground truth for comparison\n",
    ")\n",
    "\n",
    "# The results will show a detailed comparison between standard retrieval and \n",
    "# adaptive retrieval performance across different query types, highlighting\n",
    "# where adaptive strategies provide improved outcomes\n",
    "print(evaluation_results[\"comparison\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
