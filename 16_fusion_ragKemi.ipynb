{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Fusion Retrieval: Combining Vector and Keyword Search\n",
    "\n",
    "In this notebook, I implement a fusion retrieval system that combines the strengths of semantic vector search with keyword-based BM25 retrieval. This approach improves retrieval quality by capturing both conceptual similarity and exact keyword matches.\n",
    "\n",
    "## Why Fusion Retrieval Matters\n",
    "\n",
    "Traditional RAG systems typically rely on vector search alone, but this has limitations:\n",
    "\n",
    "- Vector search excels at semantic similarity but may miss exact keyword matches\n",
    "- Keyword search is great for specific terms but lacks semantic understanding\n",
    "- Different queries perform better with different retrieval methods\n",
    "\n",
    "Fusion retrieval gives us the best of both worlds by:\n",
    "\n",
    "- Performing both vector-based and keyword-based retrieval\n",
    "- Normalizing the scores from each approach\n",
    "- Combining them with a weighted formula\n",
    "- Ranking documents based on the combined score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import fitz\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")  # Print the path of the PDF being processed\n",
    "    pdf_document = fitz.open(pdf_path)  # Open the PDF file using PyMuPDF\n",
    "    text = \"\"  # Initialize an empty string to store the extracted text\n",
    "    \n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(pdf_document.page_count):\n",
    "        page = pdf_document[page_num]  # Get the page object\n",
    "        text += page.get_text()  # Extract text from the page and append to the text string\n",
    "    \n",
    "    return text  # Return the extracted text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to chunk\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of chunks with text and metadata\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store chunks\n",
    "    \n",
    "    # Iterate over the text with the specified chunk size and overlap\n",
    "    for i in range(0, len(text), chunk_size - chunk_overlap):\n",
    "        chunk = text[i:i + chunk_size]  # Extract a chunk of the specified size\n",
    "        if chunk:  # Ensure we don't add empty chunks\n",
    "            chunk_data = {\n",
    "                \"text\": chunk,  # The chunk text\n",
    "                \"metadata\": {\n",
    "                    \"start_char\": i,  # Start character index of the chunk\n",
    "                    \"end_char\": i + len(chunk)  # End character index of the chunk\n",
    "                }\n",
    "            }\n",
    "            chunks.append(chunk_data)  # Add the chunk data to the list\n",
    "    \n",
    "    print(f\"Created {len(chunks)} text chunks\")  # Print the number of created chunks\n",
    "    return chunks  # Return the list of chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing extra whitespace and special characters.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned text\n",
    "    \"\"\"\n",
    "    # Replace multiple whitespace characters (including newlines and tabs) with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Fix common OCR issues by replacing tab and newline characters with a space\n",
    "    text = text.replace('\\\\t', ' ')\n",
    "    text = text.replace('\\\\n', ' ')\n",
    "    \n",
    "    # Remove any leading or trailing whitespace and ensure single spaces between words\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Create embeddings for the given texts.\n",
    "    \n",
    "    Args:\n",
    "        texts (str or List[str]): Input text(s)\n",
    "        model (str): Embedding model name\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: Embedding vectors\n",
    "    \"\"\"\n",
    "    # Handle both string and list inputs\n",
    "    input_texts = texts if isinstance(texts, list) else [texts]\n",
    "    \n",
    "    # Process in batches if needed (OpenAI API limits)\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Iterate over the input texts in batches\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        batch = input_texts[i:i + batch_size]  # Get the current batch of texts\n",
    "        \n",
    "        # Create embeddings for the current batch\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract embeddings from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)  # Add the batch embeddings to the list\n",
    "    \n",
    "    # If input was a string, return just the first embedding\n",
    "    if isinstance(texts, str):\n",
    "        return all_embeddings[0]\n",
    "    \n",
    "    # Otherwise return all embeddings\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.vectors = []  # List to store embedding vectors\n",
    "        self.texts = []  # List to store text content\n",
    "        self.metadata = []  # List to store metadata\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text content\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))  # Append the embedding vector\n",
    "        self.texts.append(text)  # Append the text content\n",
    "        self.metadata.append(metadata or {})  # Append the metadata (or empty dict if None)\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): List of text items\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
    "            self.add_item(\n",
    "                text=item[\"text\"],  # Extract text from item\n",
    "                embedding=embedding,  # Use corresponding embedding\n",
    "                metadata={**item.get(\"metadata\", {}), \"index\": i}  # Merge item metadata with index\n",
    "            )\n",
    "    \n",
    "    def similarity_search_with_scores(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding with similarity scores.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Tuple[Dict, float]]: Top k most similar items with scores\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # Return empty list if no vectors are stored\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = cosine_similarity([query_vector], [vector])[0][0]  # Compute cosine similarity\n",
    "            similarities.append((i, similarity))  # Append index and similarity score\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results with scores\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],  # Retrieve text by index\n",
    "                \"metadata\": self.metadata[idx],  # Retrieve metadata by index\n",
    "                \"similarity\": float(score)  # Add similarity score\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_all_documents(self):\n",
    "        \"\"\"\n",
    "        Get all documents in the store.\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: All documents\n",
    "        \"\"\"\n",
    "        return [{\"text\": text, \"metadata\": meta} for text, meta in zip(self.texts, self.metadata)]  # Combine texts and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BM25 Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bm25_index(chunks):\n",
    "    \"\"\"\n",
    "    Create a BM25 index from the given chunks.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[Dict]): List of text chunks\n",
    "        \n",
    "    Returns:\n",
    "        BM25Okapi: A BM25 index\n",
    "    \"\"\"\n",
    "    # Extract text from each chunk\n",
    "    texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    \n",
    "    # Tokenize each document by splitting on whitespace\n",
    "    tokenized_docs = [text.split() for text in texts]\n",
    "    \n",
    "    # Create the BM25 index using the tokenized documents\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Print the number of documents in the BM25 index\n",
    "    print(f\"Created BM25 index with {len(texts)} documents\")\n",
    "    \n",
    "    return bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_search(bm25, chunks, query, k=5):\n",
    "    \"\"\"\n",
    "    Search the BM25 index with a query.\n",
    "    \n",
    "    Args:\n",
    "        bm25 (BM25Okapi): BM25 index\n",
    "        chunks (List[Dict]): List of text chunks\n",
    "        query (str): Query string\n",
    "        k (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results with scores\n",
    "    \"\"\"\n",
    "    # Tokenize the query by splitting it into individual words\n",
    "    query_tokens = query.split()\n",
    "    \n",
    "    # Get BM25 scores for the query tokens against the indexed documents\n",
    "    scores = bm25.get_scores(query_tokens)\n",
    "    \n",
    "    # Initialize an empty list to store results with their scores\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over the scores and corresponding chunks\n",
    "    for i, score in enumerate(scores):\n",
    "        # Create a copy of the metadata to avoid modifying the original\n",
    "        metadata = chunks[i].get(\"metadata\", {}).copy()\n",
    "        # Add index to metadata\n",
    "        metadata[\"index\"] = i\n",
    "        \n",
    "        results.append({\n",
    "            \"text\": chunks[i][\"text\"],\n",
    "            \"metadata\": metadata,  # Add metadata with index\n",
    "            \"bm25_score\": float(score)\n",
    "        })\n",
    "    \n",
    "    # Sort the results by BM25 score in descending order\n",
    "    results.sort(key=lambda x: x[\"bm25_score\"], reverse=True)\n",
    "    \n",
    "    # Return the top k results\n",
    "    return results[:k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fusion_retrieval(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Perform fusion retrieval combining vector-based and BM25 search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Query string\n",
    "        chunks (List[Dict]): Original text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of results to return\n",
    "        alpha (float): Weight for vector scores (0-1), where 1-alpha is BM25 weight\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: Top k results based on combined scores\n",
    "    \"\"\"\n",
    "    print(f\"Performing fusion retrieval for query: {query}\")\n",
    "    \n",
    "    # Define small epsilon to avoid division by zero\n",
    "    epsilon = 1e-8\n",
    "    \n",
    "    # Get vector search results\n",
    "    query_embedding = create_embeddings(query)  # Create embedding for the query\n",
    "    vector_results = vector_store.similarity_search_with_scores(query_embedding, k=len(chunks))  # Perform vector search\n",
    "    \n",
    "    # Get BM25 search results\n",
    "    bm25_results = bm25_search(bm25_index, chunks, query, k=len(chunks))  # Perform BM25 search\n",
    "    \n",
    "    # Create dictionaries to map document index to score\n",
    "    vector_scores_dict = {result[\"metadata\"][\"index\"]: result[\"similarity\"] for result in vector_results}\n",
    "    bm25_scores_dict = {result[\"metadata\"][\"index\"]: result[\"bm25_score\"] for result in bm25_results}\n",
    "    \n",
    "    # Ensure all documents have scores for both methods\n",
    "    all_docs = vector_store.get_all_documents()\n",
    "    combined_results = []\n",
    "    \n",
    "    for i, doc in enumerate(all_docs):\n",
    "        vector_score = vector_scores_dict.get(i, 0.0)  # Get vector score or 0 if not found\n",
    "        bm25_score = bm25_scores_dict.get(i, 0.0)  # Get BM25 score or 0 if not found\n",
    "        combined_results.append({\n",
    "            \"text\": doc[\"text\"],\n",
    "            \"metadata\": doc[\"metadata\"],\n",
    "            \"vector_score\": vector_score,\n",
    "            \"bm25_score\": bm25_score,\n",
    "            \"index\": i\n",
    "        })\n",
    "    \n",
    "    # Extract scores as arrays\n",
    "    vector_scores = np.array([doc[\"vector_score\"] for doc in combined_results])\n",
    "    bm25_scores = np.array([doc[\"bm25_score\"] for doc in combined_results])\n",
    "    \n",
    "    # Normalize scores\n",
    "    norm_vector_scores = (vector_scores - np.min(vector_scores)) / (np.max(vector_scores) - np.min(vector_scores) + epsilon)\n",
    "    norm_bm25_scores = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + epsilon)\n",
    "    \n",
    "    # Compute combined scores\n",
    "    combined_scores = alpha * norm_vector_scores + (1 - alpha) * norm_bm25_scores\n",
    "    \n",
    "    # Add combined scores to results\n",
    "    for i, score in enumerate(combined_scores):\n",
    "        combined_results[i][\"combined_score\"] = float(score)\n",
    "    \n",
    "    # Sort by combined score (descending)\n",
    "    combined_results.sort(key=lambda x: x[\"combined_score\"], reverse=True)\n",
    "    \n",
    "    # Return top k results\n",
    "    top_results = combined_results[:k]\n",
    "    \n",
    "    print(f\"Retrieved {len(top_results)} documents with fusion retrieval\")\n",
    "    return top_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document for fusion retrieval.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[Dict], SimpleVectorStore, BM25Okapi]: Chunks, vector store, and BM25 index\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Clean the extracted text to remove extra whitespace and special characters\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Split the cleaned text into overlapping chunks\n",
    "    chunks = chunk_text(cleaned_text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Extract the text content from each chunk for embedding creation\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    \n",
    "    # Create embeddings for the chunk texts\n",
    "    embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # Initialize the vector store\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add the chunks and their embeddings to the vector store\n",
    "    vector_store.add_items(chunks, embeddings)\n",
    "    print(f\"Added {len(chunks)} items to vector store\")\n",
    "    \n",
    "    # Create a BM25 index from the chunks\n",
    "    bm25_index = create_bm25_index(chunks)\n",
    "    \n",
    "    # Return the chunks, vector store, and BM25 index\n",
    "    return chunks, vector_store, bm25_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and context.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        context (str): Context from retrieved documents\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Define the system prompt to guide the AI assistant\n",
    "    system_prompt = \"\"\"You are a helpful AI assistant. Answer the user's question based on the provided context. \n",
    "    If the context doesn't contain relevant information to answer the question fully, acknowledge this limitation.\"\"\"\n",
    "\n",
    "    # Format the user prompt with the context and query\n",
    "    user_prompt = f\"\"\"Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "\n",
    "    Please answer the question based on the provided context.\"\"\"\n",
    "\n",
    "    # Generate the response using the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with context and query\n",
    "        ],\n",
    "        temperature=0.3  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated response\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Answer a query using fusion RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results including retrieved documents and response\n",
    "    \"\"\"\n",
    "    # Retrieve documents using fusion retrieval method\n",
    "    retrieved_docs = fusion_retrieval(query, chunks, vector_store, bm25_index, k=k, alpha=alpha)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Retrieval Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_only_rag(query, vector_store, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only vector-based RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Create query embedding\n",
    "    query_embedding = create_embeddings(query)\n",
    "    \n",
    "    # Retrieve documents using vector-based similarity search\n",
    "    retrieved_docs = vector_store.similarity_search_with_scores(query_embedding, k=k)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_only_rag(query, chunks, bm25_index, k=5):\n",
    "    \"\"\"\n",
    "    Answer a query using only BM25-based RAG.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Query results\n",
    "    \"\"\"\n",
    "    # Retrieve documents using BM25 search\n",
    "    retrieved_docs = bm25_search(bm25_index, chunks, query, k=k)\n",
    "    \n",
    "    # Format the context from the retrieved documents by joining their text with separators\n",
    "    context = \"\\n\\n---\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    \n",
    "    # Generate a response based on the query and the formatted context\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # Return the query, retrieved documents, and the generated response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": retrieved_docs,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_methods(query, chunks, vector_store, bm25_index, k=5, alpha=0.5, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare different retrieval methods for a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        chunks (List[Dict]): Text chunks\n",
    "        vector_store (SimpleVectorStore): Vector store\n",
    "        bm25_index (BM25Okapi): BM25 index\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores in fusion retrieval\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Comparing retrieval methods for query: {query} ===\\n\")\n",
    "    \n",
    "    # Run vector-only RAG\n",
    "    print(\"\\nRunning vector-only RAG...\")\n",
    "    vector_result = vector_only_rag(query, vector_store, k)\n",
    "    \n",
    "    # Run BM25-only RAG\n",
    "    print(\"\\nRunning BM25-only RAG...\")\n",
    "    bm25_result = bm25_only_rag(query, chunks, bm25_index, k)\n",
    "    \n",
    "    # Run fusion RAG\n",
    "    print(\"\\nRunning fusion RAG...\")\n",
    "    fusion_result = answer_with_fusion_rag(query, chunks, vector_store, bm25_index, k, alpha)\n",
    "    \n",
    "    # Compare responses from different retrieval methods\n",
    "    print(\"\\nComparing responses...\")\n",
    "    comparison = evaluate_responses(\n",
    "        query, \n",
    "        vector_result[\"response\"], \n",
    "        bm25_result[\"response\"], \n",
    "        fusion_result[\"response\"],\n",
    "        reference_answer\n",
    "    )\n",
    "    \n",
    "    # Return the comparison results\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"vector_result\": vector_result,\n",
    "        \"bm25_result\": bm25_result,\n",
    "        \"fusion_result\": fusion_result,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_responses(query, vector_response, bm25_response, fusion_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate the responses from different retrieval methods.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_response (str): Response from vector-only RAG\n",
    "        bm25_response (str): Response from BM25-only RAG\n",
    "        fusion_response (str): Response from fusion RAG\n",
    "        reference_answer (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Evaluation of responses\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluator to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from three different retrieval approaches:\n",
    "    1. Vector-based retrieval: Uses semantic similarity for document retrieval\n",
    "    2. BM25 keyword retrieval: Uses keyword matching for document retrieval\n",
    "    3. Fusion retrieval: Combines both vector and keyword approaches\n",
    "\n",
    "    Evaluate the responses based on:\n",
    "    - Relevance to the query\n",
    "    - Factual correctness\n",
    "    - Comprehensiveness\n",
    "    - Clarity and coherence\"\"\"\n",
    "\n",
    "    # User prompt containing the query and responses\n",
    "    user_prompt = f\"\"\"Query: {query}\n",
    "\n",
    "    Vector-based response:\n",
    "    {vector_response}\n",
    "\n",
    "    BM25 keyword response:\n",
    "    {bm25_response}\n",
    "\n",
    "    Fusion response:\n",
    "    {fusion_response}\n",
    "    \"\"\"\n",
    "\n",
    "    # Add reference answer to the prompt if provided\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "            Reference answer:\n",
    "            {reference_answer}\n",
    "        \"\"\"\n",
    "\n",
    "    # Add instructions for detailed comparison to the user prompt\n",
    "    user_prompt += \"\"\"\n",
    "    Please provide a detailed comparison of these three responses. Which approach performed best for this query and why?\n",
    "    Be specific about the strengths and weaknesses of each approach for this particular query.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the evaluation using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Specify the model to use\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the evaluator\n",
    "            {\"role\": \"user\", \"content\": user_prompt}  # User message with query and responses\n",
    "        ],\n",
    "        temperature=0  # Set the temperature for response generation\n",
    "    )\n",
    "    \n",
    "    # Return the generated evaluation content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fusion_retrieval(pdf_path, test_queries, reference_answers=None, k=5, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate fusion retrieval compared to other methods.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "        k (int): Number of documents to retrieve\n",
    "        alpha (float): Weight for vector scores in fusion retrieval\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation results\n",
    "    \"\"\"\n",
    "    print(\"=== EVALUATING FUSION RETRIEVAL ===\\n\")\n",
    "    \n",
    "    # Process the document to extract text, create chunks, and build vector and BM25 indices\n",
    "    chunks, vector_store, bm25_index = process_document(pdf_path)\n",
    "    \n",
    "    # Initialize a list to store results for each query\n",
    "    results = []\n",
    "    \n",
    "    # Iterate over each test query\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n=== Evaluating Query {i+1}/{len(test_queries)} ===\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Get the reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # Compare retrieval methods for the current query\n",
    "        comparison = compare_retrieval_methods(\n",
    "            query, \n",
    "            chunks, \n",
    "            vector_store, \n",
    "            bm25_index, \n",
    "            k=k, \n",
    "            alpha=alpha,\n",
    "            reference_answer=reference\n",
    "        )\n",
    "        \n",
    "        # Append the comparison results to the results list\n",
    "        results.append(comparison)\n",
    "        \n",
    "        # Print the responses from different retrieval methods\n",
    "        print(\"\\n=== Vector-based Response ===\")\n",
    "        print(comparison[\"vector_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== BM25 Response ===\")\n",
    "        print(comparison[\"bm25_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Fusion Response ===\")\n",
    "        print(comparison[\"fusion_result\"][\"response\"])\n",
    "        \n",
    "        print(\"\\n=== Comparison ===\")\n",
    "        print(comparison[\"comparison\"])\n",
    "    \n",
    "    # Generate an overall analysis of the fusion retrieval performance\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    # Return the results and overall analysis\n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of fusion retrieval.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from evaluating queries\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt to guide the evaluation process\n",
    "    system_prompt = \"\"\"You are an expert at evaluating information retrieval systems. \n",
    "    Based on multiple test queries, provide an overall analysis comparing three retrieval approaches:\n",
    "    1. Vector-based retrieval (semantic similarity)\n",
    "    2. BM25 keyword retrieval (keyword matching)\n",
    "    3. Fusion retrieval (combination of both)\n",
    "\n",
    "    Focus on:\n",
    "    1. Types of queries where each approach performs best\n",
    "    2. Overall strengths and weaknesses of each approach\n",
    "    3. How fusion retrieval balances the trade-offs\n",
    "    4. Recommendations for when to use each approach\"\"\"\n",
    "\n",
    "    # Create a summary of evaluations for each query\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        evaluations_summary += f\"Comparison Summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "\n",
    "    # User prompt containing the evaluations summary\n",
    "    user_prompt = f\"\"\"Based on the following evaluations of different retrieval methods across {len(results)} queries, \n",
    "    provide an overall analysis comparing these three approaches:\n",
    "\n",
    "    {evaluations_summary}\n",
    "\n",
    "    Please provide a comprehensive analysis of vector-based, BM25, and fusion retrieval approaches,\n",
    "    highlighting when and why fusion retrieval provides advantages over the individual methods.\"\"\"\n",
    "\n",
    "    # Generate the overall analysis using meta-llama/Llama-3.2-3B-Instruct\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=1\n",
    "    )\n",
    "    \n",
    "    # Return the generated analysis content\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Fusion Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING FUSION RETRIEVAL ===\n",
      "\n",
      "Extracting text from /Users/kekunkoya/Desktop/RAG Project/Resources.pdf...\n",
      "Created 14 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 14 items to vector store\n",
      "Created BM25 index with 14 documents\n",
      "\n",
      "\n",
      "=== Evaluating Query 1/1 ===\n",
      "Query: Is it safe to drink the tap water in 17104 after the flood?\n",
      "\n",
      "=== Comparing retrieval methods for query: Is it safe to drink the tap water in 17104 after the flood? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: Is it safe to drink the tap water in 17104 after the flood?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "The provided context does not contain any information regarding the safety of drinking tap water in the 17104 area after a flood. Therefore, I cannot answer your question fully. It is advisable to check with local health authorities or water management agencies for the most accurate and up-to-date information regarding water safety in that area.\n",
      "\n",
      "=== BM25 Response ===\n",
      "The provided context does not contain any information regarding the safety of tap water in the 17104 area after a flood. Therefore, I cannot answer your question based on the available information. It is advisable to check with local authorities or health departments for guidance on water safety following a flood.\n",
      "\n",
      "=== Fusion Response ===\n",
      "The provided context does not contain any information regarding the safety of tap water in the 17104 area after a flood. Therefore, I cannot answer your question based on the available information. It is advisable to check with local health authorities or water quality agencies for specific guidance on this matter.\n",
      "\n",
      "=== Comparison ===\n",
      "### Comparison of Responses\n",
      "\n",
      "#### 1. Vector-based Retrieval Response\n",
      "- **Relevance to the Query**: The response directly addresses the query by stating that there is no information available regarding the safety of tap water in the specified area after a flood. However, it lacks specific details about the potential risks associated with drinking tap water post-flood.\n",
      "- **Factual Correctness**: The response is factually correct in stating that it cannot provide an answer based on the available context. However, it misses the opportunity to provide general knowledge about the risks of drinking tap water after a flood.\n",
      "- **Comprehensiveness**: The response is limited in comprehensiveness as it does not include any information about the potential contamination of water or recommended actions (e.g., boiling water).\n",
      "- **Clarity and Coherence**: The response is clear and coherent, but it could be more informative.\n",
      "\n",
      "#### 2. BM25 Keyword Retrieval Response\n",
      "- **Relevance to the Query**: Similar to the vector-based response, it acknowledges the lack of information regarding the safety of tap water in the area after a flood. It also suggests checking with local authorities.\n",
      "- **Factual Correctness**: The response is factually correct but does not provide any additional context or information about the risks associated with drinking tap water after a flood.\n",
      "- **Comprehensiveness**: Like the vector-based response, it lacks comprehensiveness. It does not mention the potential for contamination or any specific actions to take.\n",
      "- **Clarity and Coherence**: The response is clear and coherent, but it is also somewhat generic and lacks depth.\n",
      "\n",
      "#### 3. Fusion Retrieval Response\n",
      "- **Relevance to the Query**: The fusion response mirrors the previous two responses in acknowledging the lack of specific information. It also suggests checking with local health authorities.\n",
      "- **Factual Correctness**: The response is factually correct but, like the others, fails to provide critical information about the risks of drinking tap water after a flood.\n",
      "- **Comprehensiveness**: The fusion response does not enhance the comprehensiveness compared to the other two approaches. It does not provide any actionable advice or context about water safety.\n",
      "- **Clarity and Coherence**: The response is clear and coherent, but it does not add any new insights or information.\n",
      "\n",
      "### Overall Evaluation\n",
      "- **Strengths**:\n",
      "  - All three responses correctly identify the lack of specific information in the provided context.\n",
      "  - They all suggest checking with local authorities, which is a responsible recommendation.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - None of the responses provide any information about the general risks associated with drinking tap water after a flood, which is critical for the user.\n",
      "  - They all lack actionable advice, such as boiling water or using bottled water, which is essential in this context.\n",
      "\n",
      "### Best Approach\n",
      "In this case, none of the approaches performed adequately because they all failed to provide relevant information about the safety of drinking tap water after a flood. However, if we were to rank them based on their strengths, the **fusion retrieval** approach could be considered slightly better due to its potential to combine the strengths of both vector and keyword methods, even though it did not demonstrate that in this instance.\n",
      "\n",
      "### Conclusion\n",
      "For this specific query, all three approaches fell short in providing a comprehensive and informative response. A more effective retrieval system would need to incorporate general knowledge about water safety post-flood, including specific recommendations and resources, to better serve the user's needs.\n",
      "\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "Based on an evaluation of a retrieval task involving the query \"Is it safe to drink the tap water in 17104 after the flood?\", we can derive insights into the three retrieval approaches: vector-based retrieval (semantic similarity), BM25 keyword retrieval (keyword matching), and fusion retrieval (a combination of both methods). \n",
      "\n",
      "### Analysis of Retrieval Approaches\n",
      "\n",
      "#### 1. Vector-based Retrieval (Semantic Similarity)\n",
      "- **Performance**: In this example, the vector-based approach seems to struggle with providing relevance, possibly due to the lack of semantic information in the documents it indexed. While vector-based methods are generally good at understanding context and related meanings, they may lack specificity when directly addressing fact-based queries, especially if they rely heavily on semantic embeddings that do not have relevant data.\n",
      "- **Strengths**:\n",
      "  - Excels in understanding the context, synonyms, and the broader meaning of queries.\n",
      "  - Effective with conversational queries or when users seek information about complex topics (e.g., health implications).\n",
      "- **Weaknesses**:\n",
      "  - May miss specific factual information that is critical to queries, such as current safety data.\n",
      "  - Performance can vary based on the quality of semantic embeddings and the training data used to create them.\n",
      "\n",
      "#### 2. BM25 Keyword Retrieval (Keyword Matching)\n",
      "- **Performance**: BM25 shines in situations where precise keywords are present in the documents. In cases where the query comprises specific keywords without complex semantics (like \"tap water,\" \"flood,\" \"safe\"), BM25 might retrieve documents that contain these terms specifically, leading to potentially relevant responses about water safety.\n",
      "- **Strengths**:\n",
      "  - Performs well with direct keyword matching and is often effective in finding exact matches for fact-based or straightforward queries.\n",
      "  - BM25 is robust in environments where documents contain straightforward terminology and jargon.\n",
      "- **Weaknesses**:\n",
      "  - Limited in handling synonyms or variations of keywords; it may miss out on relevant documents that do not contain the exact phrasing.\n",
      "  - Less effective with nuanced or ambiguous queries where context is critical.\n",
      "\n",
      "#### 3. Fusion Retrieval (Combination of Both)\n",
      "- **Performance**: Fusion retrieval is designed to leverage the strengths of both vector-based and BM25 approaches. It can provide balanced responses that consider both semantic understanding and keyword accuracy. For example, it can identify meaning beyond keywords while still retrieving document matches that address specific factual queries.\n",
      "- **Strengths**:\n",
      "  - Balances the strengths of both approaches, thereby improving overall retrieval effectiveness.\n",
      "  - Capable of providing more comprehensive search results by merging varied insights from multiple retrieval methodologies.\n",
      "- **Weaknesses**:\n",
      "  - Implementing fusion can complicate the retrieval process and require careful tuning to ensure each method is weighted appropriately.\n",
      "  - It may require more computational resources, depending on the complexity of the algorithms being fused.\n",
      "\n",
      "### When to Use Each Approach\n",
      "\n",
      "1. **Vector-based Retrieval**:\n",
      "   - **Use When**: The queries are complex, context-driven, or conversational, requiring a deep understanding of semantics. It is also beneficial when dealing with related content that uses varied terminology.\n",
      "\n",
      "2. **BM25 Keyword Retrieval**:\n",
      "   - **Use When**: The queries are straightforward and specific, and the focus is on fact-based information retrieval. This is particularly effective when matching precise keywords and phrases.\n",
      "\n",
      "3. **Fusion Retrieval**:\n",
      "   - **Use When**: The query context is mixed or when there's a mix of broad and specific information needs. Fusion is ideal for users looking for both detailed and comprehensive answers, benefiting from both the precise matching of BM25 and the contextual understanding of vector-based approaches.\n",
      "\n",
      "### Conclusion\n",
      "In summary, the choice between vector-based retrieval, BM25 keyword retrieval, and fusion retrieval should be guided by the nature of the search queries. Vector-based retrieval excels in understanding context, BM25 is ideal for keyword-specific searches, and fusion retrieval provides a balanced approach that accommodates varied information needs while enhancing overall relevance. Fusion retrieval often delivers superior performance, especially in complex search environments, making it a desirable choice when resources allow for its effective implementation.\n"
     ]
    }
   ],
   "source": [
    "# Path to PDF document\n",
    "# Path to PDF document containing AI information for knowledge retrieval testing\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/RAG Project/Resources.pdf\"\n",
    "\n",
    "# Define a single AI-related test query\n",
    "test_queries = [\n",
    "    \"Is it safe to drink the tap water in 17104 after the flood?\"  # AI-specific query\n",
    "]\n",
    "\n",
    "# Optional reference answer\n",
    "reference_answers = [\n",
    "    \"Tap water may be contaminated after a flood. Boil for at least 1 minute or use bottled water.\\n\\nCall 2-1-1 to find water testing kits or bottled water stations near you.\",\n",
    "]\n",
    "\n",
    "# Set parameters\n",
    "k = 5  # Number of documents to retrieve\n",
    "alpha = 0.5  # Weight for vector scores (0.5 means equal weight between vector and BM25)\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_fusion_retrieval(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers,\n",
    "    k=k,\n",
    "    alpha=alpha\n",
    ")\n",
    "\n",
    "# Print overall analysis\n",
    "print(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== EVALUATING FUSION RETRIEVAL ===\n",
      "\n",
      "Extracting text from /Users/kekunkoya/Desktop/RAG Project/Resources.pdf...\n",
      "Created 14 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Added 14 items to vector store\n",
      "Created BM25 index with 14 documents\n",
      "\n",
      "\n",
      "=== Evaluating Query 1/1 ===\n",
      "Query: Can I bring my pet to a shelter in Harrisburg?\n",
      "\n",
      "=== Comparing retrieval methods for query: Can I bring my pet to a shelter in Harrisburg? ===\n",
      "\n",
      "\n",
      "Running vector-only RAG...\n",
      "\n",
      "Running BM25-only RAG...\n",
      "\n",
      "Running fusion RAG...\n",
      "Performing fusion retrieval for query: Can I bring my pet to a shelter in Harrisburg?\n",
      "Retrieved 5 documents with fusion retrieval\n",
      "\n",
      "Comparing responses...\n",
      "\n",
      "=== Vector-based Response ===\n",
      "The provided context does not contain specific information regarding pet policies at shelters in Harrisburg. Therefore, I cannot answer your question fully. It would be best to contact local shelters directly for their pet policies.\n",
      "\n",
      "=== BM25 Response ===\n",
      "The provided context does not contain any information regarding pet policies at shelters in Harrisburg. Therefore, I cannot answer your question about bringing a pet to a shelter in that area. It would be best to contact the specific shelter directly for their pet policy.\n",
      "\n",
      "=== Fusion Response ===\n",
      "The provided context does not contain information regarding pet policies at shelters in Harrisburg. Therefore, I cannot answer your question about bringing a pet to a shelter in that area. You may want to contact local shelters directly for their specific policies on pets.\n",
      "\n",
      "=== Comparison ===\n",
      "### Comparison of Responses\n",
      "\n",
      "#### 1. Vector-based Retrieval Response\n",
      "- **Relevance to the Query**: The response is relevant in that it acknowledges the lack of specific information regarding pet policies at shelters in Harrisburg. However, it does not provide any additional context or insights that could be useful to the user.\n",
      "- **Factual Correctness**: The response is factually correct in stating that it cannot provide specific information. However, it fails to mention any general practices or alternatives that might be relevant.\n",
      "- **Comprehensiveness**: The response lacks comprehensiveness as it does not offer any additional resources or suggestions beyond contacting local shelters.\n",
      "- **Clarity and Coherence**: The response is clear and coherent, but it is somewhat lacking in depth.\n",
      "\n",
      "#### 2. BM25 Keyword Retrieval Response\n",
      "- **Relevance to the Query**: Similar to the vector-based response, it directly addresses the query but does not provide any specific information about pet policies.\n",
      "- **Factual Correctness**: The response is factually correct in stating that it cannot provide specific information. However, like the vector-based response, it does not offer any additional insights.\n",
      "- **Comprehensiveness**: The response is not comprehensive, as it does not suggest any alternative resources or actions the user could take.\n",
      "- **Clarity and Coherence**: The response is clear and coherent, but it mirrors the limitations of the vector-based response.\n",
      "\n",
      "#### 3. Fusion Retrieval Response\n",
      "- **Relevance to the Query**: The fusion response is relevant and acknowledges the lack of specific information, similar to the other two responses.\n",
      "- **Factual Correctness**: It is factually correct in stating that it cannot provide specific information about pet policies.\n",
      "- **Comprehensiveness**: While it does not provide additional information, it does suggest contacting local shelters, which is a slight improvement over the other two responses.\n",
      "- **Clarity and Coherence**: The response is clear and coherent, maintaining a consistent tone with the other responses.\n",
      "\n",
      "### Overall Evaluation\n",
      "- **Strengths**:\n",
      "  - All three approaches correctly identify the lack of specific information regarding pet policies at shelters in Harrisburg.\n",
      "  - They all suggest contacting local shelters, which is a practical next step for the user.\n",
      "\n",
      "- **Weaknesses**:\n",
      "  - None of the responses provide any additional context or information that could help the user, such as general practices regarding pets in shelters or alternative resources.\n",
      "  - All responses lack comprehensiveness and do not address the broader context of pet policies in shelters.\n",
      "\n",
      "### Best Approach\n",
      "In this case, while all three approaches performed similarly in terms of relevance and factual correctness, the **fusion retrieval** approach performed slightly better due to its suggestion to contact local shelters directly. However, all responses could have been improved significantly by including more comprehensive information, such as general practices regarding pets in shelters or alternative resources like the reference answer provided.\n",
      "\n",
      "### Conclusion\n",
      "Overall, none of the approaches fully met the needs of the query, but the fusion retrieval approach edged out the others by providing a practical next step. For future improvements, incorporating more contextual information and resources would enhance the responses significantly.\n",
      "\n",
      "\n",
      "=== OVERALL ANALYSIS ===\n",
      "\n",
      "### Overall Analysis of Retrieval Approaches\n",
      "\n",
      "#### 1. Types of Queries Where Each Approach Performs Best\n",
      "\n",
      "- **Vector-based Retrieval (Semantic Similarity)**:\n",
      "  - Best suited for queries where contextual understanding is crucial. This includes natural language queries that may contain synonyms, paraphrasing, or require comprehension of the intent behind the words. \n",
      "  - Example Queries: \"Can I bring my pet to a shelter in Harrisburg?\" may be well understood in terms of semantics, capturing nuanced terms like \"bring\" and \"shelter.\"\n",
      "\n",
      "- **BM25 Keyword Retrieval (Keyword Matching)**:\n",
      "  - Excels with straightforward, factual queries where keywords from the user’s input directly correspond to the document corpus.\n",
      "  - Example Queries: Queries such as \"Harrisburg pet regulations\" or \"shelters that accept pets\" can yield better results as they rely on explicit keyword matches.\n",
      "\n",
      "- **Fusion Retrieval (Combination of Both)**:\n",
      "  - Ideal for hybrid queries that include both specific keyword needs and semantic or contextual nuances. Fusion retrieval combines outputs from both approaches to leverage their strengths.\n",
      "  - Example Queries: Complex queries that might involve regulations, locations, or specific types of shelters.\n",
      "\n",
      "#### 2. Overall Strengths and Weaknesses of Each Approach\n",
      "\n",
      "- **Vector-based Retrieval**:\n",
      "  - **Strengths**:\n",
      "    - Captures semantic relationships and contextual meanings effectively.\n",
      "    - Works well with natural language and query expansion (synonyms, related terms).\n",
      "    - Provides high relevance in cases of variances in how information is phrased.\n",
      "  - **Weaknesses**:\n",
      "    - May struggle with precision if the query is very specific, particularly in cases where the documents contain dissimilar wording.\n",
      "    - Computationally more intensive than BM25.\n",
      "  \n",
      "- **BM25 Keyword Retrieval**:\n",
      "  - **Strengths**:\n",
      "    - Fast and highly efficient for large document sets; well-suited for environments with large-scale databases.\n",
      "    - Strong at returning highly relevant documents for well-defined, keyword-driven queries.\n",
      "    - Effective in cases where information retrieval is based on specific keywords without need for additional context.\n",
      "  - **Weaknesses**:\n",
      "    - Limited ability to handle synonyms or paraphrased concepts, resulting in potential missing relevant documents.\n",
      "    - Young users might struggle to form the exact keyword matches necessary for the system to retrieve relevant documents.\n",
      "\n",
      "- **Fusion Retrieval**:\n",
      "  - **Strengths**:\n",
      "    - Harnesses the strengths of both vector-based and BM25 approaches, resulting in a more diverse set of relevant documents.\n",
      "    - Balances the need for both contextual understanding and keyword specificity, providing a richer retrieval experience.\n",
      "    - Tends to accommodate a broader spectrum of query complexity.\n",
      "  - **Weaknesses**:\n",
      "    - Complexity in implementation and the potential for conflicting signals between the two methods which might lead to diluted results in certain cases.\n",
      "    - Needs careful tuning to balance contributions from each approach effectively, and this may require trial and error or additional computational resources.\n",
      "\n",
      "#### 3. How Fusion Retrieval Balances the Trade-Offs\n",
      "\n",
      "Fusion retrieval offers a solution to the shortcomings of both vector-based and BM25 keyword approaches. By integrating the output from both methods, it compensates for situations where one approach may fail. For instance:\n",
      "\n",
      "- In queries that are semantically rich, vector-based retrieval can supply broader context, while BM25 ensures that specific keywords required for a particular answer are included in the found documents.\n",
      "- In practice, if a semantic approach leads to documents that address a query in a non-targeted way, the keyword matching ensures precision by filtering through a corpus of explicitly relevant materials.\n",
      "- The combination can also improve user satisfaction, as users may exhibit a preference for clear, precise answers that are still within a knowledgeable context.\n",
      "\n",
      "#### 4. Recommendations for When to Use Each Approach\n",
      "\n",
      "- **Vector-based Retrieval**:\n",
      "  - Recommended for content-heavy environments where understanding diverse user phrases and contextual nuances are essential. This approach is favored in applications such as academic literature searches, nuanced customer service queries, and conversational AI.\n",
      "\n",
      "- **BM25 Keyword Retrieval**:\n",
      "  - Best when the objective is clarity and speed with straightforward questions. Suitable for applications like search engines or support ticket systems that demand quick retrieval of precise factual information.\n",
      "\n",
      "- **Fusion Retrieval**:\n",
      "  - The best choice for hybrid queries or scenarios where user intent varies widely. Implement this method in environments like comprehensive knowledge bases, complex search services, and applications where users may pose questions across different levels of specificity and detail.\n",
      "\n",
      "In summary, fusion retrieval optimally addresses user needs across diverse query types, enhancing both relevance and precision in output, thereby delivering a more comprehensive information retrieval experience.\n"
     ]
    }
   ],
   "source": [
    "# Path to PDF document\n",
    "# Path to PDF document containing AI information for knowledge retrieval testing\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/RAG Project/Resources.pdf\"\n",
    "\n",
    "# Define a single AI-related test query\n",
    "test_queries = [\n",
    "    \"Can I bring my pet to a shelter in Harrisburg?\"  # AI-specific query\n",
    "]\n",
    "\n",
    "# Optional reference answer\n",
    "reference_answers = [\n",
    "   \n",
    "\n",
    "    \"Yes, some shelters allow pets or partner with nearby animal shelters.\\n\\nCall 2-1-1 to find:\\n- Pet-friendly Red Cross shelters\\n- Temporary pet boarding options\\n\\nService animals are always accepted.\"\n",
    "    \n",
    "    ]\n",
    "\n",
    "# Set parameters\n",
    "k = 5  # Number of documents to retrieve\n",
    "alpha = 0.5  # Weight for vector scores (0.5 means equal weight between vector and BM25)\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluate_fusion_retrieval(\n",
    "    pdf_path=pdf_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers,\n",
    "    k=k,\n",
    "    alpha=alpha\n",
    ")\n",
    "\n",
    "# Print overall analysis\n",
    "print(\"\\n\\n=== OVERALL ANALYSIS ===\\n\")\n",
    "print(evaluation_results[\"overall_analysis\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
