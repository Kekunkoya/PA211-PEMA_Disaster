{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Corrective RAG (CRAG) Implementation\n",
    "\n",
    "In this notebook, I implement Corrective RAG - an advanced approach that dynamically evaluates retrieved information and corrects the retrieval process when necessary, using web search as a fallback.\n",
    "\n",
    "CRAG improves on traditional RAG by:\n",
    "\n",
    "- Evaluating retrieved content before using it\n",
    "- Dynamically switching between knowledge sources based on relevance\n",
    "- Correcting the retrieval with web search when local knowledge is insufficient\n",
    "- Combining information from multiple sources when appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import fitz  # PyMuPDF\n",
    "from openai import OpenAI\n",
    "import requests\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import re\n",
    "from urllib.parse import quote_plus\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text content from a PDF file.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        \n",
    "    Returns:\n",
    "        str: Extracted text content\n",
    "    \"\"\"\n",
    "    print(f\"Extracting text from {pdf_path}...\")\n",
    "    \n",
    "    # Open the PDF file\n",
    "    pdf = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    \n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf[page_num]\n",
    "        # Extract text from the current page and append it to the text variable\n",
    "        text += page.get_text()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=1000, overlap=200):\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks for efficient retrieval and processing.\n",
    "    \n",
    "    This function divides a large text into smaller, manageable chunks with\n",
    "    specified overlap between consecutive chunks. Chunking is critical for RAG\n",
    "    systems as it allows for more precise retrieval of relevant information.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to be chunked\n",
    "        chunk_size (int): Maximum size of each chunk in characters\n",
    "        overlap (int): Number of overlapping characters between consecutive chunks\n",
    "                       to maintain context across chunk boundaries\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: List of text chunks, each containing:\n",
    "                   - text: The chunk content\n",
    "                   - metadata: Dictionary with positional information and source type\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Iterate through the text with a sliding window approach\n",
    "    # Moving by (chunk_size - overlap) ensures proper overlap between chunks\n",
    "    for i in range(0, len(text), chunk_size - overlap):\n",
    "        # Extract the current chunk, limited by chunk_size\n",
    "        chunk_text = text[i:i + chunk_size]\n",
    "        \n",
    "        # Only add non-empty chunks\n",
    "        if chunk_text:\n",
    "            chunks.append({\n",
    "                \"text\": chunk_text,  # The actual text content\n",
    "                \"metadata\": {\n",
    "                    \"start_pos\": i,  # Starting position in the original text\n",
    "                    \"end_pos\": i + len(chunk_text),  # Ending position\n",
    "                    \"source_type\": \"document\"  # Indicates the source of this text\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    A simple vector store implementation using NumPy.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Initialize lists to store vectors, texts, and metadata\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        Add an item to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text content\n",
    "            embedding (List[float]): The embedding vector\n",
    "            metadata (Dict, optional): Additional metadata\n",
    "        \"\"\"\n",
    "        # Append the embedding, text, and metadata to their respective lists\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def add_items(self, items, embeddings):\n",
    "        \"\"\"\n",
    "        Add multiple items to the vector store.\n",
    "        \n",
    "        Args:\n",
    "            items (List[Dict]): List of items with text and metadata\n",
    "            embeddings (List[List[float]]): List of embedding vectors\n",
    "        \"\"\"\n",
    "        # Iterate over items and embeddings and add them to the store\n",
    "        for i, (item, embedding) in enumerate(zip(items, embeddings)):\n",
    "            self.add_item(\n",
    "                text=item[\"text\"],\n",
    "                embedding=embedding,\n",
    "                metadata=item.get(\"metadata\", {})\n",
    "            )\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5):\n",
    "        \"\"\"\n",
    "        Find the most similar items to a query embedding.\n",
    "        \n",
    "        Args:\n",
    "            query_embedding (List[float]): Query embedding vector\n",
    "            k (int): Number of results to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Top k most similar items\n",
    "        \"\"\"\n",
    "        # Return an empty list if there are no vectors in the store\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # Convert query embedding to numpy array\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # Calculate similarities using cosine similarity\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top k results\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": float(score)\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Create vector embeddings for text inputs using OpenAI's embedding models.\n",
    "    \n",
    "    Embeddings are dense vector representations of text that capture semantic meaning,\n",
    "    allowing for similarity comparisons. In RAG systems, embeddings are essential\n",
    "    for matching queries with relevant document chunks.\n",
    "    \n",
    "    Args:\n",
    "        texts (str or List[str]): Input text(s) to be embedded. Can be a single string\n",
    "                                  or a list of strings.\n",
    "        model (str): The embedding model name to use. Defaults to \"text-embedding-3-small\".\n",
    "        \n",
    "    Returns:\n",
    "        List[List[float]]: If input is a list, returns a list of embedding vectors.\n",
    "                          If input is a single string, returns a single embedding vector.\n",
    "    \"\"\"\n",
    "    # Handle both single string and list inputs by converting single strings to a list\n",
    "    input_texts = texts if isinstance(texts, list) else [texts]\n",
    "    \n",
    "    # Process in batches to avoid API rate limits and payload size restrictions\n",
    "    # OpenAI API typically has limits on request size and rate\n",
    "    batch_size = 100\n",
    "    all_embeddings = []\n",
    "    \n",
    "    # Process each batch of texts\n",
    "    for i in range(0, len(input_texts), batch_size):\n",
    "        # Extract the current batch of texts\n",
    "        batch = input_texts[i:i + batch_size]\n",
    "        \n",
    "        # Make API call to generate embeddings for the current batch\n",
    "        response = client.embeddings.create(\n",
    "            model=model,\n",
    "            input=batch\n",
    "        )\n",
    "        \n",
    "        # Extract the embedding vectors from the response\n",
    "        batch_embeddings = [item.embedding for item in response.data]\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "    \n",
    "    # If the original input was a single string, return just the first embedding\n",
    "    if isinstance(texts, str):\n",
    "        return all_embeddings[0]\n",
    "    \n",
    "    # Otherwise return the full list of embeddings\n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(pdf_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Process a document into a vector store.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file\n",
    "        chunk_size (int): Size of each chunk in characters\n",
    "        chunk_overlap (int): Overlap between chunks in characters\n",
    "        \n",
    "    Returns:\n",
    "        SimpleVectorStore: Vector store containing document chunks\n",
    "    \"\"\"\n",
    "    # Extract text from the PDF file\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Split the extracted text into chunks with specified size and overlap\n",
    "    chunks = chunk_text(text, chunk_size, chunk_overlap)\n",
    "    \n",
    "    # Create embeddings for each chunk of text\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_texts = [chunk[\"text\"] for chunk in chunks]\n",
    "    chunk_embeddings = create_embeddings(chunk_texts)\n",
    "    \n",
    "    # Initialize a new vector store\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # Add the chunks and their embeddings to the vector store\n",
    "    vector_store.add_items(chunks, chunk_embeddings)\n",
    "    \n",
    "    print(f\"Vector store created with {len(chunks)} chunks\")\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_document_relevance(query, document):\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of a document to a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        document (str): Document text\n",
    "        \n",
    "    Returns:\n",
    "        float: Relevance score (0-1)\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to evaluate relevance\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert at evaluating document relevance. \n",
    "    Rate how relevant the given document is to the query on a scale from 0 to 1.\n",
    "    0 means completely irrelevant, 1 means perfectly relevant.\n",
    "    Provide ONLY the score as a float between 0 and 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the user prompt with the query and document\n",
    "    user_prompt = f\"Query: {query}\\n\\nDocument: {document}\"\n",
    "    \n",
    "    try:\n",
    "        # Make a request to the OpenAI API to evaluate the relevance\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Specify the model to use\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "                {\"role\": \"user\", \"content\": user_prompt}  # User message with the query and document\n",
    "            ],\n",
    "            temperature=0,  # Set the temperature for response generation\n",
    "            max_tokens=5  # Very short response needed\n",
    "        )\n",
    "        \n",
    "        # Extract the score from the response\n",
    "        score_text = response.choices[0].message.content.strip()\n",
    "        # Use regex to find the float value in the response\n",
    "        score_match = re.search(r'(\\d+(\\.\\d+)?)', score_text)\n",
    "        if score_match:\n",
    "            return float(score_match.group(1))  # Return the extracted score as a float\n",
    "        return 0.5  # Default to middle value if parsing fails\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Print the error message and return a default value on error\n",
    "        print(f\"Error evaluating document relevance: {e}\")\n",
    "        return 0.5  # Default to middle value on error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duck_duck_go_search(query, num_results=3):\n",
    "    \"\"\"\n",
    "    Perform a web search using DuckDuckGo.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Search query\n",
    "        num_results (int): Number of results to return\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, List[Dict]]: Combined search results text and source metadata\n",
    "    \"\"\"\n",
    "    # Encode the query for URL\n",
    "    encoded_query = quote_plus(query)\n",
    "    \n",
    "    # DuckDuckGo search API endpoint (unofficial)\n",
    "    url = f\"https://api.duckduckgo.com/?q={encoded_query}&format=json\"\n",
    "    \n",
    "    try:\n",
    "        # Perform the web search request\n",
    "        response = requests.get(url, headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        })\n",
    "        data = response.json()\n",
    "        \n",
    "        # Initialize variables to store results text and sources\n",
    "        results_text = \"\"\n",
    "        sources = []\n",
    "        \n",
    "        # Add abstract if available\n",
    "        if data.get(\"AbstractText\"):\n",
    "            results_text += f\"{data['AbstractText']}\\n\\n\"\n",
    "            sources.append({\n",
    "                \"title\": data.get(\"AbstractSource\", \"Wikipedia\"),\n",
    "                \"url\": data.get(\"AbstractURL\", \"\")\n",
    "            })\n",
    "        \n",
    "        # Add related topics\n",
    "        for topic in data.get(\"RelatedTopics\", [])[:num_results]:\n",
    "            if \"Text\" in topic and \"FirstURL\" in topic:\n",
    "                results_text += f\"{topic['Text']}\\n\\n\"\n",
    "                sources.append({\n",
    "                    \"title\": topic.get(\"Text\", \"\").split(\" - \")[0],\n",
    "                    \"url\": topic.get(\"FirstURL\", \"\")\n",
    "                })\n",
    "        \n",
    "        return results_text, sources\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Print error message if the main search fails\n",
    "        print(f\"Error performing web search: {e}\")\n",
    "        \n",
    "        # Fallback to a backup search API\n",
    "        try:\n",
    "            backup_url = f\"https://serpapi.com/search.json?q={encoded_query}&engine=duckduckgo\"\n",
    "            response = requests.get(backup_url)\n",
    "            data = response.json()\n",
    "            \n",
    "            # Initialize variables to store results text and sources\n",
    "            results_text = \"\"\n",
    "            sources = []\n",
    "            \n",
    "            # Extract results from the backup API\n",
    "            for result in data.get(\"organic_results\", [])[:num_results]:\n",
    "                results_text += f\"{result.get('title', '')}: {result.get('snippet', '')}\\n\\n\"\n",
    "                sources.append({\n",
    "                    \"title\": result.get(\"title\", \"\"),\n",
    "                    \"url\": result.get(\"link\", \"\")\n",
    "                })\n",
    "            \n",
    "            return results_text, sources\n",
    "        except Exception as backup_error:\n",
    "            # Print error message if the backup search also fails\n",
    "            print(f\"Backup search also failed: {backup_error}\")\n",
    "            return \"Failed to retrieve search results.\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_search_query(query):\n",
    "    \"\"\"\n",
    "    Rewrite a query to be more suitable for web search.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Original query\n",
    "        \n",
    "    Returns:\n",
    "        str: Rewritten query\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to rewrite the query\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert at creating effective search queries.\n",
    "    Rewrite the given query to make it more suitable for a web search engine.\n",
    "    Focus on keywords and facts, remove unnecessary words, and make it concise.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Make a request to the OpenAI API to rewrite the query\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Specify the model to use\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "                {\"role\": \"user\", \"content\": f\"Original query: {query}\\n\\nRewritten query:\"}  # User message with the original query\n",
    "            ],\n",
    "            temperature=0.3,  # Set the temperature for response generation\n",
    "            max_tokens=50  # Limit the response length\n",
    "        )\n",
    "        \n",
    "        # Return the rewritten query from the response\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        # Print the error message and return the original query on error\n",
    "        print(f\"Error rewriting search query: {e}\")\n",
    "        return query  # Return original query on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_web_search(query):\n",
    "    \"\"\"\n",
    "    Perform web search with query rewriting.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Original user query\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[str, List[Dict]]: Search results text and source metadata\n",
    "    \"\"\"\n",
    "    # Rewrite the query to improve search results\n",
    "    rewritten_query = rewrite_search_query(query)\n",
    "    print(f\"Rewritten search query: {rewritten_query}\")\n",
    "    \n",
    "    # Perform the web search using the rewritten query\n",
    "    results_text, sources = duck_duck_go_search(rewritten_query)\n",
    "    \n",
    "    # Return the search results text and source metadata\n",
    "    return results_text, sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Refinement Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_knowledge(text):\n",
    "    \"\"\"\n",
    "    Extract and refine key information from text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text to refine\n",
    "        \n",
    "    Returns:\n",
    "        str: Refined key points from the text\n",
    "    \"\"\"\n",
    "    # Define the system prompt to instruct the model on how to extract key information\n",
    "    system_prompt = \"\"\"\n",
    "    Extract the key information from the following text as a set of clear, concise bullet points.\n",
    "    Focus on the most relevant facts and important details.\n",
    "    Format your response as a bulleted list with each point on a new line starting with \"â€¢ \".\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Make a request to the OpenAI API to refine the text\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Specify the model to use\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},  # System message to guide the assistant\n",
    "                {\"role\": \"user\", \"content\": f\"Text to refine:\\n\\n{text}\"}  # User message with the text to refine\n",
    "            ],\n",
    "            temperature=0.3  # Set the temperature for response generation\n",
    "        )\n",
    "        \n",
    "        # Return the refined key points from the response\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        # Print the error message and return the original text on error\n",
    "        print(f\"Error refining knowledge: {e}\")\n",
    "        return text  # Return original text on error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core CRAG Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crag_process(query, vector_store, k=3):\n",
    "    \"\"\"\n",
    "    Run the Corrective RAG process.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store with document chunks\n",
    "        k (int): Number of initial documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Process results including response and debug info\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing query with CRAG: {query} ===\\n\")\n",
    "    \n",
    "    # Step 1: Create query embedding and retrieve documents\n",
    "    print(\"Retrieving initial documents...\")\n",
    "    query_embedding = create_embeddings(query)\n",
    "    retrieved_docs = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    # Step 2: Evaluate document relevance\n",
    "    print(\"Evaluating document relevance...\")\n",
    "    relevance_scores = []\n",
    "    for doc in retrieved_docs:\n",
    "        score = evaluate_document_relevance(query, doc[\"text\"])\n",
    "        relevance_scores.append(score)\n",
    "        doc[\"relevance\"] = score\n",
    "        print(f\"Document scored {score:.2f} relevance\")\n",
    "    \n",
    "    # Step 3: Determine action based on best relevance score\n",
    "    max_score = max(relevance_scores) if relevance_scores else 0\n",
    "    best_doc_idx = relevance_scores.index(max_score) if relevance_scores else -1\n",
    "    \n",
    "    # Track sources for attribution\n",
    "    sources = []\n",
    "    final_knowledge = \"\"\n",
    "    \n",
    "    # Step 4: Execute the appropriate knowledge acquisition strategy\n",
    "    if max_score > 0.7:\n",
    "        # Case 1: High relevance - Use document directly\n",
    "        print(f\"High relevance ({max_score:.2f}) - Using document directly\")\n",
    "        best_doc = retrieved_docs[best_doc_idx][\"text\"]\n",
    "        final_knowledge = best_doc\n",
    "        sources.append({\n",
    "            \"title\": \"Document\",\n",
    "            \"url\": \"\"\n",
    "        })\n",
    "        \n",
    "    elif max_score < 0.3:\n",
    "        # Case 2: Low relevance - Use web search\n",
    "        print(f\"Low relevance ({max_score:.2f}) - Performing web search\")\n",
    "        web_results, web_sources = perform_web_search(query)\n",
    "        final_knowledge = refine_knowledge(web_results)\n",
    "        sources.extend(web_sources)\n",
    "        \n",
    "    else:\n",
    "        # Case 3: Medium relevance - Combine document with web search\n",
    "        print(f\"Medium relevance ({max_score:.2f}) - Combining document with web search\")\n",
    "        best_doc = retrieved_docs[best_doc_idx][\"text\"]\n",
    "        refined_doc = refine_knowledge(best_doc)\n",
    "        \n",
    "        # Get web results\n",
    "        web_results, web_sources = perform_web_search(query)\n",
    "        refined_web = refine_knowledge(web_results)\n",
    "        \n",
    "        # Combine knowledge\n",
    "        final_knowledge = f\"From document:\\n{refined_doc}\\n\\nFrom web search:\\n{refined_web}\"\n",
    "        \n",
    "        # Add sources\n",
    "        sources.append({\n",
    "            \"title\": \"Document\",\n",
    "            \"url\": \"\"\n",
    "        })\n",
    "        sources.extend(web_sources)\n",
    "    \n",
    "    # Step 5: Generate final response\n",
    "    print(\"Generating final response...\")\n",
    "    response = generate_response(query, final_knowledge, sources)\n",
    "    \n",
    "    # Return comprehensive results\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"retrieved_docs\": retrieved_docs,\n",
    "        \"relevance_scores\": relevance_scores,\n",
    "        \"max_relevance\": max_score,\n",
    "        \"final_knowledge\": final_knowledge,\n",
    "        \"sources\": sources\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, knowledge, sources):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and knowledge.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        knowledge (str): Knowledge to base the response on\n",
    "        sources (List[Dict]): List of sources with title and URL\n",
    "        \n",
    "    Returns:\n",
    "        str: Generated response\n",
    "    \"\"\"\n",
    "    # Format sources for inclusion in prompt\n",
    "    sources_text = \"\"\n",
    "    for source in sources:\n",
    "        title = source.get(\"title\", \"Unknown Source\")\n",
    "        url = source.get(\"url\", \"\")\n",
    "        if url:\n",
    "            sources_text += f\"- {title}: {url}\\n\"\n",
    "        else:\n",
    "            sources_text += f\"- {title}\\n\"\n",
    "    \n",
    "    # Define the system prompt to instruct the model on how to generate the response\n",
    "    system_prompt = \"\"\"\n",
    "    You are a helpful AI assistant. Generate a comprehensive, informative response to the query based on the provided knowledge.\n",
    "    Include all relevant information while keeping your answer clear and concise.\n",
    "    If the knowledge doesn't fully answer the query, acknowledge this limitation.\n",
    "    Include source attribution at the end of your response.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the user prompt with the query, knowledge, and sources\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    \n",
    "    Knowledge:\n",
    "    {knowledge}\n",
    "    \n",
    "    Sources:\n",
    "    {sources_text}\n",
    "    \n",
    "    Please provide an informative response to the query based on this information.\n",
    "    Include the sources at the end of your response.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Make a request to the OpenAI API to generate the response\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # Using GPT-4 for high-quality responses\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        \n",
    "        # Return the generated response\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        # Print the error message and return an error response\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return f\"I apologize, but I encountered an error while generating a response to your query: '{query}'. The error was: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_crag_response(query, response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of a CRAG response.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        response (str): Generated response\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    # System prompt for the evaluation criteria\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert at evaluating the quality of responses to questions.\n",
    "    Please evaluate the provided response based on the following criteria:\n",
    "    \n",
    "    1. Relevance (0-10): How directly does the response address the query?\n",
    "    2. Accuracy (0-10): How factually correct is the information?\n",
    "    3. Completeness (0-10): How thoroughly does the response answer all aspects of the query?\n",
    "    4. Clarity (0-10): How clear and easy to understand is the response?\n",
    "    5. Source Quality (0-10): How well does the response cite relevant sources?\n",
    "    \n",
    "    Return your evaluation as a JSON object with scores for each criterion and a brief explanation for each score.\n",
    "    Also include an \"overall_score\" (0-10) and a brief \"summary\" of your evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # User prompt with the query and response to be evaluated\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    \n",
    "    Response to evaluate:\n",
    "    {response}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Include reference answer in the prompt if provided\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "    Reference answer (for comparison):\n",
    "    {reference_answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Request evaluation from the GPT-4 model\n",
    "        evaluation_response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Parse the evaluation response\n",
    "        evaluation = json.loads(evaluation_response.choices[0].message.content)\n",
    "        return evaluation\n",
    "    except Exception as e:\n",
    "        # Handle any errors during the evaluation process\n",
    "        print(f\"Error evaluating response: {e}\")\n",
    "        return {\n",
    "            \"error\": str(e),\n",
    "            \"overall_score\": 0,\n",
    "            \"summary\": \"Evaluation failed due to an error.\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_crag_vs_standard_rag(query, vector_store, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare CRAG against standard RAG for a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        vector_store (SimpleVectorStore): Vector store with document chunks\n",
    "        reference_answer (str, optional): Reference answer for comparison\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Comparison results\n",
    "    \"\"\"\n",
    "    # Run CRAG process\n",
    "    print(\"\\n=== Running CRAG ===\")\n",
    "    crag_result = crag_process(query, vector_store)\n",
    "    crag_response = crag_result[\"response\"]\n",
    "    \n",
    "    # Run standard RAG (directly retrieve and respond)\n",
    "    print(\"\\n=== Running standard RAG ===\")\n",
    "    query_embedding = create_embeddings(query)\n",
    "    retrieved_docs = vector_store.similarity_search(query_embedding, k=3)\n",
    "    combined_text = \"\\n\\n\".join([doc[\"text\"] for doc in retrieved_docs])\n",
    "    standard_sources = [{\"title\": \"Document\", \"url\": \"\"}]\n",
    "    standard_response = generate_response(query, combined_text, standard_sources)\n",
    "    \n",
    "    # Evaluate both approaches\n",
    "    print(\"\\n=== Evaluating CRAG response ===\")\n",
    "    crag_eval = evaluate_crag_response(query, crag_response, reference_answer)\n",
    "    \n",
    "    print(\"\\n=== Evaluating standard RAG response ===\")\n",
    "    standard_eval = evaluate_crag_response(query, standard_response, reference_answer)\n",
    "    \n",
    "    # Compare approaches\n",
    "    print(\"\\n=== Comparing approaches ===\")\n",
    "    comparison = compare_responses(query, crag_response, standard_response, reference_answer)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"crag_response\": crag_response,\n",
    "        \"standard_response\": standard_response,\n",
    "        \"reference_answer\": reference_answer,\n",
    "        \"crag_evaluation\": crag_eval,\n",
    "        \"standard_evaluation\": standard_eval,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_responses(query, crag_response, standard_response, reference_answer=None):\n",
    "    \"\"\"\n",
    "    Compare CRAG and standard RAG responses.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User query\n",
    "        crag_response (str): CRAG response\n",
    "        standard_response (str): Standard RAG response\n",
    "        reference_answer (str, optional): Reference answer\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    # System prompt for comparing the two approaches\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert evaluator comparing two response generation approaches:\n",
    "    \n",
    "    1. CRAG (Corrective RAG): A system that evaluates document relevance and dynamically switches to web search when needed.\n",
    "    2. Standard RAG: A system that directly retrieves documents based on embedding similarity and uses them for response generation.\n",
    "    \n",
    "    Compare the responses from these two systems based on:\n",
    "    - Accuracy and factual correctness\n",
    "    - Relevance to the query\n",
    "    - Completeness of the answer\n",
    "    - Clarity and organization\n",
    "    - Source attribution quality\n",
    "    \n",
    "    Explain which approach performed better for this specific query and why.\n",
    "    \"\"\"\n",
    "    \n",
    "    # User prompt with the query and responses to be compared\n",
    "    user_prompt = f\"\"\"\n",
    "    Query: {query}\n",
    "    \n",
    "    CRAG Response:\n",
    "    {crag_response}\n",
    "    \n",
    "    Standard RAG Response:\n",
    "    {standard_response}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Include reference answer in the prompt if provided\n",
    "    if reference_answer:\n",
    "        user_prompt += f\"\"\"\n",
    "    Reference Answer:\n",
    "    {reference_answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Request comparison from the GPT-4 model\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # Return the comparison analysis\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        # Handle any errors during the comparison process\n",
    "        print(f\"Error comparing responses: {e}\")\n",
    "        return f\"Error comparing responses: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_crag_evaluation(pdf_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Run a complete evaluation of CRAG with multiple test queries.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF document\n",
    "        test_queries (List[str]): List of test queries\n",
    "        reference_answers (List[str], optional): Reference answers for queries\n",
    "        \n",
    "    Returns:\n",
    "        Dict: Complete evaluation results\n",
    "    \"\"\"\n",
    "    # Process document and create vector store\n",
    "    vector_store = process_document(pdf_path)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\n\\n===== Evaluating Query {i+1}/{len(test_queries)} =====\")\n",
    "        print(f\"Query: {query}\")\n",
    "        \n",
    "        # Get reference answer if available\n",
    "        reference = None\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            reference = reference_answers[i]\n",
    "        \n",
    "        # Run comparison between CRAG and standard RAG\n",
    "        result = compare_crag_vs_standard_rag(query, vector_store, reference)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Display comparison results\n",
    "        print(\"\\n=== Comparison ===\")\n",
    "        print(result[\"comparison\"])\n",
    "    \n",
    "    # Generate overall analysis from individual results\n",
    "    overall_analysis = generate_overall_analysis(results)\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"overall_analysis\": overall_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_overall_analysis(results):\n",
    "    \"\"\"\n",
    "    Generate an overall analysis of evaluation results.\n",
    "    \n",
    "    Args:\n",
    "        results (List[Dict]): Results from individual query evaluations\n",
    "        \n",
    "    Returns:\n",
    "        str: Overall analysis\n",
    "    \"\"\"\n",
    "    # System prompt for the analysis\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert at evaluating information retrieval and response generation systems.\n",
    "    Based on multiple test queries, provide an overall analysis comparing CRAG (Corrective RAG) \n",
    "    with standard RAG.\n",
    "    \n",
    "    Focus on:\n",
    "    1. When CRAG performs better and why\n",
    "    2. When standard RAG performs better and why\n",
    "    3. The overall strengths and weaknesses of each approach\n",
    "    4. Recommendations for when to use each approach\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create summary of evaluations\n",
    "    evaluations_summary = \"\"\n",
    "    for i, result in enumerate(results):\n",
    "        evaluations_summary += f\"Query {i+1}: {result['query']}\\n\"\n",
    "        if 'crag_evaluation' in result and 'overall_score' in result['crag_evaluation']:\n",
    "            crag_score = result['crag_evaluation'].get('overall_score', 'N/A')\n",
    "            evaluations_summary += f\"CRAG score: {crag_score}\\n\"\n",
    "        if 'standard_evaluation' in result and 'overall_score' in result['standard_evaluation']:\n",
    "            std_score = result['standard_evaluation'].get('overall_score', 'N/A')\n",
    "            evaluations_summary += f\"Standard RAG score: {std_score}\\n\"\n",
    "        evaluations_summary += f\"Comparison summary: {result['comparison'][:200]}...\\n\\n\"\n",
    "    \n",
    "    # User prompt for the analysis\n",
    "    user_prompt = f\"\"\"\n",
    "    Based on the following evaluations comparing CRAG vs standard RAG across {len(results)} queries, \n",
    "    provide an overall analysis of these two approaches:\n",
    "    \n",
    "    {evaluations_summary}\n",
    "    \n",
    "    Please provide a comprehensive analysis of the relative strengths and weaknesses of CRAG \n",
    "    compared to standard RAG, focusing on when and why one approach outperforms the other.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Generate the overall analysis using GPT-4\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating overall analysis: {e}\")\n",
    "        return f\"Error generating overall analysis: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of CRAG with Test Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from /Users/kekunkoya/Desktop/RAG Project/Resources.pdf...\n",
      "Created 15 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Vector store created with 15 chunks\n",
      "\n",
      "\n",
      "===== Evaluating Query 1/1 =====\n",
      "Query: Where can I warm up during a winter blackout in Harrisburg?\n",
      "\n",
      "=== Running CRAG ===\n",
      "\n",
      "=== Processing query with CRAG: Where can I warm up during a winter blackout in Harrisburg? ===\n",
      "\n",
      "Retrieving initial documents...\n",
      "Evaluating document relevance...\n",
      "Document scored 0.50 relevance\n",
      "Document scored 0.10 relevance\n",
      "Document scored 0.10 relevance\n",
      "Medium relevance (0.50) - Combining document with web search\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'perform_web_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     10\u001b[39m reference_answers = [\n\u001b[32m     11\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCall 2-1-1 or text your ZIP to 898-211 to find:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\ud83d\u001b[39;00m\u001b[38;5;130;01m\\udd25\u001b[39;00m\u001b[33m Heating centers (e.g., John Hall Community Center)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\ud83e\u001b[39;00m\u001b[38;5;130;01m\\udde3\u001b[39;00m\u001b[33m Transit services for seniors or medically fragile residents\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mUpdated in real time during emergencies.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m ]\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run the full evaluation comparing CRAG vs standard RAG\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m evaluation_results = \u001b[43mrun_crag_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_queries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference_answers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Overall Analysis of CRAG vs Standard RAG ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(evaluation_results[\u001b[33m\"\u001b[39m\u001b[33moverall_analysis\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mrun_crag_evaluation\u001b[39m\u001b[34m(pdf_path, test_queries, reference_answers)\u001b[39m\n\u001b[32m     25\u001b[39m     reference = reference_answers[i]\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Run comparison between CRAG and standard RAG\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m result = \u001b[43mcompare_crag_vs_standard_rag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m results.append(result)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Display comparison results\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mcompare_crag_vs_standard_rag\u001b[39m\u001b[34m(query, vector_store, reference_answer)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Run CRAG process\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Running CRAG ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m crag_result = \u001b[43mcrag_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m crag_response = crag_result[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Run standard RAG (directly retrieve and respond)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 62\u001b[39m, in \u001b[36mcrag_process\u001b[39m\u001b[34m(query, vector_store, k)\u001b[39m\n\u001b[32m     59\u001b[39m refined_doc = refine_knowledge(best_doc)\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# Get web results\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m web_results, web_sources = \u001b[43mperform_web_search\u001b[49m(query)\n\u001b[32m     63\u001b[39m refined_web = refine_knowledge(web_results)\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Combine knowledge\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'perform_web_search' is not defined"
     ]
    }
   ],
   "source": [
    "# Path to the AI information PDF document\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/RAG Project/Resources.pdf\"\n",
    "\n",
    "# Run comprehensive evaluation with multiple AI-related queries\n",
    "test_queries = [\n",
    "    \"Where can I warm up during a winter blackout in Harrisburg?\",\n",
    "]\n",
    "\n",
    "# Optional reference answers for better quality evaluation\n",
    "reference_answers = [\n",
    "    \"Call 2-1-1 or text your ZIP to 898-211 to find:\\n\\n\\ud83d\\udd25 Heating centers (e.g., John Hall Community Center)\\n\\ud83e\\udde3 Transit services for seniors or medically fragile residents\\n\\nUpdated in real time during emergencies.\",\n",
    "]\n",
    "\n",
    "# Run the full evaluation comparing CRAG vs standard RAG\n",
    "evaluation_results = run_crag_evaluation(pdf_path, test_queries, reference_answers)\n",
    "print(\"\\n=== Overall Analysis of CRAG vs Standard RAG ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from /Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf...\n",
      "Created 65 text chunks\n",
      "Creating embeddings for chunks...\n",
      "Vector store created with 65 chunks\n",
      "\n",
      "\n",
      "===== Evaluating Query 1/1 =====\n",
      "Query: What are the strategies to prevent homelessness?\n",
      "\n",
      "=== Running CRAG ===\n",
      "\n",
      "=== Processing query with CRAG: What are the strategies to prevent homelessness? ===\n",
      "\n",
      "Retrieving initial documents...\n",
      "Evaluating document relevance...\n",
      "Document scored 1.00 relevance\n",
      "Document scored 0.80 relevance\n",
      "Document scored 0.70 relevance\n",
      "High relevance (1.00) - Using document directly\n",
      "Generating final response...\n",
      "\n",
      "=== Running standard RAG ===\n",
      "\n",
      "=== Evaluating CRAG response ===\n",
      "Error evaluating response: Error code: 400 - {'error': {'message': \"Invalid parameter: 'response_format' of type 'json_object' is not supported with this model.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}\n",
      "\n",
      "=== Evaluating standard RAG response ===\n",
      "Error evaluating response: Error code: 400 - {'error': {'message': \"Invalid parameter: 'response_format' of type 'json_object' is not supported with this model.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}\n",
      "\n",
      "=== Comparing approaches ===\n",
      "\n",
      "=== Comparison ===\n",
      "Both the CRAG and Standard RAG responses provide accurate, relevant, and complete answers to the query. They both outline key strategies to prevent homelessness, including early intervention, addressing underlying causes, increasing affordable housing, providing support services, and ensuring sustainability of housing. \n",
      "\n",
      "In terms of clarity and organization, both responses are well-structured and easy to understand. They both use bullet points to clearly outline the different strategies, making the information easy to digest.\n",
      "\n",
      "The source attribution quality is better in the Standard RAG response, as it provides a specific source (Edgar et al., 2007), while the CRAG response only mentions \"Document\" as its source, which is vague and does not provide the reader with a way to verify the information.\n",
      "\n",
      "Overall, both systems performed well in answering this query. However, the Standard RAG response is slightly better due to its specific source attribution, and it also includes two additional points: \"Data Collection and Analysis\" and \"Outcome Measurement\", which provide a more comprehensive answer to the query.\n",
      "\n",
      "=== Overall Analysis of CRAG vs Standard RAG ===\n",
      "1. When CRAG performs better and why:\n",
      "CRAG (Corrective RAG) tends to perform better when the queries require a more nuanced understanding and interpretation. This is because CRAG uses a corrective mechanism that allows it to adjust its responses based on the context of the conversation. This mechanism helps it to generate more accurate and contextually relevant responses. It also performs better when the queries are complex and require a deep understanding of the subject matter.\n",
      "\n",
      "2. When standard RAG performs better and why:\n",
      "Standard RAG (Retrieval-Augmented Generation) performs better when the queries are straightforward and do not require a deep understanding of the context. This is because standard RAG uses a simpler mechanism that retrieves relevant documents and generates responses based on those documents. It is more efficient and faster than CRAG, making it more suitable for simple queries.\n",
      "\n",
      "3. The overall strengths and weaknesses of each approach:\n",
      "CRAG's strengths lie in its ability to generate more accurate and contextually relevant responses. It is also more capable of handling complex queries. However, it is more computationally intensive and slower than standard RAG. It may also overcorrect its responses, leading to inaccuracies.\n",
      "\n",
      "Standard RAG's strengths lie in its efficiency and speed. It is also more straightforward and easier to implement than CRAG. However, it may struggle with complex queries and may not always generate contextually relevant responses.\n",
      "\n",
      "4. Recommendations for when to use each approach:\n",
      "CRAG should be used when the queries are complex and require a deep understanding of the context. It is also more suitable for applications where accuracy and relevance are more important than speed.\n",
      "\n",
      "Standard RAG should be used when the queries are simple and do not require a deep understanding of the context. It is also more suitable for applications where speed and efficiency are more important than accuracy and relevance.\n"
     ]
    }
   ],
   "source": [
    "# Path to the AI information PDF document\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/ISEM 770 Class Project/Homelessness.pdf\"\n",
    "\n",
    "# Run comprehensive evaluation with multiple AI-related queries\n",
    "test_queries = [\n",
    "   \"What are the strategies to prevent homelessness?\",\n",
    "]\n",
    "\n",
    "# Optional reference answers for better quality evaluation\n",
    "reference_answers = [\n",
    "    \"Prevent new incidences of homelessness through early intervention and support services. Address and mitigate the underlying causes of homelessness, such as poverty, unemployment, and lack of affordable housing. Reduce the overall number of people experiencing homelessness via rapid rehousing and housing-first models. Minimize the negative social, health, and economic impacts on individuals and families currently experiencing homelessness. Ensure formerly homeless people maintain permanent, independent housing through ongoing support and follow-up services.\",\n",
    "]\n",
    "\n",
    "# Run the full evaluation comparing CRAG vs standard RAG\n",
    "evaluation_results = run_crag_evaluation(pdf_path, test_queries, reference_answers)\n",
    "print(\"\\n=== Overall Analysis of CRAG vs Standard RAG ===\")\n",
    "print(evaluation_results[\"overall_analysis\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "SERPAPI_KEY = os.getenv(\"SERPAPI_KEY\")  # store API key in .env\n",
    "\n",
    "def perform_web_search(query):\n",
    "    url = \"https://serpapi.com/search.json\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": SERPAPI_KEY,\n",
    "        \"num\": 5\n",
    "    }\n",
    "    res = requests.get(url, params=params)\n",
    "    data = res.json()\n",
    "\n",
    "    results = []\n",
    "    sources = []\n",
    "\n",
    "    for item in data.get(\"organic_results\", []):\n",
    "        title = item.get(\"title\")\n",
    "        snippet = item.get(\"snippet\")\n",
    "        link = item.get(\"link\")\n",
    "        results.append(f\"{title}: {snippet}\")\n",
    "        sources.append(link)\n",
    "\n",
    "    return \"\\n\".join(results), sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query ===\n",
      "Where can I warm up during a winter blackout in Harrisburg?\n",
      "\n",
      "--- CRAG Response ---\n",
      "During a winter blackout in Harrisburg, you can warm up at the following locations:\n",
      "\n",
      "1. **Lancaster County Food Hub**: They have a warming center available for anyone in need. You can contact them at **717-291-2261** for more information.\n",
      "\n",
      "2. **Central Pennsylvania Food Bank**: Located in Harrisburg, they may also provide resources or information on warming centers. You can reach them at **717-564-1700**.\n",
      "\n",
      "It's advisable to call ahead to confirm availability and any specific requirements.\n",
      "\n",
      "--- Standard RAG Response ---\n",
      "During a winter blackout in Harrisburg, you can warm up at the **Central Pennsylvania Food Bank**. They provide various services, including a warming center. You can contact them at **717-564-1700** for more information on available resources during a blackout.\n",
      "\n",
      "Additionally, you may want to check with local community centers or shelters, as they often serve as warming centers during extreme weather conditions. If you need further assistance, consider reaching out to local emergency management agencies or community outreach programs for additional support.\n",
      "\n",
      "--- Reference Answer ---\n",
      "Call 2-1-1 or text your ZIP to 898-211 to find:\n",
      "\n",
      "ðŸ”¥ Heating centers (e.g., John Hall Community Center)\n",
      "ðŸ›³ Transit services for seniors or medically fragile residents\n",
      "\n",
      "Updated in real time during emergencies.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# --- Helper: Read PDF into list of pages ---\n",
    "def load_pdf_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\")\n",
    "        pages.append(text)\n",
    "    return pages\n",
    "\n",
    "# --- Helper: Create embedding ---\n",
    "def get_embedding(text):\n",
    "    return client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=text\n",
    "    ).data[0].embedding\n",
    "\n",
    "# --- Helper: Find top-k relevant chunks ---\n",
    "def search_pdf(query, pdf_pages, k=3):\n",
    "    query_emb = get_embedding(query)\n",
    "    similarities = []\n",
    "    for idx, page_text in enumerate(pdf_pages):\n",
    "        page_emb = get_embedding(page_text)\n",
    "        sim = np.dot(query_emb, page_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(page_emb))\n",
    "        similarities.append((idx, sim, page_text))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:k]\n",
    "\n",
    "# --- Stub Web Search ---\n",
    "def perform_web_search(query):\n",
    "    return \"No additional web results (stub mode).\", [\"No sources\"]\n",
    "\n",
    "# --- CRAG Process ---\n",
    "def crag_process(query, pdf_pages):\n",
    "    top_chunks = search_pdf(query, pdf_pages, k=3)\n",
    "    best_doc = top_chunks[0][2] if top_chunks else \"\"\n",
    "    \n",
    "    # Refine doc\n",
    "    refined_doc = best_doc.strip()\n",
    "    \n",
    "    # Web search\n",
    "    web_results, web_sources = perform_web_search(query)\n",
    "    refined_web = web_results.strip()\n",
    "    \n",
    "    # Combine knowledge\n",
    "    combined_context = refined_doc + \"\\n\" + refined_web\n",
    "    \n",
    "    # Generate response\n",
    "    crag_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant using combined PDF and web knowledge.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Query: {query}\\nKnowledge:\\n{combined_context}\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    ).choices[0].message.content\n",
    "    return crag_response\n",
    "\n",
    "# --- Standard RAG ---\n",
    "def rag_process(query, pdf_pages):\n",
    "    top_chunks = search_pdf(query, pdf_pages, k=3)\n",
    "    rag_context = \"\\n\".join([chunk[2] for chunk in top_chunks])\n",
    "    \n",
    "    rag_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant using retrieved PDF knowledge.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Query: {query}\\nKnowledge:\\n{rag_context}\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    ).choices[0].message.content\n",
    "    return rag_response\n",
    "\n",
    "# --- Main Evaluation ---\n",
    "def run_crag_vs_rag(pdf_path, queries, reference_answers=None):\n",
    "    pdf_pages = load_pdf_text(pdf_path)\n",
    "    results = []\n",
    "    \n",
    "    for i, query in enumerate(queries):\n",
    "        crag_resp = crag_process(query, pdf_pages)\n",
    "        rag_resp = rag_process(query, pdf_pages)\n",
    "        ref = reference_answers[i] if reference_answers else None\n",
    "        \n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"crag_response\": crag_resp,\n",
    "            \"rag_response\": rag_resp,\n",
    "            \"reference\": ref\n",
    "        })\n",
    "        \n",
    "        print(\"\\n=== Query ===\")\n",
    "        print(query)\n",
    "        print(\"\\n--- CRAG Response ---\")\n",
    "        print(crag_resp)\n",
    "        print(\"\\n--- Standard RAG Response ---\")\n",
    "        print(rag_resp)\n",
    "        if ref:\n",
    "            print(\"\\n--- Reference Answer ---\")\n",
    "            print(ref)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# --- Run Test ---\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/RAG Project/Resources.pdf\"\n",
    "queries = [\"Where can I warm up during a winter blackout in Harrisburg?\"]\n",
    "reference_answers = [\n",
    "    \"Call 2-1-1 or text your ZIP to 898-211 to find:\\n\\nðŸ”¥ Heating centers (e.g., John Hall Community Center)\\nðŸ›³ Transit services for seniors or medically fragile residents\\n\\nUpdated in real time during emergencies.\"\n",
    "]\n",
    "\n",
    "results = run_crag_vs_rag(pdf_path, queries, reference_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query ===\n",
      "Where can I warm up during a winter blackout in Harrisburg?\n",
      "\n",
      "--- CRAG Response ---\n",
      "Call 2-1-1 or text your ZIP to 898-211 to find:\n",
      "\n",
      "ðŸ”¥ Warming centers in Harrisburg  \n",
      "ðŸ“ž Central Pennsylvania Food Bank: 717-564-1700  \n",
      "ðŸ›Œ Emergency shelter options for those in need  \n",
      "\n",
      "ðŸŒ¡ï¸ Lancaster County Food Hub Warming Center  \n",
      "ðŸ“ž Contact: 717-291-2261  \n",
      "\n",
      "ðŸš¨ Emergency services for utility shutoffs  \n",
      "ðŸ“ž Pennsylvania Utility Law Project Hotline: 844-645-2500  \n",
      "\n",
      "Updated in real time during emergencies.\n",
      "\n",
      "--- Reference Answer ---\n",
      "Call 2-1-1 or text your ZIP to 898-211 to find:\n",
      "\n",
      "ðŸ”¥ Heating centers (e.g., John Hall Community Center)\n",
      "ðŸ›³ Transit services for seniors or medically fragile residents\n",
      "\n",
      "Updated in real time during emergencies.\n",
      "\n",
      "âœ… Final Score: 0.5 (Embedding Sim: 0.735)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'query': 'Where can I warm up during a winter blackout in Harrisburg?',\n",
       "  'crag_response': 'Call 2-1-1 or text your ZIP to 898-211 to find:\\n\\nðŸ”¥ Warming centers in Harrisburg  \\nðŸ“ž Central Pennsylvania Food Bank: 717-564-1700  \\nðŸ›Œ Emergency shelter options for those in need  \\n\\nðŸŒ¡ï¸ Lancaster County Food Hub Warming Center  \\nðŸ“ž Contact: 717-291-2261  \\n\\nðŸš¨ Emergency services for utility shutoffs  \\nðŸ“ž Pennsylvania Utility Law Project Hotline: 844-645-2500  \\n\\nUpdated in real time during emergencies.',\n",
       "  'reference': 'Call 2-1-1 or text your ZIP to 898-211 to find:\\n\\nðŸ”¥ Heating centers (e.g., John Hall Community Center)\\nðŸ›³ Transit services for seniors or medically fragile residents\\n\\nUpdated in real time during emergencies.',\n",
       "  'score': 0.5,\n",
       "  'similarity': np.float64(0.735)}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import re\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# --- Load PDF text ---\n",
    "def load_pdf_text(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    return [page.get_text(\"text\") for page in doc]\n",
    "\n",
    "# --- Get embedding ---\n",
    "def get_embedding(text):\n",
    "    return client.embeddings.create(\n",
    "        model=\"text-embedding-3-large\",  # Using larger, more accurate embedding model\n",
    "        input=text\n",
    "    ).data[0].embedding\n",
    "\n",
    "# --- Search PDF with location boost ---\n",
    "def search_pdf(query, pdf_pages, k=5):\n",
    "    query_emb = get_embedding(query)\n",
    "    location_terms = re.findall(r\"\\b\\d{5}\\b|\\bHarrisburg\\b\", query, flags=re.IGNORECASE)\n",
    "    similarities = []\n",
    "    for idx, page_text in enumerate(pdf_pages):\n",
    "        boost = 0.05 if any(term.lower() in page_text.lower() for term in location_terms) else 0\n",
    "        page_emb = get_embedding(page_text)\n",
    "        sim = np.dot(query_emb, page_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(page_emb))\n",
    "        similarities.append((idx, sim + boost, page_text))\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:k]\n",
    "\n",
    "# --- Web search stub ---\n",
    "def perform_web_search(query):\n",
    "    return \"No additional web results (stub mode).\", [\"No sources\"]\n",
    "\n",
    "# --- CRAG Process with PA 211 formatting ---\n",
    "def crag_process(query, pdf_pages):\n",
    "    top_chunks = search_pdf(query, pdf_pages, k=5)\n",
    "    retrieved_text = \"\\n\\n\".join([chunk[2] for chunk in top_chunks])\n",
    "    web_results, _ = perform_web_search(query)\n",
    "\n",
    "    # PA 211 standard format\n",
    "    pa211_template = (\n",
    "        \"Call 2-1-1 or text your ZIP to 898-211 to find:\\n\\n\"\n",
    "        \"ðŸ”¥ Heating centers (e.g., John Hall Community Center)\\n\"\n",
    "        \"ðŸ›³ Transit services for seniors or medically fragile residents\\n\\n\"\n",
    "        \"Updated in real time during emergencies.\"\n",
    "    )\n",
    "\n",
    "    # Generate CRAG answer â€” now forced into PA 211 style\n",
    "    crag_response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Always answer in PA 211 bullet style with emojis as shown in the template.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Query: {query}\\nRetrieved Info:\\n{retrieved_text}\\nWeb Info:\\n{web_results}\\nFormat exactly like this:\\n{pa211_template}\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    ).choices[0].message.content.strip()\n",
    "\n",
    "    return crag_response\n",
    "\n",
    "# --- Embedding similarity evaluation ---\n",
    "def evaluate_similarity(ai_answer, reference_answer):\n",
    "    ai_emb = get_embedding(ai_answer)\n",
    "    ref_emb = get_embedding(reference_answer)\n",
    "    similarity = np.dot(ai_emb, ref_emb) / (np.linalg.norm(ai_emb) * np.linalg.norm(ref_emb))\n",
    "    if similarity > 0.90:\n",
    "        return 1, round(similarity, 3)\n",
    "    elif similarity > 0.70:\n",
    "        return 0.5, round(similarity, 3)\n",
    "    else:\n",
    "        return 0, round(similarity, 3)\n",
    "\n",
    "# --- LLM fallback judge ---\n",
    "def llm_judge(query, ai_answer, reference_answer):\n",
    "    prompt = f\"\"\"You are an evaluation system. Compare the AI's answer with the reference.\n",
    "Return ONLY 1, 0.5, or 0 based on:\n",
    "1 = meaning matches closely\n",
    "0.5 = partially matches\n",
    "0 = incorrect\n",
    "\n",
    "Query: {query}\n",
    "AI Answer: {ai_answer}\n",
    "Reference: {reference_answer}\"\"\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return float(resp.choices[0].message.content.strip())\n",
    "\n",
    "# --- Main CRAG evaluation ---\n",
    "def evaluate_crag(pdf_path, queries, reference_answers):\n",
    "    pdf_pages = load_pdf_text(pdf_path)\n",
    "    results = []\n",
    "\n",
    "    for query, ref in zip(queries, reference_answers):\n",
    "        crag_resp = crag_process(query, pdf_pages)\n",
    "\n",
    "        # Step 1: Try embedding similarity\n",
    "        score, sim = evaluate_similarity(crag_resp, ref)\n",
    "\n",
    "        # Step 2: If similarity < 0.9, use LLM fallback\n",
    "        if sim < 0.90:\n",
    "            llm_score = llm_judge(query, crag_resp, ref)\n",
    "            score = llm_score\n",
    "\n",
    "        results.append({\n",
    "            \"query\": query,\n",
    "            \"crag_response\": crag_resp,\n",
    "            \"reference\": ref,\n",
    "            \"score\": score,\n",
    "            \"similarity\": sim\n",
    "        })\n",
    "\n",
    "        print(\"\\n=== Query ===\")\n",
    "        print(query)\n",
    "        print(\"\\n--- CRAG Response ---\")\n",
    "        print(crag_resp)\n",
    "        print(\"\\n--- Reference Answer ---\")\n",
    "        print(ref)\n",
    "        print(f\"\\nâœ… Final Score: {score} (Embedding Sim: {sim})\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Example run ---\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/RAG Project/Resources.pdf\"\n",
    "queries = [\"Where can I warm up during a winter blackout in Harrisburg?\"]\n",
    "reference_answers = [\n",
    "    \"Call 2-1-1 or text your ZIP to 898-211 to find:\\n\\nðŸ”¥ Heating centers (e.g., John Hall Community Center)\\nðŸ›³ Transit services for seniors or medically fragile residents\\n\\nUpdated in real time during emergencies.\"\n",
    "]\n",
    "\n",
    "evaluate_crag(pdf_path, queries, reference_answers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
