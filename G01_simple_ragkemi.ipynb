{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Introduction to Simple RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a hybrid approach that combines information retrieval with generative models. It enhances the performance of language models by incorporating external knowledge, which improves accuracy and factual correctness.\n",
    "\n",
    "In a Simple RAG setup, we follow these steps:\n",
    "\n",
    "1. **Data Ingestion**: Load and preprocess the text data.\n",
    "2. **Chunking**: Break the data into smaller chunks to improve retrieval performance.\n",
    "3. **Embedding Creation**: Convert the text chunks into numerical representations using an embedding model.\n",
    "4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n",
    "5. **Response Generation**: Use a language model to generate a response based on retrieved text.\n",
    "\n",
    "This notebook implements a Simple RAG approach, evaluates the model’s response, and explores various improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPyPDF2\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PdfReader \u001b[38;5;66;03m# Used for PDF extraction as per your original code\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpairwise\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgenerativeai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgenai\u001b[39;00m \u001b[38;5;66;03m# Import the Gemini library\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from PyPDF2 import PdfReader # Used for PDF extraction as per your original code\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import google.generativeai as genai # Import the Gemini library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted text from the entire PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    doc = fitz.open(pdf_path)\n",
    "    all_text = []\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page in doc:\n",
    "        all_text.append(page.get_text(\"text\"))\n",
    "\n",
    "    doc.close()\n",
    "    return \"\\n\".join(all_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "\n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Generative AI configured successfully.\n",
      "Chat model initialized.\n",
      "Embedding model initialized.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# --- Configure Google Generative AI client ---\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"Error: GOOGLE_API_KEY environment variable is not set.\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring Google Generative AI: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# --- Initialize models ---\n",
    "# For chat completions\n",
    "chat_model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "# For embeddings\n",
    "embedding_model = genai.GenerativeModel(\"text-embedding-004\")\n",
    "\n",
    "# Example: verify configuration\n",
    "print(\"Google Generative AI configured successfully.\")\n",
    "print(\"Chat model initialized.\")\n",
    "print(\"Embedding model initialized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Chunking Text from a PDF File\n",
    "Now, we load the PDF, extract text, and split it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 69\n",
      "\n",
      "First text chunk:\n",
      "PENNSYLVANIA\n",
      "EMERGENCY\n",
      "PREPAREDNESS\n",
      "GUIDE\n",
      "Be Informed. Be Prepared. Be Involved. \n",
      "www.Ready.PA.gov \n",
      "readypa@pa.gov\n",
      "\n",
      "Emergency Preparedness Guide. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Table of Contents\n",
      "TABLE OF CONTENTS  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Pages 2-3\n",
      "INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Page    4\n",
      "TOP 10 EMERGENCIES . . . . . . . . . . . . . . . . . . . . . . Pages 4-7         \n",
      "       \n",
      "       \n",
      "     \n",
      "Floods • Fires • Winter Storms • Tropical Storms, Tornadoes \n",
      "and Thunderstorms • Influenza (Flu) Pandemic • Hazardous \n",
      "Material Incidents • Earthquakes and Landslides • Nuclear \n",
      "Threat • Dam Failures • Terrorism. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "BE PREPARED – MAKE A PLAN   .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    Pages 10-11      \n",
      "How to Make a Fam\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/RAG Google/PEMA.pdf\"\n",
    "\n",
    "# Extract text from the PDF file\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Chunk the extracted text into segments of 1000 characters with an overlap of 200 characters\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings transform text into numerical vectors, which allow for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# make sure you’ve already done:\n",
    "# genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "def create_embeddings(text, model_name=\"text-embedding-004\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for the given text using the specified Google AI model.\n",
    "\n",
    "    Args:\n",
    "      text (str or list of str): The input text(s) for which embeddings are to be created.\n",
    "      model_name (str): The model to be used for creating embeddings.\n",
    "\n",
    "    Returns:\n",
    "      list: A single embedding (list of floats) or a list of embeddings.\n",
    "    \"\"\"\n",
    "    # Normalize to a list of strings\n",
    "    inputs = text if isinstance(text, list) else [text]\n",
    "\n",
    "    try:\n",
    "        response = genai.embed_content(\n",
    "            model=model_name,\n",
    "            content=inputs,\n",
    "            task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "        )\n",
    "        # response['embedding'] should be a list of lists of floats\n",
    "        embeddings = response['embedding']\n",
    "\n",
    "        # If the original input was a single string, return just its embedding\n",
    "        if not isinstance(text, list):\n",
    "            return embeddings[0]\n",
    "\n",
    "        # Otherwise return the full list of embeddings\n",
    "        return embeddings\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating embeddings: {e}\")\n",
    "        return []\n",
    "\n",
    "# Usage example\n",
    "text_chunks = [\n",
    "    \"First document to embed.\",\n",
    "    \"Second document to embed.\"\n",
    "]\n",
    "\n",
    "chunk_embeddings = create_embeddings(text_chunks, model_name=\"text-embedding-004\")\n",
    "# chunk_embeddings is now [[…], […]] – a list of float arrays\n",
    "\n",
    "# If you need to mimic OpenAI’s format elsewhere:\n",
    "formatted_chunk_embeddings = [{\"embedding\": emb} for emb in chunk_embeddings]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search\n",
    "We implement cosine similarity to find the most relevant text chunks for a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): The first vector.\n",
    "    vec2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    # Handle zero vectors to avoid division by zero\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
    "        return 0.0\n",
    "    return np.dot(vec1, vec2) / (norm_vec1 * norm_vec2)\n",
    "\n",
    "def context_enriched_search(query, text_chunks, embeddings, k=1, context_size=1):\n",
    "    \"\"\"\n",
    "    Retrieves the most relevant chunk along with its neighboring chunks.\n",
    "\n",
    "    Args:\n",
    "    query (str): Search query.\n",
    "    text_chunks (List[str]): List of text chunks.\n",
    "    embeddings (List[dict]): List of chunk embeddings, where each dict has an 'embedding' key.\n",
    "    k (int): Number of relevant chunks to retrieve (currently only k=1 supported for simplicity).\n",
    "    context_size (int): Number of neighboring chunks to include.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: Relevant text chunks with contextual information.\n",
    "    \"\"\"\n",
    "    # Convert the query into an embedding vector\n",
    "    # The create_embeddings function for a single string returns a list containing one embedding\n",
    "    query_embedding = create_embeddings(query, model_name='text-embedding-004')[0]\n",
    "\n",
    "    similarity_scores = []\n",
    "\n",
    "    # Compute similarity scores between query and each text chunk embedding\n",
    "    for i, chunk_embedding_dict in enumerate(embeddings):\n",
    "        # Calculate cosine similarity between the query embedding and current chunk embedding\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding_dict['embedding']))\n",
    "        # Store the index and similarity score as a tuple\n",
    "        similarity_scores.append((i, similarity_score))\n",
    "\n",
    "    # Sort chunks by similarity score in descending order (highest similarity first)\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the index of the most relevant chunk\n",
    "    if not similarity_scores:\n",
    "        return [] # Return empty if no chunks or scores\n",
    "    top_index = similarity_scores[0][0]\n",
    "\n",
    "    # Define the range for context inclusion\n",
    "    # Ensure we don't go below 0 or beyond the length of text_chunks\n",
    "    start = max(0, top_index - context_size)\n",
    "    end = min(len(text_chunks), top_index + context_size + 1)\n",
    "\n",
    "    # Return the relevant chunk along with its neighboring context chunks\n",
    "    return [text_chunks[i] for i in range(start, end)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Query on Extracted Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What is 'Explainable AI' and why is it considered important?\n",
      "\n",
      "Context 1:\n",
      "First document to embed.\n",
      "\n",
      "Second document to embed.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# --- Your existing context_enriched_search, fixed ---\n",
    "def context_enriched_search(query, text_chunks, embeddings, k=1, context_size=1):\n",
    "    \"\"\"\n",
    "    Returns the top-k most similar chunks plus their neighbors for context.\n",
    "    - query:     str\n",
    "    - text_chunks:          list of str\n",
    "    - embeddings:           list of dicts {'embedding': [float,...]}\n",
    "    - k:         int, number of top matches\n",
    "    - context_size: int, how many neighbors on each side\n",
    "    \"\"\"\n",
    "    # 1) embed the query (assuming create_embeddings returns a flat list of floats)\n",
    "    query_emb = create_embeddings(query)\n",
    "\n",
    "    # 2) compute cosine similarity for each chunk\n",
    "    scores = []\n",
    "    for idx, emb_dict in enumerate(embeddings):\n",
    "        chunk_emb = emb_dict['embedding']\n",
    "        sim = np.dot(query_emb, chunk_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(chunk_emb))\n",
    "        scores.append((idx, float(sim)))  # cast to float here\n",
    "\n",
    "    # 3) sort by similarity descending\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 4) pick top-k indices\n",
    "    top_idxs = [idx for idx, _ in scores[:k]]\n",
    "\n",
    "    # 5) gather each top chunk plus neighbors\n",
    "    results = []\n",
    "    for idx in top_idxs:\n",
    "        start = max(0, idx - context_size)\n",
    "        end   = min(len(text_chunks), idx + context_size + 1)\n",
    "        context = \"\\n\\n\".join(text_chunks[start:end])\n",
    "        results.append(context)\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Your JSON loading & query extraction ---\n",
    "val_json_path = '/Users/kekunkoya/Desktop/ISEM 770 GOOGLE Project/data/val.json'\n",
    "try:\n",
    "    with open(val_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: '{val_json_path}' not found. Please ensure it is in the correct directory.\")\n",
    "    data = []\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from '{val_json_path}'. Check file format.\")\n",
    "    data = []\n",
    "\n",
    "if data:\n",
    "    query = data[0].get('question', '')\n",
    "else:\n",
    "    print(\"Warning: No validation data loaded. Using a default query.\")\n",
    "    query = \"What is Explainable AI?\"  # fixed typo\n",
    "\n",
    "# --- Retrieve contexts ---\n",
    "# (Assumes text_chunks and formatted_chunk_embeddings are already defined)\n",
    "top_chunks = context_enriched_search(\n",
    "    query,\n",
    "    text_chunks,\n",
    "    formatted_chunk_embeddings,\n",
    "    k=1,\n",
    "    context_size=1\n",
    ")\n",
    "\n",
    "# --- Print results ---\n",
    "print(\"\\nQuery:\", query)\n",
    "if top_chunks:\n",
    "    for i, chunk in enumerate(top_chunks, start=1):\n",
    "        print(f\"\\nContext {i}:\\n{chunk}\\n\" + \"=\"*40)\n",
    "else:\n",
    "    print(\"No relevant context chunks found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Where can I find emergency food in ZIP code 17104?\n",
      "\n",
      "--- Retrieved Contexts ---\n",
      "[Context 1]\n",
      "to make sure the organization\n",
      "asking you for money is registered as a 501(c) corporation, which means your\n",
      "donation is tax deductible: https://apps.irs.gov/app/eos/\n",
      "• Contact the Pennsylvania Departme…\n",
      "\n",
      "[Context 2]\n",
      " \n",
      "814-765-5357\n",
      "ext. 1\n",
      "Clinton County \n",
      "570-893-4090\n",
      "ext. 209\n",
      "Columbia County \n",
      "570-389-5720\n",
      "Crawford County \n",
      "814-724-2552\n",
      "Cumberland County \n",
      "717-218-2902\n",
      "Dauphin County \n",
      "717-558-6801\n",
      "Delaware County \n",
      "61…\n",
      "\n",
      "\n",
      "AI Response:\n",
      " I do not have enough information to answer that.\n"
     ]
    }
   ],
   "source": [
    "# cell: rag_with_google_ai_fixed\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import fitz                    # PyMuPDF\n",
    "from openai import OpenAI     # pip install openai\n",
    "import google.generativeai as genai  # pip install google-generativeai\n",
    "\n",
    "# --- 1) Configure both clients ---\n",
    "\n",
    "# 1a) Vertex AI via OpenAI-compatible endpoint (for chat)\n",
    "openai_client = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "# 1b) Google GenerativeAI SDK (for embeddings)\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise RuntimeError(\"Please set the GOOGLE_API_KEY environment variable.\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# --- 2) Helpers ---\n",
    "\n",
    "def extract_text_chunks(pdf_path: str, chunk_size: int = 1000) -> list[str]:\n",
    "    \"\"\"Split the entire PDF text into chunks of ~chunk_size characters.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\".join(page.get_text() for page in doc)\n",
    "    return [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "def create_embeddings(texts: list[str]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Embed a list of texts via the Google GenerativeAI SDK's top-level embed_content.\n",
    "    \"\"\"\n",
    "    inputs = texts if isinstance(texts, list) else [texts]\n",
    "    resp = genai.embed_content(\n",
    "        model=\"text-embedding-004\",\n",
    "        content=inputs,\n",
    "        task_type=\"RETRIEVAL_DOCUMENT\"\n",
    "    )\n",
    "    # resp[\"embedding\"] is a list of float lists\n",
    "    return resp[\"embedding\"]\n",
    "\n",
    "def semantic_search(query: str, chunks: list[str], embeddings: list[list[float]], k: int = 2) -> list[str]:\n",
    "    \"\"\"Return top-k text chunks most similar to the query (cosine similarity).\"\"\"\n",
    "    q_emb = create_embeddings([query])[0]\n",
    "    scores = [\n",
    "        (i, float(np.dot(q_emb, emb) / (np.linalg.norm(q_emb) * np.linalg.norm(emb))))\n",
    "        for i, emb in enumerate(embeddings)\n",
    "    ]\n",
    "    top_idxs = [i for i, _ in sorted(scores, key=lambda x: x[1], reverse=True)[:k]]\n",
    "    return [chunks[i] for i in top_idxs]\n",
    "\n",
    "def generate_with_gemini(system_prompt: str, user_message: str, model: str = \"gemini-2.0-flash\") -> str:\n",
    "    \"\"\"\n",
    "    Sends a system + user prompt to Gemini via the OpenAI-compatible endpoint\n",
    "    and returns the assistant’s reply.\n",
    "    \"\"\"\n",
    "    resp = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\",   \"content\": user_message}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "# --- 3) Run RAG pipeline ---\n",
    "\n",
    "# 3a) Extract & embed once per PDF\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/RAG Google/PEMA.pdf\"\n",
    "text_chunks      = extract_text_chunks(pdf_path)\n",
    "chunk_embeddings = create_embeddings(text_chunks)\n",
    "\n",
    "# 3b) Load your validation query\n",
    "with open(\"/Users/kekunkoya/Desktop/RAG Google/PA211_dataset.json\") as f:\n",
    "    data = json.load(f)\n",
    "query = data[0][\"question\"]\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# 3c) Retrieve top-2 chunks\n",
    "top_chunks = semantic_search(query, text_chunks, chunk_embeddings, k=2)\n",
    "\n",
    "# Debug: print what we’re sending\n",
    "print(\"\\n--- Retrieved Contexts ---\")\n",
    "for i, c in enumerate(top_chunks, start=1):\n",
    "    print(f\"[Context {i}]\\n{c[:200]}…\\n\")  # first 200 chars\n",
    "\n",
    "# 3d) Build prompts\n",
    "system_prompt = (\n",
    "    \"You are an AI assistant that strictly answers based on the given context. \"\n",
    "    \"If the answer cannot be derived directly from the provided context, \"\n",
    "    \"respond with: 'I do not have enough information to answer that.'\"\n",
    ")\n",
    "user_message = \"\\n\\n\".join(f\"Context {i}:\\n{c}\" for i, c in enumerate(top_chunks, 1))\n",
    "user_message += f\"\\n\\nQuestion: {query}\"\n",
    "\n",
    "# 3e) Get your answer from Gemini\n",
    "answer = generate_with_gemini(system_prompt, user_message)\n",
    "print(\"\\nAI Response:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We compare the AI response with the expected answer and assign a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Score = 1.0\n",
      "Q2: Score = 1.0\n",
      "Q3: Score = 1.0\n",
      "Q4: Score = 0.0\n",
      "Q5: Score = 0.0\n",
      "\n",
      "Average Evaluation Score: 0.60\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# --- 1) Initialize OpenAI-compatible client for Google Vertex AI ---\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"),\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "# --- 2) Load the dataset ---\n",
    "with open(\"/Users/kekunkoya/Desktop/RAG Google/PA211_dataset.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# --- 3) System prompt for evaluation ---\n",
    "evaluate_system_prompt = (\n",
    "    \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. \"\n",
    "    \"If the AI assistant's response is very close to the true response, assign a score of 1. \"\n",
    "    \"If the response is incorrect or unsatisfactory, assign a score of 0. \"\n",
    "    \"If the response is partially aligned, assign a score of 0.5. \"\n",
    "    \"Reply with ONLY the numeric score (0, 0.5, or 1).\"\n",
    ")\n",
    "\n",
    "# --- 4) Function to get evaluation score ---\n",
    "def evaluate_response(query, ai_response, true_answer):\n",
    "    evaluation_prompt = (\n",
    "        f\"User Query: {query}\\n\\n\"\n",
    "        f\"AI Response: {ai_response}\\n\\n\"\n",
    "        f\"True Response: {true_answer}\\n\\n\"\n",
    "        f\"{evaluate_system_prompt}\"\n",
    "    )\n",
    "    eval_resp = client.chat.completions.create(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": evaluate_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "        ],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return float(eval_resp.choices[0].message.content.strip())\n",
    "\n",
    "# --- 5) Loop through dataset ---\n",
    "scores = []\n",
    "for i, item in enumerate(data):\n",
    "    # Replace this with your actual AI-generated answer from Gemini RAG\n",
    "    ai_response = \"YOUR_AI_RESPONSE_HERE\"  \n",
    "\n",
    "    score = evaluate_response(item[\"question\"], ai_response, item[\"ideal_answer\"])\n",
    "    scores.append(score)\n",
    "    print(f\"Q{i+1}: Score = {score}\")\n",
    "\n",
    "# --- 6) Compute average score ---\n",
    "avg_score = sum(scores) / len(scores) if scores else 0\n",
    "print(f\"\\nAverage Evaluation Score: {avg_score:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
