{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Introduction to Simple RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a hybrid approach that combines information retrieval with generative models. It enhances the performance of language models by incorporating external knowledge, which improves accuracy and factual correctness.\n",
    "\n",
    "In a Simple RAG setup, we follow these steps:\n",
    "\n",
    "1. **Data Ingestion**: Load and preprocess the text data.\n",
    "2. **Chunking**: Break the data into smaller chunks to improve retrieval performance.\n",
    "3. **Embedding Creation**: Convert the text chunks into numerical representations using an embedding model.\n",
    "4. **Semantic Search**: Retrieve relevant chunks based on a user query.\n",
    "5. **Response Generation**: Use a language model to generate a response based on retrieved text.\n",
    "\n",
    "This notebook implements a Simple RAG approach, evaluates the model’s response, and explores various improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "We begin by importing necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Text from a PDF File\n",
    "To implement RAG, we first need a source of textual data. In this case, we extract text from a PDF file using the PyMuPDF library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF file and prints the first `num_chars` characters.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    str: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking the Extracted Text\n",
    "Once we have the extracted text, we divide it into smaller, overlapping chunks to improve retrieval accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    Chunks the given text into segments of n characters with overlap.\n",
    "\n",
    "    Args:\n",
    "    text (str): The text to be chunked.\n",
    "    n (int): The number of characters in each chunk.\n",
    "    overlap (int): The number of overlapping characters between chunks.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of text chunks.\n",
    "    \"\"\"\n",
    "    chunks = []  # Initialize an empty list to store the chunks\n",
    "    \n",
    "    # Loop through the text with a step size of (n - overlap)\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # Append a chunk of text from index i to i + n to the chunks list\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # Return the list of text chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the OpenAI API Client\n",
    "We initialize the OpenAI client to generate embeddings and responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client with the base URL and API key\n",
    "client = OpenAI(\n",
    " \n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # Retrieve the API key from environment variables\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting and Chunking Text from a PDF File\n",
    "Now, we load the PDF, extract text, and split it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 66\n",
      "\n",
      "First text chunk:\n",
      "PENNSYLVANIA\n",
      "EMERGENCY\n",
      "PREPAREDNESSGUIDE\n",
      "Be Informed. Be Prepared. Be Involved. \n",
      "www.Ready .PA.gov \n",
      "readypa@pa.govEmergency Preparedness GuideTable of Contents\n",
      "TABLE OF CONTENTS  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Pages 2-3\n",
      "INTRODUCTION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  Page    4\n",
      "TOP 10 EMERGENCIES . . . . . . . . . . . . . . . . . . . . . . Pages 4-7\n",
      "Floods • Fires • Winter Storms • Tropical Storms, Tornadoes \n",
      "and Thunderstorms • Influenza (Flu) Pandemic • Hazardous \n",
      "Material Incidents • Earthquakes and Landslides • Nuclear \n",
      "Threat • Dam Failures • Terrorism\n",
      "BE PREPARED – MAKE A PLAN    .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .  .    Pages 10-11\n",
      "How to Make a Family Emergency Plan\n",
      "HOME EMERGENCY KIT CHECKLIST .   .  .  .  .  .  .  .  .  .  .  .  .  .  .   Pages 12-15\n",
      "Additional Special Items\n",
      "BE PREPARED IN YOUR VEHICLE   . . . . . . . . . . . . . . . . . . . . . .  Page 16\n",
      "How to Prepare\n",
      "2EMERGING   ISSUES .   .   .  \n"
     ]
    }
   ],
   "source": [
    "# Import the required library for PDF reading\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# --- Function Definitions ---\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from all pages of a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    # Open the PDF file in binary read mode\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf = PdfReader(file)\n",
    "        # Loop through each page and extract text\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    \"\"\"Chunks text into segments with a specified overlap.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    # Loop through the text and create overlapping chunks\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunks.append(text[start:end])\n",
    "        # Move the start position forward by the chunk size minus the overlap\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# --- Your Original Logic ---\n",
    "\n",
    "# Define the path to the PDF file\n",
    "# IMPORTANT: Make sure this path is correct on your system\n",
    "pdf_path = \"/Users/kekunkoya/Desktop/RAG Project/PEMA.pdf\"\n",
    "\n",
    "# Extract text from the PDF file by calling the function we defined\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Chunk the extracted text by calling the function we defined\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# Print the number of text chunks created\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# Print the first text chunk\n",
    "print(\"\\nFirst text chunk:\")\n",
    "# Print only the first chunk if it exists\n",
    "if text_chunks:\n",
    "    print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings for Text Chunks\n",
    "Embeddings transform text into numerical vectors, which allow for efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key=os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "key = os.getenv(\"OPENAI_API_KEY\") # No default needed if you ensure env var is set\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "def create_embeddings(text_chunks, model=\"text-embedding-3-small\"):\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text_chunks\n",
    "    )\n",
    "    return response.data\n",
    "\n",
    "# You'll need to define text_chunks before calling create_embeddings\n",
    "# For example:\n",
    "# text_chunks = [\"This is a test chunk.\", \"Another piece of text.\"]\n",
    "\n",
    "# embeddings_data = create_embeddings(text_chunks)\n",
    "# print(f\"Created {len(embeddings_data)} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "key = os.getenv(\"OPENAI_API_KEY\") # No default needed if you ensure env var is set\n",
    "client = OpenAI(api_key=key)\n",
    "\n",
    "def create_embeddings(text_chunks, model=\"text-embedding-3-small\"):\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text_chunks\n",
    "    )\n",
    "    return response.data\n",
    "\n",
    "# You'll need to define text_chunks before calling create_embeddings\n",
    "# For example:\n",
    "# text_chunks = [\"This is a test chunk.\", \"Another piece of text.\"]\n",
    "\n",
    "# embeddings_data = create_embeddings(text_chunks)\n",
    "# print(f\"Created {len(embeddings_data)} embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 3 embeddings.\n",
      "\n",
      "Embedding for the first chunk (first 5 values):\n",
      "[0.027513332664966583, 0.037257637828588486, -0.004034167155623436, 0.010535563342273235, 0.012971639633178711]\n"
     ]
    }
   ],
   "source": [
    "# Make sure to install the library: pip install openai\n",
    "from openai import OpenAI\n",
    "import os # Import os to check for environment variables\n",
    "\n",
    "# It's good practice to ensure the API key is set\n",
    "# If OPENAI_API_KEY is not set, this will raise an error\n",
    "# You might want to add error handling here for production code\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"Warning: OPENAI_API_KEY environment variable is not set.\")\n",
    "    print(\"Please set it before running the script (e.g., export OPENAI_API_KEY='sk-...').\")\n",
    "    # For testing, you could temporarily uncomment the line below and put your key\n",
    "    # os.environ[\"OPENAI_API_KEY\"] = \"sk-YOUR_TEST_KEY_HERE\"\n",
    "    # exit() # Or exit if the key is mandatory for execution\n",
    "\n",
    "client = OpenAI() # This works if OPENAI_API_KEY is set in your environment\n",
    "\n",
    "def create_embeddings(text_chunks, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for a list of text chunks using the specified OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    text_chunks (list[str]): The list of input texts for which embeddings are to be created.\n",
    "    model (str): The OpenAI model to be used. Default is \"text-embedding-3-small\".\n",
    "\n",
    "    Returns:\n",
    "    list: A list of embedding objects from the OpenAI API.\n",
    "    \"\"\"\n",
    "    # The 'input' parameter for the OpenAI API can take a list of strings directly\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text_chunks\n",
    "    )\n",
    "\n",
    "    # The embeddings are located in response.data\n",
    "    return response.data\n",
    "\n",
    "# --- THIS IS THE MISSING PART ---\n",
    "# Define 'text_chunks' as a list of strings\n",
    "text_chunks = [\n",
    "    \"This is the first sentence, which we want to embed.\",\n",
    "    \"This is the second one, containing different information.\",\n",
    "    \"A third text chunk to demonstrate the functionality.\"\n",
    "]\n",
    "# --- END OF MISSING PART ---\n",
    "\n",
    "# Create embeddings for the text chunks\n",
    "embeddings_data = create_embeddings(text_chunks)\n",
    "\n",
    "# Print the number of embeddings created\n",
    "print(f\"Successfully created {len(embeddings_data)} embeddings.\")\n",
    "\n",
    "# Print the embedding for the first text chunk (optional)\n",
    "if embeddings_data:\n",
    "    print(\"\\nEmbedding for the first chunk (first 5 values):\")\n",
    "    print(embeddings_data[0].embedding[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 3 embeddings.\n",
      "\n",
      "Embedding for the first chunk (first 5 values):\n",
      "[0.027504978701472282, 0.03724632412195206, -0.00400179997086525, 0.010563506744801998, 0.012980157509446144]\n"
     ]
    }
   ],
   "source": [
    "# Make sure to install the library: pip install openai\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "client = OpenAI() # This works if OPENAI_API_KEY is set in your environment\n",
    "\n",
    "def create_embeddings(text_chunks, model=\"text-embedding-3-small\"):\n",
    "    \"\"\"\n",
    "    Creates embeddings for a list of text chunks using the specified OpenAI model.\n",
    "\n",
    "    Args:\n",
    "    text_chunks (list[str]): The list of input texts for which embeddings are to be created.\n",
    "    model (str): The OpenAI model to be used. Default is \"text-embedding-3-small\".\n",
    "\n",
    "    Returns:\n",
    "    list: A list of embedding objects from the OpenAI API.\n",
    "    \"\"\"\n",
    "    # The 'input' parameter for the OpenAI API can take a list of strings directly\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text_chunks\n",
    "    )\n",
    "\n",
    "    # The embeddings are located in response.data\n",
    "    return response.data\n",
    "\n",
    "# Assume 'text_chunks' is a list of strings from your previous code\n",
    "# Example: text_chunks = [\"This is the first sentence.\", \"This is the second one.\"]\n",
    "\n",
    "# Create embeddings for the text chunks\n",
    "embeddings_data = create_embeddings(text_chunks)\n",
    "\n",
    "# Print the number of embeddings created\n",
    "print(f\"Successfully created {len(embeddings_data)} embeddings.\")\n",
    "\n",
    "# Print the embedding for the first text chunk (optional)\n",
    "if embeddings_data:\n",
    "    print(\"\\nEmbedding for the first chunk (first 5 values):\")\n",
    "    print(embeddings_data[0].embedding[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 3 embeddings with model 'text-embedding-3-small'.\n",
      "\n",
      "First 5 values of the first embedding:\n",
      "[0.027504978701472282, 0.03724632412195206, -0.00400179997086525, 0.010563506744801998, 0.012980157509446144]\n"
     ]
    }
   ],
   "source": [
    "# 1. Import the OpenAI library\n",
    "from openai import OpenAI\n",
    "\n",
    "# 2. Initialize the client\n",
    "# The client automatically looks for the OPENAI_API_KEY environment variable.\n",
    "client = OpenAI()\n",
    "\n",
    "# Assume 'text_chunks' is a list of strings from your previous code\n",
    "# Example: text_chunks = [\"This is the first sentence.\", \"This is the second one.\"]\n",
    "\n",
    "# 3. Create embeddings using the specified OpenAI model\n",
    "model_name = \"text-embedding-3-small\"\n",
    "response = client.embeddings.create(\n",
    "    model=model_name,\n",
    "    input=text_chunks\n",
    ")\n",
    "\n",
    "# 4. Extract the embedding vectors from the response object\n",
    "# The actual embeddings are in the `.data` attribute of the response.\n",
    "embeddings = [embedding_item.embedding for embedding_item in response.data]\n",
    "\n",
    "# Check the first embedding's first few values\n",
    "if embeddings:\n",
    "    print(f\"Successfully created {len(embeddings)} embeddings with model '{model_name}'.\")\n",
    "    print(\"\\nFirst 5 values of the first embedding:\")\n",
    "    print(embeddings[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Semantic Search\n",
    "We implement cosine similarity to find the most relevant text chunks for a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "    vec1 (np.ndarray): The first vector.\n",
    "    vec2 (np.ndarray): The second vector.\n",
    "\n",
    "    Returns:\n",
    "    float: The cosine similarity between the two vectors.\n",
    "    \"\"\"\n",
    "    # Compute the dot product of the two vectors and divide by the product of their norms\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Performs semantic search on the text chunks using the given query and embeddings.\n",
    "\n",
    "    Args:\n",
    "    query (str): The query for the semantic search.\n",
    "    text_chunks (List[str]): A list of text chunks to search through.\n",
    "    embeddings (List[dict]): A list of embeddings for the text chunks.\n",
    "    k (int): The number of top relevant text chunks to return. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of the top k most relevant text chunks based on the query.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query).data[0].embedding\n",
    "    similarity_scores = []  # Initialize a list to store similarity scores\n",
    "\n",
    "    # Calculate similarity scores between the query embedding and each text chunk embedding\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n",
    "        similarity_scores.append((i, similarity_score))  # Append the index and similarity score\n",
    "\n",
    "    # Sort the similarity scores in descending order\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # Get the indices of the top k most similar text chunks\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # Return the top k most relevant text chunks\n",
    "    return [text_chunks[index] for index in top_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a Query on Extracted Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def semantic_search(query: str, text_chunks: list[str], embeddings: list[list[float]], k: int):\n",
    "    \"\"\"\n",
    "    Performs semantic search using a query, text chunks, and their embeddings.\n",
    "    \"\"\"\n",
    "    # Create an embedding for the query\n",
    "    query_embedding = create_embeddings(query)[0].embedding\n",
    "\n",
    "    # Calculate similarity scores between the query and each text chunk\n",
    "    similarity_scores = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        embeddings\n",
    "    )[0]\n",
    "\n",
    "    # Get the indices of the top k scores\n",
    "    top_k_indices = np.argsort(similarity_scores)[-k:][::-1]\n",
    "\n",
    "    # Return the corresponding text chunks using the full variable name\n",
    "    #\n",
    "    # OLD, INCORRECT line:\n",
    "    # return [text_chunks[i] for i in top_\n",
    "    #\n",
    "    # NEW, CORRECT line:\n",
    "    return [text_chunks[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a Response Based on Retrieved Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating AI response...\n",
      "\n",
      "AI Response:\n",
      "Disasters are events that cause significant damage, destruction, and disruption to communities and the environment. They can be natural, such as earthquakes, hurricanes, and wildfires, or human-made, like industrial accidents and terrorist attacks. Disasters often result in loss of life, displacement of people, and economic hardship. Preparedness, response, and recovery efforts are crucial in mitigating the impact of disasters.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from openai import AuthenticationError # Import for specific error handling\n",
    "from dotenv import load_dotenv # Optional, for loading from .env file\n",
    "\n",
    "# --- 1. Set up your OpenAI client ---\n",
    "# Load environment variables from a .env file (if you're using one)\n",
    "load_dotenv()\n",
    "\n",
    "# Get your API key from the environment variable\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Basic check for API key\n",
    "if key is None:\n",
    "    print(\"Error: OPENAI_API_KEY environment variable is not set.\")\n",
    "    print(\"Please set your API key before running the script.\")\n",
    "    exit() # Exit if the key is not found\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "try:\n",
    "    client = OpenAI(api_key=key)\n",
    "except AuthenticationError as e:\n",
    "    print(f\"Authentication Error: {e}\")\n",
    "    print(\"Please check your OpenAI API key. It might be incorrect, expired, or have insufficient permissions.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during client initialization: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Define the generate_response function ---\n",
    "def generate_response(system_prompt, user_message, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the OpenAI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The OpenAI model to be used for generating the response.\n",
    "                 Default is now \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "    dict: The response from the AI model.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# --- 3. Define the system_prompt and user_prompt variables ---\n",
    "# THIS IS THE MISSING PART THAT CAUSED THE NAMERROR\n",
    "system_prompt = \"You are a helpful and creative AI assistant. Provide concise and relevant answers.\"\n",
    "user_prompt = \"Tell me about disasters.\"\n",
    "# --- END OF MISSING PART ---\n",
    "\n",
    "\n",
    "# --- 4. Generate AI response using the corrected function ---\n",
    "print(\"Generating AI response...\")\n",
    "try:\n",
    "    ai_response = generate_response(system_prompt, user_prompt)\n",
    "\n",
    "    # Print the final response content from the AI\n",
    "    print(\"\\nAI Response:\")\n",
    "    print(ai_response.choices[0].message.content)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during AI response generation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the AI Response\n",
    "We compare the AI response with the expected answer and assign a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What can I find emergency food in 17104?\n",
      "AI Assistant's Response: You can find emergency food resources in the 17104 area by contacting local food banks, community centers, or organizations like the Central Pennsylvania Food Bank. They can provide you with information on where to access emergency food assistance in your area.\n",
      "\n",
      "Generating Evaluation Response...\n",
      "Evaluation Score: 0.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from openai import AuthenticationError # Import for specific error handling\n",
    "from dotenv import load_dotenv # Optional, for loading from .env file\n",
    "\n",
    "# --- 1. Set up your OpenAI client ---\n",
    "# Load environment variables from a .env file (if you're using one)\n",
    "load_dotenv()\n",
    "\n",
    "# Get your API key from the environment variable\n",
    "key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Basic check for API key\n",
    "if key is None:\n",
    "    print(\"Error: OPENAI_API_KEY environment variable is not set.\")\n",
    "    print(\"Please set your API key before running the script.\")\n",
    "    exit() # Exit if the key is not found\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "try:\n",
    "    client = OpenAI(api_key=key)\n",
    "except AuthenticationError as e:\n",
    "    print(f\"Authentication Error: {e}\")\n",
    "    print(\"Please check your OpenAI API key. It might be incorrect, expired, or have insufficient permissions.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred during client initialization: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Define the generate_response function ---\n",
    "def generate_response(system_prompt, user_message, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Generates a response from the OpenAI model based on the system prompt and user message.\n",
    "\n",
    "    Args:\n",
    "    system_prompt (str): The system prompt to guide the AI's behavior.\n",
    "    user_message (str): The user's message or query.\n",
    "    model (str): The OpenAI model to be used for generating the response.\n",
    "                 Default is now \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "    object: The response object from the AI model (e.g., ChatCompletion object).\n",
    "            You access content via .choices[0].message.content\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# --- 3. Define the variables needed for the AI response and evaluation ---\n",
    "# Define the user's query\n",
    "query = \"What can I find emergency food in 17104?\"\n",
    "\n",
    "\n",
    "# Define the data structure containing the true response\n",
    "# This 'data' variable simulates where your true answers would come from,\n",
    "# perhaps a loaded dataset or a database.\n",
    "\n",
    "data = [\n",
    "    {\"question\": \"Where can I find emergency food in ZIP code 17104?\", \"ideal_answer\": \"Thanks for reaching out. Based on your ZIP code (17104), here are food resources available near you:\\n\\n🍫 Central Pennsylvania Food Bank\\nLocation: 3908 Corey Rd, Harrisburg, PA 17109\\nPhone: (717) 564-1700\\nServices: Emergency food boxes, drive-thru distribution (Mon–Fri, 9am–3pm)\\n\\n🥗 St. Francis of Assisi Church Pantry\\nAddress: 1439 Market St, Harrisburg, PA 17103\\nServices: Walk-in pantry on Wednesdays, 10am–1pm\\n\\n🫾 You may also qualify for Disaster SNAP benefits (D-SNAP). A 211 Navigator can guide you through the process.\\n\\n📞 You can always call 2-1-1 or text 898-211 for live assistance.\"}\n",
    "]\n",
    "# Define the system prompt for the AI assistant\n",
    "# This is the 'system_prompt' that generate_response uses to guide the AI's initial behavior\n",
    "ai_assistant_system_prompt = \"You are a helpful and creative AI assistant. Provide concise and relevant answers.\"\n",
    "\n",
    "# --- 4. Generate AI response ---\n",
    "print(f\"User Query: {query}\")\n",
    "try:\n",
    "    # Use ai_assistant_system_prompt for the AI assistant's behavior\n",
    "    ai_response = generate_response(ai_assistant_system_prompt, query)\n",
    "    ai_response_content = ai_response.choices[0].message.content\n",
    "    print(f\"AI Assistant's Response: {ai_response_content}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during AI response generation: {e}\")\n",
    "    # Handle the error, maybe set ai_response_content to an error message\n",
    "    ai_response_content = \"Error generating AI response.\"\n",
    "\n",
    "\n",
    "# --- 5. Define the evaluation system prompt ---\n",
    "evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5. Provide only the score (e.g., '1', '0', or '0.5') and nothing else.\" # Added instruction for only score\n",
    "\n",
    "# --- 6. Create the evaluation prompt ---\n",
    "# This evaluation_prompt acts as the 'user_message' for the evaluation AI\n",
    "evaluation_prompt = (\n",
    "    f\"User Query: {query}\\n\"\n",
    "    f\"AI Response:\\n{ai_response_content}\\n\" # Use the content directly\n",
    "    f\"True Response: {data[0]['ideal_answer']}\\n\"\n",
    "    f\"Instructions: Based on the true response, evaluate the AI response and provide a score according to the following rules: Score 1 if very close. Score 0 if incorrect/unsatisfactory. Score 0.5 if partially aligned. Output only the score.\"\n",
    ")\n",
    "\n",
    "# --- 7. Generate the evaluation response ---\n",
    "print(\"\\nGenerating Evaluation Response...\")\n",
    "try:\n",
    "    # Use evaluate_system_prompt for the evaluation AI's behavior\n",
    "    # evaluation_prompt becomes the user_message for the evaluation AI\n",
    "    evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "    evaluation_score = evaluation_response.choices[0].message.content\n",
    "\n",
    "    # Print the evaluation response\n",
    "    print(f\"Evaluation Score: {evaluation_score}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during evaluation response generation: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
